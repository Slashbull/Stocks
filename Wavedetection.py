"""
Wave Detection Ultimate 3.0 - FINAL ENHANCED PRODUCTION VERSION
===============================================================
Professional Stock Ranking System with Advanced Analytics
All bugs fixed, optimized for Streamlit Community Cloud
Enhanced with all valuable features from previous versions

Version: 3.0.8-FINAL-PRODUCTION
Last Updated: July 2025
Status: PRODUCTION READY - Feature Complete and Locked
"""

# ============================================
# IMPORTS AND SETUP
# ============================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timezone
import logging
from typing import Dict, List, Tuple, Optional, Any, Union, Set
from dataclasses import dataclass, field
from functools import lru_cache, wraps
import time
from io import BytesIO, StringIO
import warnings
import gc
import re
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import hashlib

# Suppress warnings for clean output
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# ============================================
# LOGGING CONFIGURATION
# ============================================

log_level = logging.INFO

logging.basicConfig(
    level=log_level,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================
# CONFIGURATION AND CONSTANTS
# ============================================

@dataclass(frozen=True)
class Config:
    """System configuration with validated weights and thresholds"""
    
    # Data source - GID retained, base URL is dynamic
    DEFAULT_GID: str = "1823439984" 
    
    # Cache settings optimized for Streamlit Community Cloud
    CACHE_TTL: int = 3600  # 1 hour
    
    # Master Score 3.0 weights (total = 100%) - DO NOT MODIFY
    POSITION_WEIGHT: float = 0.30
    VOLUME_WEIGHT: float = 0.25
    MOMENTUM_WEIGHT: float = 0.15
    ACCELERATION_WEIGHT: float = 0.10
    BREAKOUT_WEIGHT: float = 0.10
    RVOL_WEIGHT: float = 0.10
    
    # Display settings
    DEFAULT_TOP_N: int = 50
    AVAILABLE_TOP_N: List[int] = field(default_factory=lambda: [10, 20, 50, 100, 200, 500])
    
    # Critical columns (app fails without these)
    CRITICAL_COLUMNS: List[str] = field(default_factory=lambda: ['ticker', 'price', 'volume_1d'])
    
    # All percentage columns for consistent handling
    PERCENTAGE_COLUMNS: List[str] = field(default_factory=lambda: [
        'from_low_pct', 'from_high_pct',
        'ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 
        'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y',
        'eps_change_pct'
    ])
    
    # Volume ratio columns
    VOLUME_RATIO_COLUMNS: List[str] = field(default_factory=lambda: [
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d'
    ])
    
    # Pattern thresholds
    PATTERN_THRESHOLDS: Dict[str, float] = field(default_factory=lambda: {
        "ðŸ”¥ CAT LEADER": 90,
        "ðŸ’Ž HIDDEN GEM": 80,
        "ðŸš€ ACCELERATING": 85,
        "ðŸ¦ INSTITUTIONAL": 75,
        "âš¡ VOL EXPLOSION": 95,
        "ðŸŽ¯ BREAKOUT": 80,
        "ðŸ‘‘ MARKET LEADER": 95,
        "ðŸŒŠ MOMENTUM WAVE": 75,
        "ðŸ’° LIQUID LEADER": 80,
        "ðŸ’ª LONG STRENGTH": 80,
        "ðŸ“ˆ QUALITY TREND": 80,
        "ðŸ’Ž VALUE MOMENTUM": 70,
        "ðŸ“Š EARNINGS ROCKET": 70,
        "ðŸ† QUALITY LEADER": 80,
        "âš¡ TURNAROUND": 60,
        "âš ï¸ HIGH PE": 100,
        "ðŸŽ¯ 52W HIGH APPROACH": 70,
        "ðŸ”„ 52W LOW BOUNCE": 80,
        "ðŸ‘‘ GOLDEN ZONE": 70,
        "ðŸ“Š VOL ACCUMULATION": 80,
        "ðŸ”€ MOMENTUM DIVERGE": 85,
        "ðŸŽ¯ RANGE COMPRESS": 75,
        "ðŸ¤« STEALTH": 60,
        "ðŸ§› VAMPIRE": 70,
        "â›ˆï¸ PERFECT STORM": 80
    })
    
    # Value bounds for data validation
    VALUE_BOUNDS: Dict[str, Tuple[float, float]] = field(default_factory=lambda: {
        'price': (0.01, 1_000_000),
        'rvol': (0.001, 1_000_000.0),
        'pe': (-10000, 10000),
        'returns': (-99.99, 9999.99),
        'volume': (0, 1e12)
    })
    
    # Performance targets (for logging warnings)
    PERFORMANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        'data_processing': 2.0,  # seconds
        'filtering': 0.2,
        'pattern_detection': 0.5,
        'export_generation': 1.0,
        'search': 0.05
    })

    # Tier definitions with proper boundaries
    TIERS: Dict[str, Dict[str, Tuple[float, float]]] = field(default_factory=lambda: {
        "eps": {
            "Loss": (-float('inf'), 0),
            "0-5": (0, 5),
            "5-10": (5, 10),
            "10-20": (10, 20),
            "20-50": (20, 50),
            "50-100": (50, 100),
            "100+": (100, float('inf'))
        },
        "pe": {
            "Negative/NA": (-float('inf'), 0),
            "0-10": (0, 10),
            "10-15": (10, 15),
            "15-20": (15, 20),
            "20-30": (20, 30),
            "30-50": (30, 50),
            "50+": (50, float('inf'))
        },
        "price": {
            "0-100": (0, 100),
            "100-250": (100, 250),
            "250-500": (250, 500),
            "500-1000": (500, 1000),
            "1000-2500": (1000, 2500),
            "2500-5000": (2500, 5000),
            "5000+": (5000, float('inf'))
        }
    })

# Global configuration instance
CONFIG = Config()

# ============================================
# PERFORMANCE MONITORING
# ============================================

class PerformanceMonitor:
    """Track and report performance metrics"""
    
    @staticmethod
    def timer(target_time: Optional[float] = None):
        """Performance timing decorator with target comparison"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                try:
                    result = func(*args, **kwargs)
                    elapsed = time.perf_counter() - start
                    
                    if target_time and elapsed > target_time:
                        logger.warning(f"{func.__name__} took {elapsed:.2f}s (target: {target_time}s)")
                    elif elapsed > 1.0:
                        logger.info(f"{func.__name__} completed in {elapsed:.2f}s")
                    
                    if 'performance_metrics' not in st.session_state:
                        st.session_state.performance_metrics = {}
                    st.session_state.performance_metrics[func.__name__] = elapsed
                    
                    return result
                except Exception as e:
                    elapsed = time.perf_counter() - start
                    logger.error(f"{func.__name__} failed after {elapsed:.2f}s: {str(e)}")
                    raise
            return wrapper
        return decorator

# ============================================
# DATA VALIDATION AND SANITIZATION
# ============================================

class DataValidator:
    """Comprehensive data validation and sanitization"""
    
    _clipping_counts: Dict[str, int] = {}
    
    @staticmethod
    def get_clipping_counts() -> Dict[str, int]:
        """Returns the current clipping counts and resets them."""
        counts = DataValidator._clipping_counts.copy()
        DataValidator._clipping_counts.clear()
        return counts
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame, required_cols: List[str], context: str) -> Tuple[bool, str]:
        """Validate dataframe structure and data quality"""
        if df is None:
            return False, f"{context}: DataFrame is None"
        
        if df.empty:
            return False, f"{context}: DataFrame is empty"
        
        missing_critical = [col for col in CONFIG.CRITICAL_COLUMNS if col not in df.columns]
        if missing_critical:
            return False, f"{context}: Missing critical columns: {missing_critical}"
        
        duplicates = df['ticker'].duplicated().sum()
        if duplicates > 0:
            logger.warning(f"{context}: Found {duplicates} duplicate tickers")
        
        total_cells = len(df) * len(df.columns)
        filled_cells = df.notna().sum().sum()
        completeness = (filled_cells / total_cells * 100) if total_cells > 0 else 0
        
        if completeness < 50:
            logger.warning(f"{context}: Low data completeness ({completeness:.1f}%)")
        
        if 'data_quality' not in st.session_state:
            st.session_state.data_quality = {}
        
        st.session_state.data_quality.update({
            'completeness': completeness,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'duplicate_tickers': duplicates,
            'context': context,
            'timestamp': datetime.now(timezone.utc)
        })
        
        logger.info(f"{context}: Validated {len(df)} rows, {len(df.columns)} columns, {completeness:.1f}% complete")
        return True, "Valid"
    
    @staticmethod
    def clean_numeric_value(value: Any, col_name: str, is_percentage: bool = False, bounds: Optional[Tuple[float, float]] = None) -> Optional[float]:
        """Clean and convert numeric values with bounds checking and clipping notification"""
        if pd.isna(value) or value == '' or value is None:
            return np.nan
        
        try:
            cleaned = str(value).strip()
            
            if cleaned.upper() in ['', '-', 'N/A', 'NA', 'NAN', 'NONE', '#VALUE!', '#ERROR!', '#DIV/0!', 'INF', '-INF']:
                return np.nan
            
            cleaned = cleaned.replace('â‚¹', '').replace('$', '').replace(',', '').replace(' ', '').replace('%', '')
            result = float(cleaned)
            
            if bounds:
                min_val, max_val = bounds
                original_result = result
                
                if result < min_val:
                    result = min_val
                    logger.warning(f"Value clipped for column '{col_name}': Original {original_result:.2f} clipped to min {min_val:.2f}.")
                    DataValidator._clipping_counts[col_name] = DataValidator._clipping_counts.get(col_name, 0) + 1
                elif result > max_val:
                    result = max_val
                    logger.warning(f"Value clipped for column '{col_name}': Original {original_result:.2f} clipped to max {max_val:.2f}.")
                    DataValidator._clipping_counts[col_name] = DataValidator._clipping_counts.get(col_name, 0) + 1
            
            if np.isnan(result) or np.isinf(result):
                return np.nan
            
            return result
            
        except (ValueError, TypeError, AttributeError):
            return np.nan
    
    @staticmethod
    def sanitize_string(value: Any, default: str = "Unknown") -> str:
        """Sanitize string values"""
        if pd.isna(value) or value is None:
            return default
        
        cleaned = str(value).strip()
        if cleaned.upper() in ['', 'N/A', 'NA', 'NAN', 'NONE', 'NULL', '-']:
            return default
        
        cleaned = ' '.join(cleaned.split())
        
        return cleaned

# ============================================
# SMART CACHING WITH RETRY AND DYNAMIC SOURCING
# ============================================

def get_requests_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504, 429),
    session=None,
) -> requests.Session:
    """Configures a requests session with retry logic for robust HTTP requests."""
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

@st.cache_data(ttl=CONFIG.CACHE_TTL, persist="disk", show_spinner=False)
def load_and_process_data(source_type: str = "sheet", file_data=None, 
                         spreadsheet_id: str = None, 
                         data_version: str = "1.0") -> Tuple[pd.DataFrame, datetime, Dict[str, Any]]:
    """
    Load and process data with smart caching. 
    The cache key includes a hash of the spreadsheet ID and the date for daily invalidation.
    """
    
    start_time = time.perf_counter()
    metadata = {
        'source_type': source_type,
        'data_version': data_version,
        'processing_start': datetime.now(timezone.utc),
        'errors': [],
        'warnings': []
    }
    
    try:
        if source_type == "upload" and file_data is not None:
            logger.info("Loading data from uploaded CSV")
            df = pd.read_csv(file_data, low_memory=False)
            metadata['source'] = "User Upload"
        else: # Sheet mode
            if not spreadsheet_id:
                raise ValueError("A valid Google Spreadsheet ID is required to load data.")
            
            final_spreadsheet_id_to_use = spreadsheet_id.strip()
            
            # Construct CSV export URL
            base_url = f"https://docs.google.com/spreadsheets/d/{final_spreadsheet_id_to_use}"
            csv_url = f"{base_url}/export?format=csv&gid={CONFIG.DEFAULT_GID}"
            
            logger.info(f"Attempting to load data from Google Sheets with ID: {final_spreadsheet_id_to_use}")
            
            try:
                # Use requests with retry for robust fetching and timeout
                session = get_requests_retry_session()
                response = session.get(csv_url, timeout=10) # 10 second timeout
                response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
                
                df = pd.read_csv(StringIO(response.text), low_memory=False)
                metadata['source'] = f"Google Sheets (ID: {final_spreadsheet_id_to_use}, GID: {CONFIG.DEFAULT_GID})"
                st.session_state.last_loaded_url = csv_url
            except requests.exceptions.RequestException as req_e:
                error_msg = f"Network or HTTP error loading Google Sheet: {req_e}"
                logger.error(error_msg)
                metadata['errors'].append(error_msg)
                
                # Try to use cached data as fallback
                if 'last_good_data' in st.session_state:
                    logger.info("Using cached data as fallback due to network/HTTP error.")
                    df, timestamp, old_metadata = st.session_state.last_good_data
                    metadata['warnings'].append("Using cached data due to load failure")
                    metadata['cache_used'] = True
                    return df, timestamp, metadata
                raise ValueError(error_msg) from req_e
            except Exception as e:
                error_msg = f"Failed to load CSV from Google Sheet: {str(e)}"
                logger.error(error_msg)
                metadata['errors'].append(error_msg)
                
                if 'last_good_data' in st.session_state:
                    logger.info("Using cached data as fallback due to CSV parsing error.")
                    df, timestamp, old_metadata = st.session_state.last_good_data
                    metadata['warnings'].append("Using cached data due to load failure")
                    metadata['cache_used'] = True
                    return df, timestamp, metadata
                raise ValueError(error_msg) from e
        
        is_valid, validation_msg = DataValidator.validate_dataframe(df, CONFIG.CRITICAL_COLUMNS, "Initial load")
        if not is_valid:
            raise ValueError(validation_msg)
        
        df = DataProcessor.process_dataframe(df, metadata)
        df = RankingEngine.calculate_all_scores(df)
        df = AdvancedMetrics.calculate_all_metrics(df)
        df = PatternDetector.detect_all_patterns(df)
        
        is_valid, validation_msg = DataValidator.validate_dataframe(df, ['master_score', 'rank'], "Final processed")
        if not is_valid:
            raise ValueError(validation_msg)
        
        timestamp = datetime.now(timezone.utc)
        st.session_state.last_good_data = (df.copy(), timestamp, metadata)
        
        processing_time = time.perf_counter() - start_time
        metadata['processing_time'] = processing_time
        metadata['processing_end'] = datetime.now(timezone.utc)
        
        logger.info(f"Data processing complete: {len(df)} stocks in {processing_time:.2f}s")
        
        clipping_info = DataValidator.get_clipping_counts()
        if clipping_info:
            logger.warning(f"Data clipping occurred: {clipping_info}")
            metadata['warnings'].append(f"Some numeric values were clipped: {clipping_info}. See logs for details.")

        gc.collect()
        
        return df, timestamp, metadata
        
    except Exception as e:
        logger.error(f"Failed to load and process data: {str(e)}")
        metadata['errors'].append(str(e))
        raise

# ============================================
# DATA PROCESSING ENGINE
# ============================================

class DataProcessor:
    """Handle all data processing with validation and optimization"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def process_dataframe(df: pd.DataFrame, metadata: Dict[str, Any]) -> pd.DataFrame:
        """Complete data processing pipeline"""
        
        df = df.copy()
        initial_count = len(df)
        
        numeric_cols_to_process = [col for col in df.columns if col not in ['ticker', 'company_name', 'category', 'sector', 'industry', 'year', 'market_cap']]
        
        for col in numeric_cols_to_process:
            if col in df.columns:
                is_pct = col in CONFIG.PERCENTAGE_COLUMNS
                
                bounds = None
                if 'volume' in col.lower():
                    bounds = CONFIG.VALUE_BOUNDS['volume']
                elif col == 'rvol':
                    bounds = CONFIG.VALUE_BOUNDS['rvol']
                elif col == 'pe':
                    bounds = CONFIG.VALUE_BOUNDS['pe']
                elif is_pct:
                    bounds = CONFIG.VALUE_BOUNDS['returns']
                elif col == 'price':
                    bounds = CONFIG.VALUE_BOUNDS['price']
                
                df[col] = df[col].apply(lambda x: DataValidator.clean_numeric_value(x, col, is_pct, bounds))
        
        string_cols = ['ticker', 'company_name', 'category', 'sector', 'industry']
        for col in string_cols:
            if col in df.columns:
                df[col] = df[col].apply(DataValidator.sanitize_string)
        
        for col in CONFIG.VOLUME_RATIO_COLUMNS:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                # Convert percentage change to ratio
                df[col] = (100 + df[col]) / 100
                df[col] = df[col].clip(0.01, 1000.0)
                df[col] = df[col].fillna(1.0)
        
        df = df.dropna(subset=['ticker', 'price'], how='any')
        df = df[df['price'] > 0.01]
        
        before_dedup = len(df)
        df = df.drop_duplicates(subset=['ticker'], keep='first')
        if before_dedup > len(df):
            metadata['warnings'].append(f"Removed {before_dedup - len(df)} duplicate tickers")
        
        df = DataProcessor._add_tier_classifications(df)
        
        removed = initial_count - len(df)
        if removed > 0:
            metadata['warnings'].append(f"Removed {removed} invalid rows during processing")
        
        logger.info(f"Processed {len(df)} valid stocks from {initial_count} initial rows")
        
        return df
    
    @staticmethod
    def _add_tier_classifications(df: pd.DataFrame) -> pd.DataFrame:
        """Add tier classifications with proper boundary handling"""
        
        def classify_tier(value: float, tier_dict: Dict[str, Tuple[float, float]]) -> str:
            if pd.isna(value):
                return "Unknown"
            
            for tier_name, (min_val, max_val) in tier_dict.items():
                if min_val < value <= max_val:
                    return tier_name
                if min_val == -float('inf') and value <= max_val:
                    return tier_name
            return "Unknown"
        
        if 'eps_current' in df.columns:
            df['eps_tier'] = df['eps_current'].apply(
                lambda x: classify_tier(x, CONFIG.TIERS['eps'])
            )
        
        if 'pe' in df.columns:
            df['pe_tier'] = df['pe'].apply(
                lambda x: "Negative/NA" if pd.isna(x) or x <= 0 
                else classify_tier(x, CONFIG.TIERS['pe'])
            )
        
        if 'price' in df.columns:
            df['price_tier'] = df['price'].apply(
                lambda x: classify_tier(x, CONFIG.TIERS['price'])
            )
        
        return df

# ============================================
# ADVANCED METRICS CALCULATOR
# ============================================

class AdvancedMetrics:
    """Calculate advanced metrics and indicators"""
    
    @staticmethod
    def calculate_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all advanced metrics"""
        
        if all(col in df.columns for col in ['price', 'volume_1d', 'rvol']):
            df['money_flow_mm'] = (df['price'].fillna(0) * df['volume_1d'].fillna(0) * df['rvol'].fillna(1.0)) / 1_000_000
        else:
            df['money_flow_mm'] = np.nan
        
        if all(col in df.columns for col in ['vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'vol_ratio_90d_180d']):
            df['vmi'] = (
                df['vol_ratio_1d_90d'].fillna(1.0) * 4 +
                df['vol_ratio_7d_90d'].fillna(1.0) * 3 +
                df['vol_ratio_30d_90d'].fillna(1.0) * 2 +
                df['vol_ratio_90d_180d'].fillna(1.0) * 1
            ) / 10
        else:
            df['vmi'] = np.nan
        
        if all(col in df.columns for col in ['from_low_pct', 'from_high_pct']):
            df['position_tension'] = df['from_low_pct'].fillna(0) + abs(df['from_high_pct'].fillna(0))
        else:
            df['position_tension'] = np.nan
        
        df['momentum_harmony'] = 0
        
        if 'ret_1d' in df.columns:
            df['momentum_harmony'] += (df['ret_1d'].fillna(0) > 0).astype(int)
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d']):
            daily_ret_7d = df['ret_7d'].fillna(0) / 7
            daily_ret_30d = df['ret_30d'].fillna(0) / 30
            df['momentum_harmony'] += (daily_ret_7d > daily_ret_30d).astype(int)
        
        if all(col in df.columns for col in ['ret_30d', 'ret_3m']):
            daily_ret_30d_comp = df['ret_30d'].fillna(0) / 30
            daily_ret_3m_comp = df['ret_3m'].fillna(0) / 90
            df['momentum_harmony'] += (daily_ret_30d_comp > daily_ret_3m_comp).astype(int)
        
        if 'ret_3m' in df.columns:
            df['momentum_harmony'] += (df['ret_3m'].fillna(0) > 0).astype(int)
        
        df['wave_state'] = df.apply(AdvancedMetrics._get_wave_state, axis=1)

        if all(col in df.columns for col in ['momentum_score', 'acceleration_score', 'rvol_score', 'breakout_score']):
            df['overall_wave_strength'] = (
                df['momentum_score'].fillna(0) * 0.3 +
                df['acceleration_score'].fillna(0) * 0.3 +
                df['rvol_score'].fillna(0) * 0.2 +
                df['breakout_score'].fillna(0) * 0.2
            )
        else:
            df['overall_wave_strength'] = np.nan
        
        return df
    
    @staticmethod
    def _get_wave_state(row: pd.Series) -> str:
        """Determine wave state for a stock"""
        signals = 0
        
        if row.get('momentum_score', 0) > 70:
            signals += 1
        if row.get('volume_score', 0) > 70:
            signals += 1
        if row.get('acceleration_score', 0) > 70:
            signals += 1
        if row.get('rvol', 0) > 2:
            signals += 1
        
        if signals >= 4:
            return "ðŸŒŠðŸŒŠðŸŒŠ CRESTING"
        elif signals >= 3:
            return "ðŸŒŠðŸŒŠ BUILDING"
        elif signals >= 1:
            return "ðŸŒŠ FORMING"
        else:
            return "ðŸ’¥ BREAKING"

# ============================================
# RANKING ENGINE - OPTIMIZED
# ============================================

class RankingEngine:
    """Core ranking calculations - optimized with numpy"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def calculate_all_scores(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all component scores and master score"""
        
        if df.empty:
            return df
        
        logger.info("Starting optimized ranking calculations...")
        
        df['position_score'] = RankingEngine._calculate_position_score(df)
        df['volume_score'] = RankingEngine._calculate_volume_score(df)
        df['momentum_score'] = RankingEngine._calculate_momentum_score(df)
        df['acceleration_score'] = RankingEngine._calculate_acceleration_score(df)
        df['breakout_score'] = RankingEngine._calculate_breakout_score(df)
        df['rvol_score'] = RankingEngine._calculate_rvol_score(df)
        
        df['trend_quality'] = RankingEngine._calculate_trend_quality(df)
        df['long_term_strength'] = RankingEngine._calculate_long_term_strength(df)
        df['liquidity_score'] = RankingEngine._calculate_liquidity_score(df)
        
        scores_matrix = np.column_stack([
            df['position_score'].fillna(50),
            df['volume_score'].fillna(50),
            df['momentum_score'].fillna(50),
            df['acceleration_score'].fillna(50),
            df['breakout_score'].fillna(50),
            df['rvol_score'].fillna(50)
        ])
        
        weights = np.array([
            CONFIG.POSITION_WEIGHT,
            CONFIG.VOLUME_WEIGHT,
            CONFIG.MOMENTUM_WEIGHT,
            CONFIG.ACCELERATION_WEIGHT,
            CONFIG.BREAKOUT_WEIGHT,
            CONFIG.RVOL_WEIGHT
        ])
        
        df['master_score'] = np.dot(scores_matrix, weights).clip(0, 100)
        
        df['rank'] = df['master_score'].rank(method='first', ascending=False, na_option='bottom')
        df['rank'] = df['rank'].fillna(len(df) + 1).astype(int)
        
        df['percentile'] = df['master_score'].rank(pct=True, ascending=True, na_option='bottom') * 100
        df['percentile'] = df['percentile'].fillna(0)
        
        df = RankingEngine._calculate_category_ranks(df)
        
        logger.info(f"Ranking complete: {len(df)} stocks processed")
        
        return df
    
    @staticmethod
    def _safe_rank(series: pd.Series, pct: bool = True, ascending: bool = True) -> pd.Series:
        """Safely rank a series with proper edge case handling for NaNs and Infs."""
        if series is None or series.empty:
            return pd.Series(np.nan, dtype=float)
        
        series = series.replace([np.inf, -np.inf], np.nan)
        
        valid_count = series.notna().sum()
        if valid_count == 0:
            return pd.Series(np.nan, index=series.index)
        
        if pct:
            ranks = series.rank(pct=True, ascending=ascending, na_option='bottom') * 100
        else:
            ranks = series.rank(ascending=ascending, method='min', na_option='bottom')
        
        return ranks
    
    @staticmethod
    def _calculate_position_score(df: pd.DataFrame) -> pd.Series:
        """Calculate position score from 52-week range"""
        position_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        has_from_low = 'from_low_pct' in df.columns and df['from_low_pct'].notna().any()
        has_from_high = 'from_high_pct' in df.columns and df['from_high_pct'].notna().any()
        
        if not has_from_low and not has_from_high:
            logger.warning("No position data available, position scores will be NaN.")
            return position_score
        
        from_low = df['from_low_pct'] if has_from_low else pd.Series(np.nan, index=df.index)
        from_high = df['from_high_pct'] if has_from_high else pd.Series(np.nan, index=df.index)
        
        rank_from_low = RankingEngine._safe_rank(from_low, pct=True, ascending=True) if has_from_low else pd.Series(np.nan, index=df.index)
        rank_from_high = RankingEngine._safe_rank(from_high, pct=True, ascending=False) if has_from_high else pd.Series(np.nan, index=df.index)
        
        combined_score = (rank_from_low.fillna(50) * 0.6 + rank_from_high.fillna(50) * 0.4)
        
        all_nan_mask = from_low.isna() & from_high.isna()
        combined_score.loc[all_nan_mask] = np.nan

        return combined_score.clip(0, 100)
    
    @staticmethod
    def _calculate_volume_score(df: pd.DataFrame) -> pd.Series:
        """Calculate comprehensive volume score"""
        volume_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        vol_cols = [
            ('vol_ratio_1d_90d', 0.20),
            ('vol_ratio_7d_90d', 0.20),
            ('vol_ratio_30d_90d', 0.20),
            ('vol_ratio_30d_180d', 0.15),
            ('vol_ratio_90d_180d', 0.25)
        ]
        
        total_weight = 0
        weighted_score = pd.Series(0.0, index=df.index, dtype=float)
        
        all_vol_cols_exist = True
        for col, weight in vol_cols:
            if col in df.columns and df[col].notna().any():
                col_rank = RankingEngine._safe_rank(df[col], pct=True, ascending=True)
                weighted_score += col_rank.fillna(0) * weight
                total_weight += weight
            else:
                all_vol_cols_exist = False
        
        if total_weight > 0:
            volume_score = weighted_score / total_weight
            nan_mask = df[[col for col, _ in vol_cols if col in df.columns]].isna().all(axis=1)
            volume_score[nan_mask] = np.nan
        else:
            logger.warning("No valid volume ratio data available, volume scores will be NaN.")
        
        return volume_score.clip(0, 100)
    
    @staticmethod
    def _calculate_momentum_score(df: pd.DataFrame) -> pd.Series:
        """Calculate momentum score based on returns, propagating NaN properly."""
        momentum_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        has_ret_30d = 'ret_30d' in df.columns and df['ret_30d'].notna().any()
        has_ret_7d = 'ret_7d' in df.columns and df['ret_7d'].notna().any()
        
        if not has_ret_30d and not has_ret_7d:
            logger.warning("No return data available for momentum calculation, scores will be NaN.")
            return momentum_score
        
        ret_30d = df['ret_30d'] if has_ret_30d else pd.Series(np.nan, index=df.index)
        ret_7d = df['ret_7d'] if has_ret_7d else pd.Series(np.nan, index=df.index)
        
        if has_ret_30d:
            momentum_score = RankingEngine._safe_rank(ret_30d, pct=True, ascending=True)
        elif has_ret_7d:
            momentum_score = RankingEngine._safe_rank(ret_7d, pct=True, ascending=True)
            logger.info("Using 7-day returns for momentum score due to missing 30-day data.")
        
        if has_ret_7d and has_ret_30d:
            consistency_bonus = pd.Series(0, index=df.index, dtype=float)
            
            all_positive = (ret_7d.fillna(-1) > 0) & (ret_30d.fillna(-1) > 0)
            consistency_bonus[all_positive] = 5
            
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_ret_7d = np.where(ret_7d.fillna(0) != 0, ret_7d.fillna(0) / 7, np.nan)
                daily_ret_30d = np.where(ret_30d.fillna(0) != 0, ret_30d.fillna(0) / 30, np.nan)
            
            accelerating_mask = all_positive & (daily_ret_7d.fillna(-np.inf) > daily_ret_30d.fillna(-np.inf))
            consistency_bonus[accelerating_mask] = 10
            
            momentum_score = (momentum_score.fillna(50) + consistency_bonus).clip(0, 100)
            
            nan_mask = ret_30d.isna() & ret_7d.isna()
            momentum_score.loc[nan_mask] = np.nan
        
        return momentum_score
    
    @staticmethod
    def _calculate_acceleration_score(df: pd.DataFrame) -> pd.Series:
        """Calculate if momentum is accelerating, handling NaNs from division properly."""
        acceleration_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        req_cols = ['ret_1d', 'ret_7d', 'ret_30d']
        
        ret_1d = df['ret_1d'] if 'ret_1d' in df.columns else pd.Series(np.nan, index=df.index)
        ret_7d = df['ret_7d'] if 'ret_7d' in df.columns else pd.Series(np.nan, index=df.index)
        ret_30d = df['ret_30d'] if 'ret_30d' in df.columns else pd.Series(np.nan, index=df.index)
        
        has_all_data = ret_1d.notna() & ret_7d.notna() & ret_30d.notna()
        
        if not has_all_data.any():
            logger.warning("Insufficient return data for acceleration calculation, scores will be NaN.")
            return acceleration_score
        
        with np.errstate(divide='ignore', invalid='ignore'):
            avg_daily_1d = ret_1d
            avg_daily_7d = np.where(ret_7d.fillna(0) != 0, ret_7d.fillna(0) / 7, np.nan)
            avg_daily_30d = np.where(ret_30d.fillna(0) != 0, ret_30d.fillna(0) / 30, np.nan)
        
        acceleration_score.loc[has_all_data] = 50.0
        
        if has_all_data.any():
            perfect = has_all_data & (avg_daily_1d > avg_daily_7d) & (avg_daily_7d > avg_daily_30d) & (ret_1d > 0)
            acceleration_score.loc[perfect] = 100
            
            good = has_all_data & (~perfect) & (avg_daily_1d > avg_daily_7d) & (ret_1d > 0)
            acceleration_score.loc[good] = 80
            
            moderate = has_all_data & (~perfect) & (~good) & (ret_1d > 0)
            acceleration_score.loc[moderate] = 60
            
            slight_decel = has_all_data & (ret_1d <= 0) & (ret_7d > 0)
            acceleration_score.loc[slight_decel] = 40
            
            strong_decel = has_all_data & (ret_1d <= 0) & (ret_7d <= 0)
            acceleration_score.loc[strong_decel] = 20
        
        return acceleration_score.clip(0, 100)
    
    @staticmethod
    def _calculate_breakout_score(df: pd.DataFrame) -> pd.Series:
        """Calculate breakout probability, propagating NaN."""
        breakout_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        if not all(col in df.columns for col in ['from_high_pct', 'vol_ratio_7d_90d']):
            return breakout_score

        distance_factor = pd.Series(np.nan, index=df.index)
        distance_from_high = -df['from_high_pct']
        distance_factor = (100 - distance_from_high).clip(0, 100)

        volume_factor = pd.Series(np.nan, index=df.index)
        vol_ratio = df['vol_ratio_7d_90d']
        volume_factor = ((vol_ratio - 1) * 100).clip(0, 100)

        trend_factor = pd.Series(np.nan, index=df.index, dtype=float)
        if 'price' in df.columns:
            current_price = df['price']
            sma_cols = ['sma_20d', 'sma_50d', 'sma_200d']
            
            conditions_sum = pd.Series(0, index=df.index, dtype=float)
            valid_sma_count = pd.Series(0, index=df.index, dtype=int)

            for sma_col in sma_cols:
                if sma_col in df.columns and df[sma_col].notna().any():
                    has_data = current_price.notna() & df[sma_col].notna()
                    conditions_sum.loc[has_data] += (current_price.loc[has_data] > df[sma_col].loc[has_data]).astype(float)
                    valid_sma_count.loc[has_data] += 1
            
            trend_factor.loc[valid_sma_count > 0] = (conditions_sum.loc[valid_sma_count > 0] / valid_sma_count.loc[valid_sma_count > 0]) * 100
        
        trend_factor = trend_factor.clip(0, 100)
        
        combined_score = (
            distance_factor.fillna(50) * 0.4 +
            volume_factor.fillna(50) * 0.4 +
            trend_factor.fillna(50) * 0.2
        )
        
        all_nan_mask = distance_factor.isna() & volume_factor.isna() & trend_factor.isna()
        combined_score.loc[all_nan_mask] = np.nan

        return combined_score.clip(0, 100)
    
    @staticmethod
    def _calculate_rvol_score(df: pd.DataFrame) -> pd.Series:
        """Calculate RVOL-based score, propagating NaN."""
        if 'rvol' not in df.columns or df['rvol'].isna().all():
            return pd.Series(np.nan, index=df.index)
        
        rvol = df['rvol']
        rvol_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        has_rvol = rvol.notna()
        
        rvol_score.loc[has_rvol & (rvol > 10)] = 95
        rvol_score.loc[has_rvol & (rvol > 5) & (rvol <= 10)] = 90
        rvol_score.loc[has_rvol & (rvol > 3) & (rvol <= 5)] = 85
        rvol_score.loc[has_rvol & (rvol > 2) & (rvol <= 3)] = 80
        rvol_score.loc[has_rvol & (rvol > 1.5) & (rvol <= 2)] = 70
        rvol_score.loc[has_rvol & (rvol > 1.2) & (rvol <= 1.5)] = 60
        rvol_score.loc[has_rvol & (rvol > 0.8) & (rvol <= 1.2)] = 50
        rvol_score.loc[has_rvol & (rvol > 0.5) & (rvol <= 0.8)] = 40
        rvol_score.loc[has_rvol & (rvol > 0.3) & (rvol <= 0.5)] = 30
        rvol_score.loc[has_rvol & (rvol <= 0.3)] = 20
        
        return rvol_score.clip(0, 100)
    
    @staticmethod
    def _calculate_trend_quality(df: pd.DataFrame) -> pd.Series:
        """Calculate trend quality score based on SMA alignment, propagating NaN."""
        trend_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        if 'price' not in df.columns or df['price'].isna().all():
            return trend_score
        
        current_price = df['price']
        sma_cols = ['sma_20d', 'sma_50d', 'sma_200d']
        
        above_sma_count = pd.Series(0, index=df.index, dtype=int)
        rows_with_any_sma_data = pd.Series(False, index=df.index, dtype=bool)

        for sma_col in sma_cols:
            if sma_col in df.columns and df[sma_col].notna().any():
                valid_comparison_mask = current_price.notna() & df[sma_col].notna()
                above_sma_count.loc[valid_comparison_mask] += (current_price.loc[valid_comparison_mask] > df[sma_col].loc[valid_comparison_mask]).astype(int)
                rows_with_any_sma_data.loc[valid_comparison_mask] = True
        
        rows_to_score = df.index[rows_with_any_sma_data]
        
        if len(rows_to_score) > 0:
            trend_score.loc[rows_to_score] = 50.0

            if all(col in df.columns for col in sma_cols):
                perfect_trend = (
                    (current_price > df['sma_20d']) & 
                    (df['sma_20d'] > df['sma_50d']) & 
                    (df['sma_50d'] > df['sma_200d'])
                ).fillna(False)
                trend_score.loc[perfect_trend] = 100
                
                strong_trend = (
                    (~perfect_trend) &
                    (current_price > df['sma_20d']) & 
                    (current_price > df['sma_50d']) & 
                    (current_price > df['sma_200d'])
                ).fillna(False)
                trend_score.loc[strong_trend] = 85
            
            good_trend = rows_with_any_sma_data & (above_sma_count == 2) & ~trend_score.notna()
            trend_score.loc[good_trend] = 70
            
            weak_trend = rows_with_any_sma_data & (above_sma_count == 1) & ~trend_score.notna()
            trend_score.loc[weak_trend] = 40
            
            poor_trend = rows_with_any_sma_data & (above_sma_count == 0) & ~trend_score.notna()
            trend_score.loc[poor_trend] = 20

        return trend_score.clip(0, 100)
    
    @staticmethod
    def _calculate_long_term_strength(df: pd.DataFrame) -> pd.Series:
        """Calculate long-term strength score, propagating NaN."""
        strength_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        lt_cols = ['ret_3m', 'ret_6m', 'ret_1y']
        available_cols = [col for col in lt_cols if col in df.columns and df[col].notna().any()]
        
        if not available_cols:
            return strength_score
        
        lt_returns = df[available_cols]
        avg_return = lt_returns.mean(axis=1)
        
        has_any_lt_data = df[available_cols].notna().any(axis=1)
        
        strength_score.loc[has_any_lt_data & (avg_return > 100)] = 100
        strength_score.loc[has_any_lt_data & (avg_return > 50) & (avg_return <= 100)] = 90
        strength_score.loc[has_any_lt_data & (avg_return > 30) & (avg_return <= 50)] = 80
        strength_score.loc[has_any_lt_data & (avg_return > 15) & (avg_return <= 30)] = 70
        strength_score.loc[has_any_lt_data & (avg_return > 5) & (avg_return <= 15)] = 60
        strength_score.loc[has_any_lt_data & (avg_return > 0) & (avg_return <= 5)] = 50
        strength_score.loc[has_any_lt_data & (avg_return > -10) & (avg_return <= 0)] = 40
        strength_score.loc[has_any_lt_data & (avg_return > -25) & (avg_return <= -10)] = 30
        strength_score.loc[has_any_lt_data & (avg_return <= -25)] = 20
        
        return strength_score.clip(0, 100)
    
    @staticmethod
    def _calculate_liquidity_score(df: pd.DataFrame) -> pd.Series:
        """Calculate liquidity score based on trading volume, propagating NaN."""
        liquidity_score = pd.Series(np.nan, index=df.index, dtype=float)
        
        if 'volume_30d' in df.columns and 'price' in df.columns:
            dollar_volume = df['volume_30d'].fillna(0) * df['price'].fillna(0)
            has_valid_dollar_volume = dollar_volume.notna() & (dollar_volume > 0)
            
            if has_valid_dollar_volume.any():
                liquidity_score.loc[has_valid_dollar_volume] = RankingEngine._safe_rank(
                    dollar_volume.loc[has_valid_dollar_volume], pct=True, ascending=True
                )
        
        return liquidity_score.clip(0, 100)
    
    @staticmethod
    def _calculate_category_ranks(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate percentile ranks within each category"""
        df['category_rank'] = np.nan
        df['category_percentile'] = np.nan
        
        categories = df['category'].dropna().unique()
        
        for category in categories:
            mask = df['category'] == category
            cat_df = df[mask]
            
            if not cat_df.empty and 'master_score' in cat_df.columns and cat_df['master_score'].notna().any():
                cat_ranks = cat_df['master_score'].rank(method='first', ascending=False, na_option='bottom')
                df.loc[mask, 'category_rank'] = cat_ranks.astype(int)
                
                cat_percentiles = cat_df['master_score'].rank(pct=True, ascending=True, na_option='bottom') * 100
                df.loc[mask, 'category_percentile'] = cat_percentiles
        
        return df

# ============================================
# PATTERN DETECTION ENGINE - OPTIMIZED
# ============================================

class PatternDetector:
    """Detect all patterns using vectorized operations"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def detect_all_patterns(df: pd.DataFrame) -> pd.DataFrame:
        """Detect all 25 patterns efficiently using vectorized pandas operations."""
        
        if df.empty:
            df['patterns'] = [''] * len(df)
            return df

        patterns_with_masks = PatternDetector._get_all_pattern_definitions(df)
        
        if not patterns_with_masks:
            df['patterns'] = [''] * len(df)
            return df

        # Create a boolean DataFrame from the list of (name, mask) tuples
        pattern_matrix = pd.DataFrame({name: mask for name, mask in patterns_with_masks})
        
        # Apply the logic to generate the combined string for each row
        df['patterns'] = pattern_matrix.apply(
            lambda row: ' | '.join(row.index[row].tolist()), axis=1
        )
        
        return df
    
    @staticmethod
    def _get_all_pattern_definitions(df: pd.DataFrame) -> List[Tuple[str, pd.Series]]:
        """
        Get all pattern definitions with masks.
        Returns a list of (pattern_name: str, mask: pd.Series) tuples.
        """
        patterns = []
        
        # Helper to safely get column with NaNs filled to False for logical checks
        def get_col_safe(col_name: str, fill_value: Any = False) -> pd.Series:
            return df[col_name].fillna(fill_value) if col_name in df.columns else pd.Series(fill_value, index=df.index)

        # Helper to check if a condition can be meaningfully evaluated on a column
        def can_evaluate(col_name):
            return col_name in df.columns and df[col_name].notna().any()

        # 1. Category Leader
        if can_evaluate('category_percentile'):
            mask = get_col_safe('category_percentile', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ”¥ CAT LEADER']
            patterns.append(('ðŸ”¥ CAT LEADER', mask))
        
        # 2. Hidden Gem
        if can_evaluate('category_percentile') and can_evaluate('percentile'):
            mask = (
                (get_col_safe('category_percentile', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ’Ž HIDDEN GEM']) & 
                (get_col_safe('percentile', 100) < 70)
            )
            patterns.append(('ðŸ’Ž HIDDEN GEM', mask))
        
        # 3. Accelerating
        if can_evaluate('acceleration_score'):
            mask = get_col_safe('acceleration_score', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸš€ ACCELERATING']
            patterns.append(('ðŸš€ ACCELERATING', mask))
        
        # 4. Institutional
        if can_evaluate('volume_score') and can_evaluate('vol_ratio_90d_180d'):
            mask = (
                (get_col_safe('volume_score', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ¦ INSTITUTIONAL']) &
                (get_col_safe('vol_ratio_90d_180d', 0) > 1.1)
            )
            patterns.append(('ðŸ¦ INSTITUTIONAL', mask))
        
        # 5. Volume Explosion
        if can_evaluate('rvol'):
            mask = get_col_safe('rvol', 0) > 3
            patterns.append(('âš¡ VOL EXPLOSION', mask))
        
        # 6. Breakout Ready
        if can_evaluate('breakout_score'):
            mask = get_col_safe('breakout_score', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸŽ¯ BREAKOUT']
            patterns.append(('ðŸŽ¯ BREAKOUT', mask))
        
        # 7. Market Leader
        if can_evaluate('percentile'):
            mask = get_col_safe('percentile', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ‘‘ MARKET LEADER']
            patterns.append(('ðŸ‘‘ MARKET LEADER', mask))
        
        # 8. Momentum Wave
        if can_evaluate('momentum_score') and can_evaluate('acceleration_score'):
            mask = (
                (get_col_safe('momentum_score', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸŒŠ MOMENTUM WAVE']) &
                (get_col_safe('acceleration_score', 0) >= 70)
            )
            patterns.append(('ðŸŒŠ MOMENTUM WAVE', mask))
        
        # 9. Liquid Leader
        if can_evaluate('liquidity_score') and can_evaluate('percentile'):
            mask = (
                (get_col_safe('liquidity_score', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ’° LIQUID LEADER']) &
                (get_col_safe('percentile', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ’° LIQUID LEADER'])
            )
            patterns.append(('ðŸ’° LIQUID LEADER', mask))
        
        # 10. Long-term Strength
        if can_evaluate('long_term_strength'):
            mask = get_col_safe('long_term_strength', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ’ª LONG STRENGTH']
            patterns.append(('ðŸ’ª LONG STRENGTH', mask))
        
        # 11. Quality Trend
        if can_evaluate('trend_quality'):
            mask = get_col_safe('trend_quality', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ“ˆ QUALITY TREND']
            patterns.append(('ðŸ“ˆ QUALITY TREND', mask))
        
        # 12. Value Momentum (Fundamental)
        if can_evaluate('pe') and can_evaluate('master_score'):
            has_valid_pe = get_col_safe('pe', np.nan).notna() & (get_col_safe('pe', 0) > 0) & (get_col_safe('pe', 10000) < 10000)
            mask = has_valid_pe & (get_col_safe('pe', 0) < 15) & (get_col_safe('master_score', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ’Ž VALUE MOMENTUM'])
            patterns.append(('ðŸ’Ž VALUE MOMENTUM', mask))
        
        # 13. Earnings Rocket
        if can_evaluate('eps_change_pct') and can_evaluate('acceleration_score'):
            eps_chg = get_col_safe('eps_change_pct', 0)
            accel_score = get_col_safe('acceleration_score', 0)
            mask = ((eps_chg > 1000) & (accel_score >= 80)) | ((eps_chg > 50) & (eps_chg <= 1000) & (accel_score >= 70))
            patterns.append(('ðŸ“Š EARNINGS ROCKET', mask))
        
        # 14. Quality Leader
        if can_evaluate('pe') and can_evaluate('eps_change_pct') and can_evaluate('percentile'):
            mask = (
                (get_col_safe('pe', np.nan).between(10, 25)) &
                (get_col_safe('eps_change_pct', 0) > 20) &
                (get_col_safe('percentile', 0) >= CONFIG.PATTERN_THRESHOLDS['ðŸ† QUALITY LEADER'])
            )
            patterns.append(('ðŸ† QUALITY LEADER', mask))
        
        # 15. Turnaround Play
        if can_evaluate('eps_change_pct') and can_evaluate('volume_score'):
            eps_chg = get_col_safe('eps_change_pct', 0)
            vol_score = get_col_safe('volume_score', 0)
            mask = ((eps_chg > 500) & (vol_score >= 60)) | ((eps_chg > 100) & (eps_chg <= 500) & (vol_score >= 70))
            patterns.append(('âš¡ TURNAROUND', mask))
        
        # 16. High PE Warning
        if can_evaluate('pe'):
            mask = (get_col_safe('pe', 0) > CONFIG.PATTERN_THRESHOLDS['âš ï¸ HIGH PE'])
            patterns.append(('âš ï¸ HIGH PE', mask))
        
        # 17. 52W High Approach
        if can_evaluate('from_high_pct') and can_evaluate('volume_score') and can_evaluate('momentum_score'):
            mask = (
                (get_col_safe('from_high_pct', -100) > -5) & 
                (get_col_safe('volume_score', 0) >= 70) & 
                (get_col_safe('momentum_score', 0) >= 60)
            )
            patterns.append(('ðŸŽ¯ 52W HIGH APPROACH', mask))
        
        # 18. 52W Low Bounce
        if can_evaluate('from_low_pct') and can_evaluate('acceleration_score') and can_evaluate('ret_30d'):
            mask = (
                (get_col_safe('from_low_pct', 100) < 20) & 
                (get_col_safe('acceleration_score', 0) >= 80) & 
                (get_col_safe('ret_30d', 0) > 10)
            )
            patterns.append(('ðŸ”„ 52W LOW BOUNCE', mask))
        
        # 19. Golden Zone
        if can_evaluate('from_low_pct') and can_evaluate('from_high_pct') and can_evaluate('trend_quality'):
            mask = (
                (get_col_safe('from_low_pct', 0) > 60) & 
                (get_col_safe('from_high_pct', 0) > -40) & 
                (get_col_safe('trend_quality', 0) >= 70)
            )
            patterns.append(('ðŸ‘‘ GOLDEN ZONE', mask))
        
        # 20. Volume Accumulation
        if can_evaluate('vol_ratio_30d_90d') and can_evaluate('vol_ratio_90d_180d') and can_evaluate('ret_30d'):
            mask = (
                (get_col_safe('vol_ratio_30d_90d', 0) > 1.2) & 
                (get_col_safe('vol_ratio_90d_180d', 0) > 1.1) & 
                (get_col_safe('ret_30d', 0) > 5)
            )
            patterns.append(('ðŸ“Š VOL ACCUMULATION', mask))
        
        # 21. Momentum Divergence
        if all(can_evaluate(c) for c in ['ret_7d', 'ret_30d', 'acceleration_score', 'rvol']):
            daily_7d_pace = get_col_safe('ret_7d', 0) / 7
            daily_30d_pace = get_col_safe('ret_30d', 0) / 30
            mask = (
                (daily_7d_pace > daily_30d_pace * 1.5) & 
                (get_col_safe('acceleration_score', 0) >= 85) & 
                (get_col_safe('rvol', 0) > 2)
            )
            patterns.append(('ðŸ”€ MOMENTUM DIVERGE', mask))
        
        # 22. Range Compression
        if all(can_evaluate(c) for c in ['high_52w', 'low_52w', 'from_low_pct']):
            range_pct = (get_col_safe('high_52w', 0) - get_col_safe('low_52w', 0)) / get_col_safe('low_52w', 1e-6) * 100
            mask = (range_pct.replace([np.inf, -np.inf], 100) < 50) & (get_col_safe('from_low_pct', 0) > 30)
            patterns.append(('ðŸŽ¯ RANGE COMPRESS', mask))
        
        # 23. Stealth Accumulator (NEW)
        if all(can_evaluate(c) for c in ['vol_ratio_90d_180d', 'vol_ratio_30d_90d', 'from_low_pct', 'ret_7d', 'ret_30d']):
            ret_ratio = get_col_safe('ret_7d', 0) / (get_col_safe('ret_30d', 1e-6) / 4)
            mask = (
                (get_col_safe('vol_ratio_90d_180d', 0) > 1.1) &
                (get_col_safe('vol_ratio_30d_90d', 0).between(0.9, 1.1)) &
                (get_col_safe('from_low_pct', 0) > 40) &
                (ret_ratio.replace([np.inf, -np.inf], 0) > 1)
            )
            patterns.append(('ðŸ¤« STEALTH', mask))
        
        # 24. Momentum Vampire (NEW)
        if all(can_evaluate(c) for c in ['ret_1d', 'ret_7d', 'rvol', 'from_high_pct', 'category']):
            daily_pace_ratio = get_col_safe('ret_1d', 0) / (get_col_safe('ret_7d', 1e-6) / 7)
            mask = (
                (daily_pace_ratio.replace([np.inf, -np.inf], 0) > 2) &
                (get_col_safe('rvol', 0) > 3) &
                (get_col_safe('from_high_pct', -100) > -15) &
                (df['category'].isin(['Small Cap', 'Micro Cap']))
            )
            patterns.append(('ðŸ§› VAMPIRE', mask))
        
        # 25. Perfect Storm (NEW)
        if can_evaluate('momentum_harmony') and can_evaluate('master_score'):
            mask = (
                (get_col_safe('momentum_harmony', 0) == 4) &
                (get_col_safe('master_score', 0) > 80)
            )
            patterns.append(('â›ˆï¸ PERFECT STORM', mask))
        
        return patterns

# ============================================
# MARKET INTELLIGENCE
# ============================================

class MarketIntelligence:
    """Advanced market analysis and regime detection"""
    
    @staticmethod
    def detect_market_regime(df: pd.DataFrame) -> Tuple[str, Dict[str, Any]]:
        """Detect current market regime with supporting data"""
        
        if df.empty:
            return "ðŸ˜´ NO DATA", {}
        
        metrics = {}
        
        if 'category' in df.columns and 'master_score' in df.columns:
            category_scores = df.groupby('category')['master_score'].mean().fillna(50) # Neutral fill for regime
            
            micro_small_avg = category_scores[category_scores.index.str.contains('Micro|Small')].mean()
            large_mega_avg = category_scores[category_scores.index.str.contains('Large|Mega')].mean()
            
            metrics['micro_small_avg'] = micro_small_avg
            metrics['large_mega_avg'] = large_mega_avg
            metrics['category_spread'] = micro_small_avg - large_mega_avg
        else:
            micro_small_avg = 50
            large_mega_avg = 50
        
        if 'ret_30d' in df.columns:
            breadth = len(df[df['ret_30d'].fillna(0) > 0]) / len(df) if len(df) > 0 else 0
            metrics['breadth'] = breadth
        else:
            breadth = 0.5
        
        if 'rvol' in df.columns:
            avg_rvol = df['rvol'].fillna(1.0).median()
            metrics['avg_rvol'] = avg_rvol
        else:
            avg_rvol = 1.0
        
        if micro_small_avg > large_mega_avg + 10 and breadth > 0.6:
            regime = "ðŸ”¥ RISK-ON BULL"
        elif large_mega_avg > micro_small_avg + 10 and breadth < 0.4:
            regime = "ðŸ›¡ï¸ RISK-OFF DEFENSIVE"
        elif avg_rvol > 1.5 and breadth > 0.5:
            regime = "âš¡ VOLATILE OPPORTUNITY"
        else:
            regime = "ðŸ˜´ RANGE-BOUND"
        
        metrics['regime'] = regime
        
        return regime, metrics
    
    @staticmethod
    def calculate_advance_decline_ratio(df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate advance/decline ratio and related metrics"""
        
        ad_metrics = {}
        
        if 'ret_1d' in df.columns:
            advancing = len(df[df['ret_1d'].fillna(0) > 0])
            declining = len(df[df['ret_1d'].fillna(0) < 0])
            unchanged = len(df[df['ret_1d'].fillna(0) == 0])
            
            ad_metrics['advancing'] = advancing
            ad_metrics['declining'] = declining
            ad_metrics['unchanged'] = unchanged
            
            if declining > 0:
                ad_metrics['ad_ratio'] = advancing / declining
            else:
                ad_metrics['ad_ratio'] = float('inf') if advancing > 0 else 1.0
            
            ad_metrics['ad_line'] = advancing - declining
            ad_metrics['breadth_pct'] = (advancing / len(df)) * 100 if len(df) > 0 else 0
        
        return ad_metrics
    
    @staticmethod
    def _apply_dynamic_sampling(df_group: pd.DataFrame) -> pd.DataFrame:
        """Helper to apply dynamic sampling based on group size."""
        group_size = len(df_group)
        
        if 1 <= group_size <= 5:
            sample_count = group_size
        elif 6 <= group_size <= 20:
            sample_count = max(1, int(group_size * 0.80))
        elif 21 <= group_size <= 50:
            sample_count = max(1, int(group_size * 0.60))
        elif 51 <= group_size <= 100:
            sample_count = max(1, int(group_size * 0.40))
        else:
            sample_count = min(50, int(group_size * 0.25))
        
        if sample_count > 0 and 'master_score' in df_group.columns:
            # Fillna(0) for master_score to handle NaNs consistently during nlargest
            return df_group.nlargest(sample_count, 'master_score', keep='first')
        else:
            return pd.DataFrame()

    @staticmethod
    def _calculate_flow_metrics(normalized_df: pd.DataFrame, group_col: str) -> pd.DataFrame:
        """Helper to calculate common flow metrics for sector/industry rotation."""
        agg_dict = {
            'master_score': ['mean', 'median', 'std', 'count'],
            'momentum_score': 'mean',
            'volume_score': 'mean',
            'rvol': 'mean',
            'ret_30d': 'mean',
            'money_flow_mm': 'sum'
        }
        
        available_agg_dict = {
            k: v for k, v in agg_dict.items() if k in normalized_df.columns
        }

        group_metrics = normalized_df.groupby(group_col).agg(available_agg_dict).round(2)
        
        group_metrics.columns = ['_'.join(col).strip() for col in group_metrics.columns.values]
        
        rename_map = {
            'master_score_mean': 'avg_score',
            'master_score_median': 'median_score',
            'master_score_std': 'std_score',
            'master_score_count': 'count',
            'momentum_score_mean': 'avg_momentum',
            'volume_score_mean': 'avg_volume',
            'rvol_mean': 'avg_rvol',
            'ret_30d_mean': 'avg_ret_30d',
            'money_flow_mm_sum': 'total_money_flow'
        }
        group_metrics = group_metrics.rename(columns=rename_map)

        group_metrics['flow_score'] = (
            group_metrics.get('avg_score', pd.Series(0)) * 0.3 +
            group_metrics.get('median_score', pd.Series(0)) * 0.2 +
            group_metrics.get('avg_momentum', pd.Series(0)) * 0.25 +
            group_metrics.get('avg_volume', pd.Series(0)) * 0.25
        )
        
        group_metrics['rank'] = group_metrics['flow_score'].rank(ascending=False, method='min')
        
        return group_metrics.sort_values('flow_score', ascending=False)

    @staticmethod
    def detect_sector_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect sector rotation patterns with normalized analysis and dynamic sampling.
        """
        if 'sector' not in df.columns or df.empty:
            return pd.DataFrame()
        
        sector_dfs = []
        
        grouped_sectors = df.groupby('sector')
        for sector_name, sector_group_df in grouped_sectors:
            if sector_name != 'Unknown':
                sampled_sector_df = MarketIntelligence._apply_dynamic_sampling(sector_group_df.copy())
                if not sampled_sector_df.empty:
                    sector_dfs.append(sampled_sector_df)
        
        if not sector_dfs:
            return pd.DataFrame()
        
        normalized_df = pd.concat(sector_dfs, ignore_index=True)
        sector_metrics = MarketIntelligence._calculate_flow_metrics(normalized_df, 'sector')
        
        original_counts = df.groupby('sector').size().rename('total_stocks')
        sector_metrics = sector_metrics.join(original_counts, how='left')
        sector_metrics['analyzed_stocks'] = sector_metrics['count']
        
        return sector_metrics

    @staticmethod
    def detect_industry_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect industry rotation patterns with normalized analysis and dynamic sampling.
        """
        if 'industry' not in df.columns or df.empty:
            return pd.DataFrame()
        
        industry_dfs = []
        
        grouped_industries = df.groupby('industry')
        for industry_name, industry_group_df in grouped_industries:
            if industry_name != 'Unknown':
                sampled_industry_df = MarketIntelligence._apply_dynamic_sampling(industry_group_df.copy())
                if not sampled_industry_df.empty:
                    industry_dfs.append(sampled_industry_df)
        
        if not industry_dfs:
            return pd.DataFrame()
        
        normalized_df = pd.concat(industry_dfs, ignore_index=True)
        industry_metrics = MarketIntelligence._calculate_flow_metrics(normalized_df, 'industry')
        
        original_counts = df.groupby('industry').size().rename('total_stocks')
        industry_metrics = industry_metrics.join(original_counts, how='left')
        industry_metrics['analyzed_stocks'] = industry_metrics['count']
        
        return industry_metrics

# ============================================
# VISUALIZATION ENGINE
# ============================================

class Visualizer:
    """Create all visualizations with proper error handling"""
    
    @staticmethod
    def create_score_distribution(df: pd.DataFrame) -> go.Figure:
        """Create score distribution chart"""
        fig = go.Figure()
        
        if df.empty:
            fig.add_annotation(
                text="No data available for visualization",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig
        
        scores = [
            ('position_score', 'Position', '#3498db'),
            ('volume_score', 'Volume', '#e74c3c'),
            ('momentum_score', 'Momentum', '#2ecc71'),
            ('acceleration_score', 'Acceleration', '#f39c12'),
            ('breakout_score', 'Breakout', '#9b59b6'),
            ('rvol_score', 'RVOL', '#e67e22')
        ]
        
        for score_col, label, color in scores:
            if score_col in df.columns:
                score_data = df[score_col].dropna()
                if not score_data.empty:
                    fig.add_trace(go.Box(
                        y=score_data,
                        name=label,
                        marker_color=color,
                        boxpoints='outliers',
                        hovertemplate=f'{label}<br>Score: %{{y:.1f}}<extra></extra>'
                    ))
        
        fig.update_layout(
            title="Score Component Distribution",
            yaxis_title="Score (0-100)",
            template='plotly_white',
            height=400,
            showlegend=False
        )
        
        return fig

    @staticmethod
    def create_acceleration_profiles(df: pd.DataFrame, n: int = 10) -> go.Figure:
        """Create acceleration profiles showing momentum over time"""
        try:
            plot_df = df.dropna(subset=['ret_1d', 'ret_7d', 'ret_30d'], how='any')
            if plot_df.empty:
                logger.info("No stocks with complete return data for acceleration profiles.")
                fig = go.Figure()
                fig.add_annotation(
                    text="No complete return data available for this chart.",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False
                )
                return fig
            
            accel_df = plot_df.nlargest(min(n, len(plot_df)), 'acceleration_score', keep='first')
            
            if accel_df.empty:
                fig = go.Figure()
                fig.add_annotation(
                    text="No stocks meet criteria for acceleration profiles.",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False
                )
                return fig
            
            fig = go.Figure()
            
            for _, stock in accel_df.iterrows():
                x_points = ['Start']
                y_points = [0]
                
                if 'ret_30d' in stock.index and pd.notna(stock['ret_30d']):
                    x_points.append('30D')
                    y_points.append(stock['ret_30d'])
                
                if 'ret_7d' in stock.index and pd.notna(stock['ret_7d']):
                    x_points.append('7D')
                    y_points.append(stock['ret_7d'])
                
                if 'ret_1d' in stock.index and pd.notna(stock['ret_1d']):
                    x_points.append('Today')
                    y_points.append(stock['ret_1d'])
                
                if len(x_points) > 1:
                    accel_score = stock.get('acceleration_score', 0)
                    if accel_score >= 85:
                        line_style = dict(width=3, dash='solid')
                        marker_style = dict(size=10, symbol='star', line=dict(color='DarkSlateGrey', width=1))
                    elif accel_score >= 70:
                        line_style = dict(width=2, dash='solid')
                        marker_style = dict(size=8)
                    else:
                        line_style = dict(width=2, dash='dot')
                        marker_style = dict(size=6)
                    
                    fig.add_trace(go.Scatter(
                        x=x_points,
                        y=y_points,
                        mode='lines+markers',
                        name=f"{stock['ticker']} ({accel_score:.0f})",
                        line=line_style,
                        marker=marker_style,
                        hovertemplate=(
                            f"<b>{stock['ticker']}</b><br>" +
                            "%{x}: %{y:.1f}%<br>" +
                            f"Accel Score: {accel_score:.0f}<extra></extra>"
                        )
                    ))
            
            fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)
            
            fig.update_layout(
                title=f"Acceleration Profiles - Top {len(accel_df)} Momentum Builders",
                xaxis_title="Time Frame",
                yaxis_title="Return %",
                height=400,
                template='plotly_white',
                showlegend=True,
                legend=dict(
                    orientation="v",
                    yanchor="top",
                    y=1,
                    xanchor="left",
                    x=1.02
                ),
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            logger.error(f"Error creating acceleration profiles: {str(e)}")
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error generating chart: {e}",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig

# ============================================
# FILTER ENGINE - OPTIMIZED
# ============================================

class FilterEngine:
    """Handle all filtering operations efficiently"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.2)
    def apply_filters(df: pd.DataFrame, filters: Dict[str, Any]) -> pd.DataFrame:
        """Apply all filters with optimized performance using numpy.logical_and.reduce"""
        
        if df.empty:
            return df
        
        masks = []
        
        categories = filters.get('categories', [])
        if categories and 'category' in df.columns:
            masks.append(df['category'].isin(categories))
        
        sectors = filters.get('sectors', [])
        if sectors and 'sector' in df.columns:
            masks.append(df['sector'].isin(sectors))

        industries = filters.get('industries', [])
        if industries and 'industry' in df.columns:
            masks.append(df['industry'].isin(industries))
        
        min_score = filters.get('min_score', 0)
        if min_score > 0 and 'master_score' in df.columns:
            masks.append(df['master_score'].fillna(0) >= min_score)
        
        min_eps_change = filters.get('min_eps_change')
        if min_eps_change is not None and 'eps_change_pct' in df.columns:
            masks.append(df['eps_change_pct'].notna() & (df['eps_change_pct'] >= min_eps_change))
        
        patterns = filters.get('patterns', [])
        if patterns and 'patterns' in df.columns:
            pattern_regex = '|'.join(re.escape(p) for p in patterns)
            masks.append(df['patterns'].str.contains(pattern_regex, case=False, na=False, regex=True))
        
        trend_range = filters.get('trend_range')
        if filters.get('trend_filter') != 'All Trends' and trend_range and 'trend_quality' in df.columns:
            min_trend, max_trend = trend_range
            masks.append(df['trend_quality'].notna() & (df['trend_quality'] >= min_trend) & (df['trend_quality'] <= max_trend))
        
        min_pe = filters.get('min_pe')
        if min_pe is not None and 'pe' in df.columns:
            masks.append(df['pe'].notna() & (df['pe'] > 0) & (df['pe'] >= min_pe))
        
        max_pe = filters.get('max_pe')
        if max_pe is not None and 'pe' in df.columns:
            masks.append(df['pe'].notna() & (df['pe'] > 0) & (df['pe'] <= max_pe))
        
        for tier_type_key, col_name_suffix in [
            ('eps_tiers', 'eps_tier'),
            ('pe_tiers', 'pe_tier'),
            ('price_tiers', 'price_tier')
        ]:
            tier_values = filters.get(tier_type_key, [])
            col_name = col_name_suffix
            if tier_values and col_name in df.columns:
                masks.append(df[col_name].isin(tier_values))
        
        if filters.get('require_fundamental_data', False):
            if 'pe' in df.columns and 'eps_change_pct' in df.columns:
                masks.append(df['pe'].notna() & (df['pe'] > 0) & df['eps_change_pct'].notna())
            
        wave_states = filters.get('wave_states', [])
        if wave_states and 'wave_state' in df.columns:
            masks.append(df['wave_state'].isin(wave_states))

        wave_strength_range = filters.get('wave_strength_range')
        if wave_strength_range and wave_strength_range != (0, 100) and 'overall_wave_strength' in df.columns:
            min_ws, max_ws = wave_strength_range
            masks.append(df['overall_wave_strength'].notna() & (df['overall_wave_strength'] >= min_ws) & (df['overall_wave_strength'] <= max_ws))

        if masks:
            combined_mask = np.logical_and.reduce(masks)
            filtered_df = df[combined_mask].copy()
        else:
            filtered_df = df.copy()
        
        logger.info(f"Filters reduced {len(df)} to {len(filtered_df)} stocks")
        
        return filtered_df
    
    @staticmethod
    def get_filter_options(df: pd.DataFrame, column: str, current_filters: Dict[str, Any]) -> List[str]:
        """Get available filter options with smart interconnection"""
        
        if df.empty or column not in df.columns:
            return []
        
        temp_filters = current_filters.copy()
        
        filter_key_map = {
            'category': 'categories',
            'sector': 'sectors',
            'industry': 'industries', 
            'eps_tier': 'eps_tiers',
            'pe_tier': 'pe_tiers',
            'price_tier': 'price_tiers',
            'wave_state': 'wave_states'
        }
        
        if column in filter_key_map and filter_key_map[column] in temp_filters:
            temp_filters.pop(filter_key_map[column], None)
        
        filtered_df = FilterEngine.apply_filters(df, temp_filters)
        
        values = filtered_df[column].dropna().astype(str).unique()
        values = [v for v in values if v.strip().lower() not in ['unknown', '', 'nan', 'n/a', 'none', '-']]
        
        return sorted(values)

# ============================================
# SEARCH ENGINE - OPTIMIZED
# ============================================

class SearchEngine:
    """Optimized search functionality"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.05)
    def search_stocks(df: pd.DataFrame, query: str) -> pd.DataFrame:
        """Search stocks with optimized performance"""
        
        if not query or df.empty:
            return pd.DataFrame()
        
        try:
            query_upper = query.upper().strip()
            
            mask_ticker_exact = pd.Series(False, index=df.index)
            if 'ticker' in df.columns:
                mask_ticker_exact = (df['ticker'].str.upper() == query_upper).fillna(False)
                if mask_ticker_exact.any():
                    return df[mask_ticker_exact].copy()

            ticker_upper = df['ticker'].str.upper().fillna('') if 'ticker' in df.columns else pd.Series('', index=df.index)
            company_upper = df['company_name'].str.upper().fillna('') if 'company_name' in df.columns else pd.Series('', index=df.index)
            
            mask_ticker_contains = ticker_upper.str.contains(query_upper, regex=False)
            mask_company_contains = company_upper.str.contains(query_upper, regex=False)
            
            if 'company_name' in df.columns and not df['company_name'].empty:
                mask_company_word_match = df['company_name'].str.contains(r'\b' + re.escape(query_upper), case=False, na=False, regex=True)
            else:
                mask_company_word_match = pd.Series(False, index=df.index)

            combined_mask = mask_ticker_exact | mask_ticker_contains | mask_company_contains | mask_company_word_match
            all_matches = df[combined_mask].copy()
            
            if not all_matches.empty:
                all_matches['relevance'] = 0
                all_matches.loc[mask_ticker_exact[combined_mask], 'relevance'] = 100
                all_matches.loc[mask_ticker_contains[combined_mask], 'relevance'] += 50
                all_matches.loc[mask_company_contains[combined_mask], 'relevance'] += 30
                all_matches.loc[mask_company_word_match[combined_mask], 'relevance'] += 20
                
                return all_matches.sort_values(['relevance', 'master_score'], ascending=[False, False]).drop('relevance', axis=1)
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            return pd.DataFrame()

# ============================================
# EXPORT ENGINE - ENHANCED
# ============================================

class ExportEngine:
    """Handle all export operations with streaming for large datasets"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def create_excel_report(df: pd.DataFrame, template: str = 'full') -> BytesIO:
        """Create comprehensive Excel report with smart templates"""
        
        output = BytesIO()
        
        templates = {
            'day_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'rvol', 
                           'momentum_score', 'acceleration_score', 'ret_1d', 'ret_7d', 
                           'volume_score', 'vmi', 'wave_state', 'patterns', 'category', 'sector', 'industry'], 
                'focus': 'Intraday momentum and volume'
            },
            'swing_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 
                           'breakout_score', 'position_score', 'position_tension',
                           'from_high_pct', 'from_low_pct', 'trend_quality', 
                           'momentum_harmony', 'patterns', 'sector', 'industry'], 
                'focus': 'Position and breakout setups'
            },
            'investor': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'pe', 
                           'eps_current', 'eps_change_pct', 'ret_1y', 'ret_3y', 
                           'long_term_strength', 'money_flow_mm', 'category', 'sector', 'industry'], 
                'focus': 'Fundamentals and long-term performance'
            },
            'full': {
                'columns': None,
                'focus': 'Complete analysis'
            }
        }
        
        try:
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                workbook = writer.book
                
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#3498db',
                    'font_color': 'white',
                    'border': 1
                })
                
                top_100 = df.nlargest(min(100, len(df)), 'master_score', keep='first')
                
                if template in templates and templates[template]['columns']:
                    export_cols = [col for col in templates[template]['columns'] if col in top_100.columns]
                else:
                    internal_cols = ['percentile', 'category_rank', 'category_percentile', 'eps_tier', 'pe_tier', 'price_tier', 'signal_count', 'shift_strength', 'surge_score', 'total_stocks', 'analyzed_stocks', 'flow_score', 'avg_score', 'median_score', 'std_score', 'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'total_money_flow', 'rank_flow_score', 'dummy_money_flow']
                    export_cols = [col for col in top_100.columns if col not in internal_cols]
                
                top_100_export = top_100[export_cols]
                top_100_export.to_excel(writer, sheet_name='Top 100 Stocks', index=False)
                
                worksheet = writer.sheets['Top 100 Stocks']
                for i, col in enumerate(top_100_export.columns):
                    worksheet.write(0, i, col, header_format)
                worksheet.autofit()
                
                intel_data = []
                
                regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
                intel_data.append({
                    'Metric': 'Market Regime',
                    'Value': regime,
                    'Details': f"Breadth: {regime_metrics.get('breadth', 0):.1%}"
                })
                
                ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
                intel_data.append({
                    'Metric': 'Advance/Decline Ratio (1D)',
                    'Value': f"{ad_metrics.get('ad_ratio', 1):.2f}",
                    'Details': f"Advances: {ad_metrics.get('advancing', 0)}, Declines: {ad_metrics.get('declining', 0)}, Unchanged: {ad_metrics.get('unchanged', 0)}"
                })
                
                intel_df = pd.DataFrame(intel_data)
                intel_df.to_excel(writer, sheet_name='Market Intelligence', index=False)
                worksheet = writer.sheets['Market Intelligence']
                for i, col in enumerate(intel_df.columns): worksheet.write(0, i, col, header_format)
                worksheet.autofit()

                sector_rotation = MarketIntelligence.detect_sector_rotation(df)
                if not sector_rotation.empty:
                    sector_rotation.to_excel(writer, sheet_name='Sector Rotation')
                    worksheet = writer.sheets['Sector Rotation']
                    for i, col in enumerate(sector_rotation.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()
                
                industry_rotation = MarketIntelligence.detect_industry_rotation(df)
                if not industry_rotation.empty:
                    industry_rotation.to_excel(writer, sheet_name='Industry Rotation')
                    worksheet = writer.sheets['Industry Rotation']
                    for i, col in enumerate(industry_rotation.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()

                pattern_counts = {}
                for patterns_str in df['patterns'].dropna():
                    if patterns_str:
                        for p in patterns_str.split(' | '):
                            pattern_counts[p] = pattern_counts.get(p, 0) + 1
                
                if pattern_counts:
                    pattern_df = pd.DataFrame(
                        list(pattern_counts.items()),
                        columns=['Pattern', 'Count']
                    ).sort_values('Count', ascending=False)
                    pattern_df.to_excel(writer, sheet_name='Pattern Analysis', index=False)
                    worksheet = writer.sheets['Pattern Analysis']
                    for i, col in enumerate(pattern_df.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()
                
                wave_signals = df[
                    (df['momentum_score'].fillna(0) >= 60) & 
                    (df['acceleration_score'].fillna(0) >= 70) &
                    (df['rvol'].fillna(0) >= 2)
                ].nlargest(50, 'master_score', keep='first')
                
                if not wave_signals.empty:
                    wave_cols = ['ticker', 'company_name', 'master_score', 
                                'momentum_score', 'acceleration_score', 'rvol',
                                'wave_state', 'patterns', 'category', 'sector', 'industry']
                    available_wave_cols = [col for col in wave_cols if col in wave_signals.columns]
                    
                    wave_signals[available_wave_cols].to_excel(
                        writer, sheet_name='Wave Radar Signals', index=False
                    )
                    worksheet = writer.sheets['Wave Radar Signals']
                    for i, col in enumerate(wave_signals.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()

                summary_stats = {
                    'Total Stocks Processed': len(df),
                    'Average Master Score (All)': df['master_score'].mean() if not df.empty else 0,
                    'Stocks with Patterns (All)': (df['patterns'].fillna('') != '').sum() if 'patterns' in df.columns else 0,
                    'High RVOL (>2x) (All)': (df['rvol'].fillna(0) > 2).sum() if 'rvol' in df.columns else 0,
                    'Positive 30D Returns (All)': (df['ret_30d'].fillna(0) > 0).sum() if 'ret_30d' in df.columns else 0,
                    'Data Completeness %': st.session_state.data_quality.get('completeness', 0),
                    'Clipping Events Count': sum(DataValidator.get_clipping_counts().values()),
                    'Template Used': template,
                    'Export Date (UTC)': datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
                }
                
                summary_df = pd.DataFrame(list(summary_stats.items()), columns=['Metric', 'Value'])
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                worksheet = writer.sheets['Summary']
                for i, col in enumerate(summary_df.columns): worksheet.write(0, i, col, header_format)
                worksheet.autofit()

                logger.info(f"Excel report created successfully with {len(writer.sheets)} sheets")
                
        except Exception as e:
            logger.error(f"Error creating Excel report: {str(e)}", exc_info=True)
            raise
        
        output.seek(0)
        return output
    
    @staticmethod
    def create_csv_export(df: pd.DataFrame) -> str:
        """Create CSV export efficiently"""
        
        export_cols = [
            'rank', 'ticker', 'company_name', 'master_score',
            'position_score', 'volume_score', 'momentum_score',
            'acceleration_score', 'breakout_score', 'rvol_score',
            'trend_quality', 'price', 'pe', 'eps_current', 'eps_change_pct',
            'from_low_pct', 'from_high_pct',
            'ret_1d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y',
            'rvol', 'vmi', 'money_flow_mm', 'position_tension',
            'momentum_harmony', 'wave_state', 'patterns', 
            'category', 'sector', 'industry', 'eps_tier', 'pe_tier', 'price_tier', 'overall_wave_strength' 
        ]
        
        available_cols = [col for col in export_cols if col in df.columns]
        
        export_df = df[available_cols].copy()
        
        for col_name in CONFIG.VOLUME_RATIO_COLUMNS:
            if col_name in export_df.columns:
                export_df[col_name] = (export_df[col_name].fillna(1.0) - 1) * 100
                
        for col in export_df.select_dtypes(include=np.number).columns:
            export_df[col] = export_df[col].fillna('')

        for col in export_df.select_dtypes(include='object').columns:
            export_df[col] = export_df[col].fillna('')

        return export_df.to_csv(index=False)

# ============================================
# UI COMPONENTS
# ============================================

class UIComponents:
    """Reusable UI components"""
    
    @staticmethod
    def render_metric_card(label: str, value: Any, delta: Optional[str] = None, 
                          help_text: Optional[str] = None) -> None:
        """Render a styled metric card"""
        if help_text:
            st.metric(label, value, delta, help=help_text)
        else:
            st.metric(label, value, delta)
    
    @staticmethod
    def render_summary_section(df: pd.DataFrame) -> None:
        """Render enhanced summary dashboard"""
        
        if df.empty:
            st.warning("No data available for summary")
            return
        
        st.markdown("### ðŸ“Š Market Pulse")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
            ad_ratio = ad_metrics.get('ad_ratio', 1.0)
            
            display_ad_ratio = f"{ad_ratio:.2f}" if ad_ratio != float('inf') else "âˆž"
            if ad_ratio > 2: ad_emoji = "ðŸ”¥"
            elif ad_ratio > 1: ad_emoji = "ðŸ“ˆ"
            else: ad_emoji = "ðŸ“‰"
            
            UIComponents.render_metric_card(
                "A/D Ratio",
                f"{ad_emoji} {display_ad_ratio}",
                f"{ad_metrics.get('advancing', 0)}/{ad_metrics.get('declining', 0)}",
                "Advance/Decline Ratio (Advancing stocks / Declining stocks over 1 Day)"
            )
        
        with col2:
            if 'momentum_score' in df.columns:
                high_momentum = len(df[df['momentum_score'].fillna(0) >= 70])
                momentum_pct = (high_momentum / len(df) * 100) if len(df) > 0 else 0
            else:
                high_momentum, momentum_pct = 0, 0
            
            UIComponents.render_metric_card(
                "Momentum Health",
                f"{momentum_pct:.0f}%",
                f"{high_momentum} strong stocks",
                "Percentage of stocks with Momentum Score â‰¥ 70."
            )
        
        with col3:
            if 'rvol' in df.columns:
                avg_rvol = df['rvol'].fillna(1.0).median()
                high_vol_count = len(df[df['rvol'].fillna(0) > 2])
            else:
                avg_rvol, high_vol_count = 1.0, 0
            
            if avg_rvol > 1.5: vol_emoji = "ðŸŒŠ"
            elif avg_rvol > 1.2: vol_emoji = "ðŸ’§"
            else: vol_emoji = "ðŸœï¸"
            
            UIComponents.render_metric_card(
                "Volume State",
                f"{vol_emoji} {avg_rvol:.1f}x",
                f"{high_vol_count} surges",
                "Median Relative Volume (RVOL). Surges indicate stocks with RVOL > 2x."
            )
        
        with col4:
            risk_factors = 0
            
            if 'from_high_pct' in df.columns and 'momentum_score' in df.columns:
                overextended = len(df[(df['from_high_pct'].fillna(-100) >= 0) & (df['momentum_score'].fillna(0) < 50)])
                if overextended > 20: risk_factors += 1
            
            if 'rvol' in df.columns:
                pump_risk = len(df[(df['rvol'].fillna(0) > 10) & (df['master_score'].fillna(0) < 50)])
                if pump_risk > 10: risk_factors += 1
            
            if 'trend_quality' in df.columns:
                downtrends = len(df[df['trend_quality'].fillna(50) < 40])
                if downtrends > len(df) * 0.3 and len(df) > 0: risk_factors += 1
            
            risk_levels = ["ðŸŸ¢ LOW", "ðŸŸ¡ MODERATE", "ðŸŸ  HIGH", "ðŸ”´ EXTREME"]
            risk_level = risk_levels[min(risk_factors, 3)]
            
            UIComponents.render_metric_card(
                "Risk Level",
                risk_level,
                f"{risk_factors} factors",
                "Composite risk assessment based on overextension, extreme volume, and downtrends."
            )
        
        st.markdown("### ðŸŽ¯ Today's Best Opportunities")
        
        opp_col1, opp_col2, opp_col3 = st.columns(3)
        
        with opp_col1:
            ready_to_run = df[
                (df['momentum_score'].fillna(0) >= 70) & 
                (df['acceleration_score'].fillna(0) >= 70) &
                (df['rvol'].fillna(0) >= 2)
            ].nlargest(5, 'master_score', keep='first')
            
            st.markdown("**ðŸš€ Ready to Run**")
            if not ready_to_run.empty:
                for _, stock in ready_to_run.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"Score: {stock.get('master_score', np.nan):.1f} | RVOL: {stock.get('rvol', np.nan):.1f}x")
            else: st.info("No momentum leaders found")
        
        with opp_col2:
            hidden_gems = df[df['patterns'].str.contains('ðŸ’Ž HIDDEN GEM', na=False)].nlargest(5, 'master_score', keep='first')
            
            st.markdown("**ðŸ’Ž Hidden Gems**")
            if not hidden_gems.empty:
                for _, stock in hidden_gems.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"Cat %ile: {stock.get('category_percentile', np.nan):.0f} | Score: {stock.get('master_score', np.nan):.1f}")
            else: st.info("No hidden gems today")
        
        with opp_col3:
            volume_alerts = df[df['rvol'].fillna(0) > 3].nlargest(5, 'master_score', keep='first')
            
            st.markdown("**âš¡ Volume Alerts**")
            if not volume_alerts.empty:
                for _, stock in volume_alerts.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"RVOL: {stock.get('rvol', np.nan):.1f}x | {stock.get('wave_state', 'N/A')}")
            else: st.info("No extreme volume detected")
        
        st.markdown("### ðŸ§  Market Intelligence")
        
        intel_col1, intel_col2 = st.columns([2, 1])
        
        with intel_col1:
            sector_rotation = MarketIntelligence.detect_sector_rotation(df)
            
            if not sector_rotation.empty:
                fig = go.Figure()
                
                fig.add_trace(go.Bar(
                    x=sector_rotation.index[:10],
                    y=sector_rotation['flow_score'][:10],
                    text=[f"{val:.1f}" for val in sector_rotation['flow_score'][:10]],
                    textposition='outside',
                    marker_color=['#2ecc71' if score > 60 else '#e74c3c' if score < 40 else '#f39c12' 
                                 for score in sector_rotation['flow_score'][:10]],
                    hovertemplate=(
                        'Sector: %{x}<br>'
                        'Flow Score: %{y:.1f}<br>'
                        'Analyzed: %{customdata[0]} of %{customdata[1]} stocks<br>'
                        'Avg Score: %{customdata[2]:.1f}<br>'
                        'Median Score: %{customdata[3]:.1f}<extra></extra>'
                    ),
                    customdata=np.column_stack((
                        sector_rotation['analyzed_stocks'][:10],
                        sector_rotation['total_stocks'][:10],
                        sector_rotation['avg_score'][:10],
                        sector_rotation['median_score'][:10]
                    ))
                ))
                
                flow_direction = MarketIntelligence.detect_market_regime(df)[0]
                
                fig.update_layout(
                    title="Sector Rotation Map - Smart Money Flow (Dynamically Sampled)",
                    xaxis_title="Sector",
                    yaxis_title="Flow Score",
                    height=400,
                    template='plotly_white',
                    showlegend=False
                )
                
                st.plotly_chart(fig, use_container_width=True)
            else: st.info("No sector rotation data available for visualization.")
        
        with intel_col2:
            regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
            
            st.markdown(f"**ðŸŽ¯ Market Regime**")
            st.markdown(f"### {regime}")
            
            st.markdown("**ðŸ“¡ Key Signals**")
            
            signals = []
            
            breadth = regime_metrics.get('breadth', 0.5)
            if breadth > 0.6: signals.append("âœ… Strong breadth")
            elif breadth < 0.4: signals.append("âš ï¸ Weak breadth")
            
            category_spread = regime_metrics.get('category_spread', 0)
            if category_spread > 10: signals.append("ðŸ”„ Small caps leading")
            elif category_spread < -10: signals.append("ðŸ›¡ï¸ Large caps defensive")
            
            avg_rvol = regime_metrics.get('avg_rvol', 1.0)
            if avg_rvol > 1.5: signals.append("ðŸŒŠ High volume activity")
            
            pattern_count = (df['patterns'].fillna('') != '').sum()
            if pattern_count > len(df) * 0.2 and len(df) > 0: signals.append("ðŸŽ¯ Many patterns emerging")
            
            if signals:
                for signal in signals: st.write(signal)
            else: st.info("No significant market signals detected.")
            
            st.markdown("**ðŸ’ª Market Strength**")
            
            strength_score = ((breadth * 50) + (min(avg_rvol, 2) * 25) + ((pattern_count / len(df) if len(df) > 0 else 0) * 25))
            
            if strength_score > 70: strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢"
            elif strength_score > 50: strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢âšª"
            elif strength_score > 30: strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢âšªâšª"
            else: strength_meter = "ðŸŸ¢ðŸŸ¢âšªâšªâšª"
            
            st.write(strength_meter)
        
        else: st.warning("No data available for summary. Please adjust filters.")
    
    with tabs[1]:
        st.markdown("### ðŸ† Top Ranked Stocks")
        
        col1, col2, col3 = st.columns([2, 2, 6])
        with col1:
            display_count = st.selectbox(
                "Show top",
                options=CONFIG.AVAILABLE_TOP_N,
                index=CONFIG.AVAILABLE_TOP_N.index(st.session_state.user_preferences['default_top_n']),
                key="wd_rankings_display_count"
            )
            st.session_state.user_preferences['default_top_n'] = display_count
        
        with col2:
            sort_options = ['Rank', 'Master Score', 'RVOL', 'Momentum', 'Money Flow']
            if 'trend_quality' in filtered_df.columns: sort_options.append('Trend')
            
            sort_by = st.selectbox(
                "Sort by",
                options=sort_options,
                index=0,
                key="wd_rankings_sort_by"
            )
        
        display_df = filtered_df.copy()
        
        if sort_by == 'Master Score': display_df = display_df.sort_values('master_score', ascending=False)
        elif sort_by == 'RVOL': display_df = display_df.sort_values('rvol', ascending=False)
        elif sort_by == 'Momentum': display_df = display_df.sort_values('momentum_score', ascending=False)
        elif sort_by == 'Money Flow' and 'money_flow_mm' in display_df.columns: display_df = display_df.sort_values('money_flow_mm', ascending=False)
        elif sort_by == 'Trend' and 'trend_quality' in display_df.columns: display_df = display_df.sort_values('trend_quality', ascending=False)
        
        if not display_df.empty:
            if 'trend_quality' in display_df.columns:
                def get_trend_indicator(score):
                    if pd.isna(score): return "âž–"
                    elif score >= 80: return "ðŸ”¥"
                    elif score >= 60: return "âœ…"
                    elif score >= 40: return "âž¡ï¸"
                    else: return "âš ï¸"
                display_df['trend_indicator'] = display_df['trend_quality'].apply(get_trend_indicator)
            
            display_cols = {
                'rank': 'Rank', 'ticker': 'Ticker', 'company_name': 'Company',
                'master_score': 'Score', 'wave_state': 'Wave'
            }
            if 'trend_indicator' in display_df.columns: display_cols['trend_indicator'] = 'Trend'
            display_cols['price'] = 'Price'
            if show_fundamentals:
                if 'pe' in display_df.columns: display_cols['pe'] = 'PE'
                if 'eps_change_pct' in display_df.columns: display_cols['eps_change_pct'] = 'EPS Î”%'
            display_cols.update({
                'from_low_pct': 'From Low', 'ret_30d': '30D Ret',
                'rvol': 'RVOL', 'vmi': 'VMI', 'patterns': 'Patterns',
                'category': 'Category', 'sector': 'Sector', 'industry': 'Industry'
            })
            
            format_rules = {
                'master_score': '{:.1f}', 'price': 'â‚¹{:,.0f}', 'from_low_pct': '{:.0f}%',
                'ret_30d': '{:+.1f}%', 'rvol': '{:.1f}x', 'vmi': '{:.2f}'
            }
            def format_pe(value):
                try:
                    if pd.isna(value): return '-'
                    val = float(value)
                    if val <= 0: return 'Loss'
                    elif val > 10000: return '>10K'
                    elif val > 1000: return f"{val:.0f}"
                    else: return f"{val:.1f}"
                except: return '-'
            def format_eps_change(value):
                try:
                    if pd.isna(value): return '-'
                    val = float(value)
                    if abs(val) >= 1000: return f"{val/1000:+.1f}K%"
                    elif abs(val) >= 100: return f"{val:+.0f}%"
                    else: return f"{val:+.1f}%"
                except: return '-'
            
            for col, fmt in format_rules.items():
                if col in display_df.columns:
                    try:
                        display_df[col] = display_df[col].apply(lambda x: fmt.format(x) if pd.notna(x) and isinstance(x, (int, float)) else '-')
                    except Exception as e:
                        logger.error(f"Error formatting column {col}: {e}"); pass
            
            if show_fundamentals:
                if 'pe' in display_df.columns: display_df['pe'] = display_df['pe'].apply(format_pe)
                if 'eps_change_pct' in display_df.columns: display_df['eps_change_pct'] = display_df['eps_change_pct'].apply(format_eps_change)
            
            available_display_cols = [c for c in display_cols.keys() if c in display_df.columns]
            display_df = display_df[available_display_cols]
            display_df.columns = [display_cols[c] for c in available_display_cols]
            
            paginated_df = UIComponents.render_pagination_controls(display_df, display_count, 'rankings')
            
            st.dataframe(paginated_df, use_container_width=True, height=min(600, len(paginated_df) * 35 + 50), hide_index=True)
            
            with st.expander("ðŸ“Š Quick Statistics"):
                stat_cols = st.columns(4)
                with stat_cols[0]:
                    st.markdown("**Score Distribution**")
                    if 'master_score' in filtered_df.columns:
                        scores_data = filtered_df['master_score'].dropna()
                        if not scores_data.empty: st.text(f"Max: {scores_data.max():.1f}"); st.text(f"Min: {scores_data.min():.1f}"); st.text(f"Mean: {scores_data.mean():.1f}"); st.text(f"Median: {scores_data.median():.1f}"); st.text(f"Std: {scores_data.std():.1f}")
                        else: st.text("No valid scores.")
                with stat_cols[1]:
                    st.markdown("**Returns (30D)**")
                    if 'ret_30d' in filtered_df.columns:
                        returns_data = filtered_df['ret_30d'].dropna()
                        if not returns_data.empty: st.text(f"Max: {returns_data.max():.1f}%"); st.text(f"Min: {returns_data.min():.1f}%"); st.text(f"Avg: {returns_data.mean():.1f}%"); st.text(f"Positive: {(returns_data > 0).sum()}")
                        else: st.text("No valid 30D returns.")
                with stat_cols[2]:
                    if show_fundamentals:
                        st.markdown("**Fundamentals**")
                        if 'pe' in filtered_df.columns:
                            valid_pe = filtered_df['pe'].dropna(); valid_pe = valid_pe[(valid_pe > 0) & (valid_pe < 10000)];
                            if not valid_pe.empty: median_pe = valid_pe.median(); st.text(f"Median PE: {median_pe:.1f}x")
                            else: st.text("No valid PE.")
                        if 'eps_change_pct' in filtered_df.columns:
                            valid_eps = filtered_df['eps_change_pct'].dropna();
                            if not valid_eps.empty: positive = (valid_eps > 0).sum(); st.text(f"Positive EPS: {positive}")
                            else: st.text("No valid EPS change.")
                    else:
                        st.markdown("**Volume**")
                        if 'rvol' in filtered_df.columns:
                            rvol_data = filtered_df['rvol'].dropna();
                            if not rvol_data.empty: st.text(f"Max: {rvol_data.max():.1f}x"); st.text(f"Avg: {rvol_data.mean():.1f}x"); st.text(f">2x: {(rvol_data > 2).sum()}")
                            else: st.text("No valid RVOL.")
                with stat_cols[3]:
                    st.markdown("**Trend Distribution**")
                    if 'trend_quality' in filtered_df.columns:
                        trend_data = filtered_df['trend_quality'].dropna();
                        if not trend_data.empty:
                            total_stocks_in_filter = len(trend_data); avg_trend_score = trend_data.mean(); stocks_above_all_smas = (trend_data >= 85).sum(); stocks_in_uptrend = (trend_data >= 60).sum(); stocks_in_downtrend = (trend_data < 40).sum();
                            st.text(f"Avg Trend Score: {avg_trend_score:.1f}"); st.text(f"Above All SMAs: {stocks_above_all_smas}"); st.text(f"In Uptrend (60+): {stocks_in_uptrend}"); st.text(f"In Downtrend (<40): {stocks_in_downtrend}")
                        else: st.text("No valid trend data.")
        else: st.warning("No stocks match the selected filters.")
        
    with tabs[2]:
        st.markdown("### ðŸŒŠ Wave Radar - Early Momentum Detection System")
        st.markdown("*Catch waves as they form, not after they've peaked!*")
        radar_col1, radar_col2, radar_col3, radar_col4 = st.columns([2, 2, 2, 1])
        with radar_col1:
            wave_timeframe = st.selectbox(
                "Wave Detection Timeframe",
                options=["All Waves", "Intraday Surge", "3-Day Buildup", "Weekly Breakout", "Monthly Trend"],
                index=["All Waves", "Intraday Surge", "3-Day Buildup", "Weekly Breakout", "Monthly Trend"].index(st.session_state.get('wd_wave_timeframe_select', "All Waves")), key="wd_wave_timeframe_select",
                help="""ðŸŒŠ All Waves: Complete unfiltered view
                âš¡ Intraday Surge: High RVOL & today's movers
                ðŸ“ˆ 3-Day Buildup: Building momentum patterns
                ðŸš€ Weekly Breakout: Near 52w highs with volume
                ðŸ’ª Monthly Trend: Established trends with SMAs""")
        with radar_col2:
            sensitivity = st.select_slider("Detection Sensitivity", options=["Conservative", "Balanced", "Aggressive"], value=st.session_state.get('wd_wave_sensitivity', "Balanced"), key="wd_wave_sensitivity", help="Conservative = Stronger signals, Aggressive = More signals")
            show_sensitivity_details = st.checkbox("Show thresholds", value=st.session_state.get('wd_show_sensitivity_details', False), key="wd_show_sensitivity_details", help="Display exact threshold values for current sensitivity")
        with radar_col3:
            show_market_regime = st.checkbox("ðŸ“Š Market Regime Analysis", value=st.session_state.get('wd_show_market_regime', True), key="wd_show_market_regime", help="Show category rotation flow and market regime detection")
        wave_filtered_df = filtered_df.copy()
        with radar_col4:
            if not wave_filtered_df.empty and 'overall_wave_strength' in wave_filtered_df.columns:
                wave_strength_score = wave_filtered_df['overall_wave_strength'].fillna(0).mean()
                if wave_strength_score > 70: wave_emoji, wave_color_delta = "ðŸŒŠðŸ”¥", "ðŸŸ¢"
                elif wave_strength_score > 50: wave_emoji, wave_color_delta = "ðŸŒŠ", "ðŸŸ¡"
                else: wave_emoji, wave_color_delta = "ðŸ’¤", "ðŸ”´"
                UIComponents.render_metric_card("Wave Strength", f"{wave_emoji} {wave_strength_score:.0f}%", f"{wave_color_delta} Market", "Overall strength of wave signals in the current filtered dataset. Reflects market trend bias.")
            else: UIComponents.render_metric_card("Wave Strength", "N/A", "Error calculating strength.")
        if show_sensitivity_details:
            with st.expander("ðŸ“Š Current Sensitivity Thresholds", expanded=True):
                if sensitivity == "Conservative": st.markdown("""**Conservative Settings** ðŸ›¡ï¸ - **Momentum Shifts:** Score â‰¥ 60, Acceleration â‰¥ 70 - **Emerging Patterns:** Within 5% of qualifying threshold - **Volume Surges:** RVOL â‰¥ 3.0x - **Acceleration Alerts:** Score â‰¥ 85 - **Pattern Distance:** 5% from qualification""")
                elif sensitivity == "Balanced": st.markdown("""**Balanced Settings** âš–ï¸ - **Momentum Shifts:** Score â‰¥ 50, Acceleration â‰¥ 60 - **Emerging Patterns:** Within 10% of qualifying threshold - **Volume Surges:** RVOL â‰¥ 2.0x - **Acceleration Alerts:** Score â‰¥ 70 - **Pattern Distance:** 10% from qualification""")
                else: st.markdown("""**Aggressive Settings** ðŸš€ - **Momentum Shifts:** Score â‰¥ 40, Acceleration â‰¥ 50 - **Emerging Patterns:** Within 15% of qualifying threshold - **Volume Surges:** RVOL â‰¥ 1.5x - **Acceleration Alerts:** Score â‰¥ 60 - **Pattern Distance:** 15% from qualification""")
                st.info("ðŸ’¡ **Tip**: Start with Balanced, then adjust based on market conditions and your risk tolerance.")
        if wave_timeframe != "All Waves":
            try:
                if wave_timeframe == "Intraday Surge":
                    if all(col in wave_filtered_df.columns for col in ['rvol', 'ret_1d', 'price', 'prev_close']):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['rvol'].fillna(0) >= 2.5) & (wave_filtered_df['ret_1d'].fillna(0) > 2) &
                            (wave_filtered_df['price'].fillna(0) > wave_filtered_df['prev_close'].fillna(0) * 1.02)]
                elif wave_timeframe == "3-Day Buildup":
                    if all(col in wave_filtered_df.columns for col in ['ret_3d', 'vol_ratio_7d_90d', 'price', 'sma_20d']):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['ret_3d'].fillna(0) > 5) & (wave_filtered_df['vol_ratio_7d_90d'].fillna(0) > 1.5) &
                            (wave_filtered_df['price'].fillna(0) > wave_filtered_df['sma_20d'].fillna(0))]
                elif wave_timeframe == "Weekly Breakout":
                    if all(col in wave_filtered_df.columns for col in ['ret_7d', 'vol_ratio_7d_90d', 'from_high_pct']):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['ret_7d'].fillna(0) > 8) & (wave_filtered_df['vol_ratio_7d_90d'].fillna(0) > 2.0) &
                            (wave_filtered_df['from_high_pct'].fillna(-100) > -10)]
                elif wave_timeframe == "Monthly Trend":
                    if all(col in wave_filtered_df.columns for col in ['ret_30d', 'price', 'sma_20d', 'sma_50d', 'vol_ratio_30d_180d', 'from_low_pct']):
                        wave_filtered_df = wave_filtered_df[
                            (wave_filtered_df['ret_30d'].fillna(0) > 15) & (wave_filtered_df['price'].fillna(0) > wave_filtered_df['sma_20d'].fillna(0)) &
                            (wave_filtered_df['sma_20d'].fillna(0) > wave_filtered_df['sma_50d'].fillna(0)) & (wave_filtered_df['vol_ratio_30d_180d'].fillna(0) > 1.2) &
                            (wave_filtered_df['from_low_pct'].fillna(0) > 30)]
            except Exception as e: logger.warning(f"Error applying {wave_timeframe} filter: {e}"); st.warning(f"Some data not available for {wave_timeframe} filter, showing all relevant stocks."); wave_filtered_df = filtered_df.copy()
        
        if not wave_filtered_df.empty:
            st.markdown("#### ðŸš€ Momentum Shifts - Stocks Entering Strength")
            if sensitivity == "Conservative": momentum_threshold, acceleration_threshold, min_rvol_signal = 60, 70, 3.0
            elif sensitivity == "Balanced": momentum_threshold, acceleration_threshold, min_rvol_signal = 50, 60, 2.0
            else: momentum_threshold, acceleration_threshold, min_rvol_signal = 40, 50, 1.5
            
            momentum_shifts = wave_filtered_df.copy()
            momentum_shifts = momentum_shifts[(momentum_shifts['momentum_score'].fillna(0) >= momentum_threshold) & (momentum_shifts['acceleration_score'].fillna(0) >= acceleration_threshold)]
            
            if not momentum_shifts.empty:
                momentum_shifts['signal_count'] = 0
                momentum_shifts.loc[momentum_shifts['momentum_score'].fillna(0) >= momentum_threshold, 'signal_count'] += 1
                momentum_shifts.loc[momentum_shifts['acceleration_score'].fillna(0) >= acceleration_threshold, 'signal_count'] += 1
                momentum_shifts.loc[momentum_shifts['rvol'].fillna(0) >= min_rvol_signal, 'signal_count'] += 1
                if 'breakout_score' in momentum_shifts.columns: momentum_shifts.loc[momentum_shifts['breakout_score'].fillna(0) >= 75, 'signal_count'] += 1
                if 'vol_ratio_7d_90d' in momentum_shifts.columns: momentum_shifts.loc[momentum_shifts['vol_ratio_7d_90d'].fillna(0) >= 1.5, 'signal_count'] += 1
                
                momentum_shifts['shift_strength'] = (
                    momentum_shifts['momentum_score'].fillna(50) * 0.4 +
                    momentum_shifts['acceleration_score'].fillna(50) * 0.4 +
                    momentum_shifts['rvol_score'].fillna(50) * 0.2)
                
                top_shifts = momentum_shifts.sort_values(['signal_count', 'shift_strength'], ascending=[False, False]).head(20)
                
                display_columns = ['ticker', 'company_name', 'master_score', 'momentum_score', 'acceleration_score', 'rvol', 'signal_count', 'wave_state', 'category', 'sector', 'industry']
                if 'ret_7d' in top_shifts.columns: display_columns.insert(-3, 'ret_7d')
                
                shift_display = top_shifts[[col for col in display_columns if col in top_shifts.columns]].copy()
                shift_display['Signals'] = shift_display['signal_count'].apply(lambda x: f"{'ðŸ”¥' * min(x, 3)} {x}/5")
                
                if 'ret_7d' in shift_display.columns: shift_display['7D Return'] = shift_display['ret_7d'].apply(lambda x: f"{x:.1f}%" if pd.notna(x) else '-')
                shift_display['RVOL'] = shift_display['rvol'].apply(lambda x: f"{x:.1f}x" if pd.notna(x) else '-')
                
                shift_display = shift_display.rename(columns={'ticker': 'Ticker', 'company_name': 'Company', 'master_score': 'Score', 'momentum_score': 'Momentum', 'acceleration_score': 'Acceleration', 'wave_state': 'Wave', 'category': 'Category', 'sector': 'Sector', 'industry': 'Industry'})
                shift_display = shift_display.drop('signal_count', axis=1)
                
                st.dataframe(shift_display, use_container_width=True, hide_index=True)
                
                multi_signal = len(top_shifts[top_shifts['signal_count'] >= 3])
                if multi_signal > 0: st.success(f"ðŸ† Found {multi_signal} stocks with 3+ signals (strongest momentum)")
                super_signals = top_shifts[top_shifts['signal_count'] >= 4]
                if not super_signals.empty: st.warning(f"ðŸ”¥ðŸ”¥ {len(super_signals)} stocks showing EXTREME momentum (4+ signals)!")
            else: st.info(f"No momentum shifts detected in {wave_timeframe} timeframe for '{sensitivity}' sensitivity.")
            
            st.markdown("#### ðŸš€ Acceleration Profiles - Momentum Building Over Time")
            if sensitivity == "Conservative": accel_threshold = 85
            elif sensitivity == "Balanced": accel_threshold = 70
            else: accel_threshold = 60
            
            if 'acceleration_score' in wave_filtered_df.columns:
                accelerating_stocks = wave_filtered_df[wave_filtered_df['acceleration_score'].fillna(0) >= accel_threshold].nlargest(10, 'acceleration_score', keep='first')
            else: accelerating_stocks = pd.DataFrame()
            
            if not accelerating_stocks.empty:
                fig_accel = Visualizer.create_acceleration_profiles(accelerating_stocks, n=10)
                st.plotly_chart(fig_accel, use_container_width=True)
                col1, col2, col3 = st.columns(3)
                with col1: st.metric("Perfect Acceleration (90+)", len(accelerating_stocks[accelerating_stocks['acceleration_score'].fillna(0) >= 90]), help_text="Number of stocks with Acceleration Score >= 90.")
                with col2: st.metric("Strong Acceleration (80+)", len(accelerating_stocks[accelerating_stocks['acceleration_score'].fillna(0) >= 80]), help_text="Number of stocks with Acceleration Score >= 80.")
                with col3: st.metric("Avg Acceleration Score", f"{accelerating_stocks['acceleration_score'].fillna(0).mean():.1f}", help_text="Average Acceleration Score for displayed stocks.")
            else: st.info(f"No stocks meet the acceleration threshold ({accel_threshold}+) for '{sensitivity}' sensitivity.")
            
            if show_market_regime:
                st.markdown("#### ðŸ’° Category Rotation - Smart Money Flow")
                col1, col2 = st.columns([3, 2])
                with col1:
                    try:
                        if 'category' in wave_filtered_df.columns:
                            category_dfs = []
                            for cat in wave_filtered_df['category'].dropna().unique():
                                sampled_cat_df = MarketIntelligence._apply_dynamic_sampling(wave_filtered_df[wave_filtered_df['category'] == cat].copy())
                                if not sampled_cat_df.empty: category_dfs.append(sampled_cat_df)
                            if category_dfs:
                                normalized_cat_df = pd.concat(category_dfs, ignore_index=True)
                                category_flow = MarketIntelligence._calculate_flow_metrics(normalized_cat_df, 'category')
                                original_cat_counts = df.groupby('category').size().rename('total_stocks')
                                category_flow = category_flow.join(original_cat_counts, how='left')
                                category_flow['analyzed_stocks'] = category_flow['count']
                                if not category_flow.empty:
                                    top_category_name = category_flow.index[0]
                                    if 'Small' in top_category_name or 'Micro' in top_category_name: flow_direction = "ðŸ”¥ RISK-ON"
                                    elif 'Large' in top_category_name or 'Mega' in top_category_name: flow_direction = "â„ï¸ RISK-OFF"
                                    else: flow_direction = "âž¡ï¸ Neutral"
                                    fig_flow = go.Figure()
                                    fig_flow.add_trace(go.Bar(
                                        x=category_flow.index, y=category_flow['flow_score'], text=[f"{val:.1f}" for val in category_flow['flow_score']], textposition='outside',
                                        marker_color=['#2ecc71' if score > 60 else '#e74c3c' if score < 40 else '#f39c12' for score in category_flow['flow_score']],
                                        hovertemplate='Category: %{x}<br>Flow Score: %{y:.1f}<br>Analyzed: %{customdata[0]} of %{customdata[1]} stocks<extra></extra>',
                                        customdata=np.column_stack((category_flow['analyzed_stocks'], category_flow['total_stocks']))
                                    ))
                                    fig_flow.update_layout(title=f"Smart Money Flow Direction: {flow_direction} (Dynamically Sampled)", xaxis_title="Market Cap Category", yaxis_title="Flow Score", height=300, template='plotly_white', showlegend=False)
                                    st.plotly_chart(fig_flow, use_container_width=True)
                                else: st.info("Insufficient data for category flow analysis after sampling.")
                            else: st.info("No valid stocks found in categories for flow analysis after sampling.")
                    except Exception as e: logger.error(f"Error in category flow analysis: {e}"); st.error("Unable to analyze category flow")
                with col2:
                    if 'category_flow' in locals() and not category_flow.empty:
                        st.markdown(f"**ðŸŽ¯ Market Regime: {flow_direction}**")
                        st.markdown("**ðŸ’Ž Strongest Categories:**")
                        for i, (cat, row) in enumerate(category_flow.head(3).iterrows()):
                            emoji = "ðŸ¥‡" if i == 0 else "ðŸ¥ˆ" if i == 1 else "ðŸ¥‰"
                            st.write(f"{emoji} **{cat}**: Score {row['flow_score']:.1f}")
                        st.markdown("**ðŸ”„ Category Shifts:**")
                        small_caps_score = category_flow.loc[category_flow.index.str.contains('Small|Micro'), 'flow_score'].mean()
                        large_caps_score = category_flow.loc[category_flow.index.str.contains('Large|Mega'), 'flow_score'].mean()
                        if small_caps_score > large_caps_score + 10: st.success("ðŸ“ˆ Small Caps Leading - Early Bull Signal!")
                        elif large_caps_score > small_caps_score + 10: st.warning("ðŸ“‰ Large Caps Leading - Defensive Mode")
                        else: st.info("âž¡ï¸ Balanced Market - No Clear Leader")
                    else: st.info("Category data not available")
            st.markdown("#### ðŸŽ¯ Emerging Patterns - About to Qualify")
            pattern_distance = {"Conservative": 5, "Balanced": 10, "Aggressive": 15}[sensitivity]
            emergence_data = []
            if 'category_percentile' in wave_filtered_df.columns and 'master_score' in wave_filtered_df.columns:
                close_to_leader = wave_filtered_df[(wave_filtered_df['category_percentile'].fillna(0) >= (90 - pattern_distance)) & (wave_filtered_df['category_percentile'].fillna(0) < 90)]
                for _, stock in close_to_leader.iterrows():
                    emergence_data.append({'Ticker': stock['ticker'], 'Company': stock['company_name'], 'Pattern': 'ðŸ”¥ CAT LEADER', 'Distance': f"{90 - stock['category_percentile'].fillna(0):.1f}% away", 'Current': f"{stock['category_percentile'].fillna(0):.1f}%ile", 'Score': stock['master_score']})
            if 'breakout_score' in wave_filtered_df.columns and 'master_score' in wave_filtered_df.columns:
                close_to_breakout = wave_filtered_df[(wave_filtered_df['breakout_score'].fillna(0) >= (80 - pattern_distance)) & (wave_filtered_df['breakout_score'].fillna(0) < 80)]
                for _, stock in close_to_breakout.iterrows():
                    emergence_data.append({'Ticker': stock['ticker'], 'Company': stock['company_name'], 'Pattern': 'ðŸŽ¯ BREAKOUT', 'Distance': f"{80 - stock['breakout_score'].fillna(0):.1f} pts away", 'Current': f"{stock['breakout_score'].fillna(0):.1f} score", 'Score': stock['master_score']})
            if emergence_data:
                emergence_df = pd.DataFrame(emergence_data).sort_values('Score', ascending=False).head(15)
                col1, col2 = st.columns([3, 1])
                with col1: st.dataframe(emergence_df, use_container_width=True, hide_index=True)
                with col2: UIComponents.render_metric_card("Emerging Patterns", len(emergence_df))
            else: st.info(f"No patterns emerging within {pattern_distance}% threshold.")
            
            st.markdown("#### ðŸŒŠ Volume Surges - Unusual Activity NOW")
            rvol_threshold_display = {"Conservative": 3.0, "Balanced": 2.0, "Aggressive": 1.5}[sensitivity]
            if 'rvol' in wave_filtered_df.columns:
                volume_surges = wave_filtered_df[wave_filtered_df['rvol'].fillna(0) >= rvol_threshold_display].copy()
            else: volume_surges = pd.DataFrame()
            if not volume_surges.empty:
                volume_surges['surge_score'] = (volume_surges['rvol_score'].fillna(50) * 0.5 + volume_surges['volume_score'].fillna(50) * 0.3 + volume_surges['momentum_score'].fillna(50) * 0.2)
                top_surges = volume_surges.nlargest(15, 'surge_score', keep='first')
                col1, col2 = st.columns([2, 1])
                with col1:
                    display_cols_surge = ['ticker', 'company_name', 'rvol', 'price', 'money_flow_mm', 'wave_state', 'category', 'sector', 'industry']
                    if 'ret_1d' in top_surges.columns: display_cols_surge.insert(3, 'ret_1d')
                    surge_display = top_surges[[col for col in display_cols_surge if col in top_surges.columns]].copy()
                    surge_display['Type'] = surge_display['rvol'].apply(lambda x: "ðŸ”¥ðŸ”¥ðŸ”¥" if x > 5 else "ðŸ”¥ðŸ”¥" if x > 3 else "ðŸ”¥" if x > 1.5 else "-")
                    if 'ret_1d' in surge_display.columns: surge_display['ret_1d'] = surge_display['ret_1d'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else '-')
                    if 'money_flow_mm' in surge_display.columns: surge_display['money_flow_mm'] = surge_display['money_flow_mm'].apply(lambda x: f"â‚¹{x:.1f}M" if pd.notna(x) else '-')
                    surge_display['price'] = surge_display['price'].apply(lambda x: f"â‚¹{x:,.0f}" if pd.notna(x) else '-')
                    surge_display['rvol'] = surge_display['rvol'].apply(lambda x: f"{x:.1f}x" if pd.notna(x) else '-')
                    rename_dict_surge = {'ticker': 'Ticker', 'company_name': 'Company', 'rvol': 'RVOL', 'price': 'Price', 'money_flow_mm': 'Money Flow', 'wave_state': 'Wave', 'category': 'Category', 'sector': 'Sector', 'industry': 'Industry', 'ret_1d': '1D Ret'}
                    surge_display = surge_display.rename(columns=rename_dict_surge)
                    st.dataframe(surge_display, use_container_width=True, hide_index=True)
                with col2:
                    UIComponents.render_metric_card("Active Surges", len(volume_surges), help_text=f"Number of stocks with RVOL >= {rvol_threshold_display}x.")
                    UIComponents.render_metric_card("Extreme (>5x)", len(volume_surges[volume_surges['rvol'].fillna(0) > 5]), help_text="Stocks with RVOL > 5x.")
                    UIComponents.render_metric_card("High (>3x)", len(volume_surges[volume_surges['rvol'].fillna(0) > 3]), help_text="Stocks with RVOL > 3x.")
                    if 'category' in volume_surges.columns:
                        st.markdown("**ðŸ“Š Surge by Category:**")
                        surge_categories = volume_surges['category'].dropna().value_counts()
                        if not surge_categories.empty:
                            for cat, count in surge_categories.head(3).items(): st.caption(f"â€¢ {cat}: {count} stocks")
                        else: st.caption("No categories with surges.")
            else: st.info(f"No volume surges detected with '{sensitivity}' sensitivity (requires RVOL â‰¥ {rvol_threshold_display}x).")
        else: st.warning(f"No data available for Wave Radar analysis with '{wave_timeframe}' timeframe. Please adjust filters or timeframe.")
    
    with tabs[3]:
        st.markdown("### ðŸ“Š Market Analysis")
        if not filtered_df.empty:
            col1, col2 = st.columns(2)
            with col1:
                fig_dist = Visualizer.create_score_distribution(filtered_df)
                st.plotly_chart(fig_dist, use_container_width=True)
            with col2:
                pattern_counts = {};
                for patterns_str in filtered_df['patterns'].dropna():
                    if patterns_str:
                        for p in patterns_str.split(' | '): pattern_counts[p] = pattern_counts.get(p, 0) + 1
                if pattern_counts:
                    pattern_df = pd.DataFrame(list(pattern_counts.items()), columns=['Pattern', 'Count']).sort_values('Count', ascending=True).tail(15)
                    fig_patterns = go.Figure([go.Bar(x=pattern_df['Count'], y=pattern_df['Pattern'], orientation='h', marker_color='#3498db', text=pattern_df['Count'], textposition='outside')])
                    fig_patterns.update_layout(title="Pattern Frequency Analysis", xaxis_title="Number of Stocks", yaxis_title="Pattern", template='plotly_white', height=400, margin=dict(l=150))
                    st.plotly_chart(fig_patterns, use_container_width=True)
                else: st.info("No patterns detected in current selection")
            st.markdown("---")
            st.markdown("#### Sector Performance (Dynamically Sampled)")
            sector_overview_df_local = MarketIntelligence.detect_sector_rotation(filtered_df)
            if not sector_overview_df_local.empty:
                display_cols_overview = ['flow_score', 'avg_score', 'median_score', 'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'analyzed_stocks', 'total_stocks']
                available_overview_cols = [col for col in display_cols_overview if col in sector_overview_df_local.columns]
                sector_overview_display = sector_overview_df_local[available_overview_cols].copy()
                sector_overview_display.columns = ['Flow Score', 'Avg Score', 'Median Score', 'Avg Momentum', 'Avg Volume', 'Avg RVOL', 'Avg 30D Ret', 'Analyzed Stocks', 'Total Stocks']
                sector_overview_display['Coverage %'] = ((sector_overview_display['Analyzed Stocks'] / sector_overview_display['Total Stocks'] * 100).replace([np.inf, -np.inf], np.nan).fillna(0).round(1).apply(lambda x: f"{x}%"))
                st.dataframe(sector_overview_display.style.background_gradient(subset=['Flow Score', 'Avg Score']), use_container_width=True)
                st.info("ðŸ“Š **Normalized Analysis**: Shows metrics for dynamically sampled stocks per sector (by Master Score) to ensure fair comparison across sectors of different sizes.")
            else: st.info("No sector data available in the filtered dataset for analysis. Please check your filters.")
            st.markdown("#### Category Performance")
            if 'category' in filtered_df.columns:
                category_df_agg = filtered_df.groupby('category').agg({'master_score': ['mean', 'count'], 'category_percentile': 'mean', 'money_flow_mm': 'sum' if 'money_flow_mm' in filtered_df.columns else lambda x: 0})
                category_df_agg.columns = ['_'.join(col).strip() for col in category_df_agg.columns.values]
                category_df_display = category_df_agg.rename(columns={'master_score_mean': 'Avg Score', 'master_score_count': 'Count', 'category_percentile_mean': 'Avg Cat %ile', 'money_flow_mm_sum': 'Total Money Flow'})
                category_df_display = category_df_display.sort_values('Avg Score', ascending=False)
                st.dataframe(category_df_display.style.background_gradient(subset=['Avg Score']), use_container_width=True)
            else: st.info("Category column not available in data.")
            st.markdown("#### Industry Performance (Dynamically Sampled)")
            industry_overview_df_local = MarketIntelligence.detect_industry_rotation(filtered_df)
            if not industry_overview_df_local.empty:
                display_cols_overview_industry = ['flow_score', 'avg_score', 'median_score', 'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'analyzed_stocks', 'total_stocks']
                available_overview_cols_industry = [col for col in display_cols_overview_industry if col in industry_overview_df_local.columns]
                industry_overview_display = industry_overview_df_local[available_overview_cols_industry].copy()
                industry_overview_display.columns = ['Flow Score', 'Avg Score', 'Median Score', 'Avg Momentum', 'Avg Volume', 'Avg RVOL', 'Avg 30D Ret', 'Analyzed Stocks', 'Total Stocks']
                industry_overview_display['Coverage %'] = ((industry_overview_display['Analyzed Stocks'] / industry_overview_display['Total Stocks'] * 100).replace([np.inf, -np.inf], np.nan).fillna(0).round(1).apply(lambda x: f"{x}%"))
                st.dataframe(industry_overview_display.style.background_gradient(subset=['Flow Score', 'Avg Score']), use_container_width=True)
                st.info("ðŸ“Š **Normalized Analysis**: Shows metrics for dynamically sampled stocks per industry (by Master Score) to ensure fair comparison across industries of different sizes.")
            else: st.info("No industry data available in the filtered dataset for analysis. Please check your filters.")
        else: st.info("No data available for analysis.")
    
    with tabs[4]:
        st.markdown("### ðŸ” Advanced Stock Search")
        col1, col2 = st.columns([4, 1])
        with col1: search_query = st.text_input("Search stocks", value=st.session_state.get('wd_search_query', ''), placeholder="Enter ticker or company name...", help="Search by ticker symbol or company name", key="wd_search_input")
        with col2: st.markdown("<br>", unsafe_allow_html=True); search_clicked = st.button("ðŸ”Ž Search", type="primary", use_container_width=True, key="wd_search_button")
        
        if st.session_state.wd_search_input != st.session_state.get('wd_search_query', ''):
            st.session_state.wd_search_query = st.session_state.wd_search_input; st.rerun()
        
        if st.session_state.wd_search_query or search_clicked:
            if not st.session_state.wd_search_query.strip(): st.info("Please enter a search query."); search_results = pd.DataFrame()
            else:
                with st.spinner("Searching..."): search_results = SearchEngine.search_stocks(filtered_df, st.session_state.wd_search_query)
            if not search_results.empty:
                st.success(f"Found {len(search_results)} matching stock(s)")
                for idx, stock in search_results.iterrows():
                    with st.expander(f"ðŸ“Š {stock['ticker']} - {stock['company_name']} (Rank #{int(stock['rank']) if pd.notna(stock['rank']) else 'N/A'})", expanded=True):
                        metric_cols = st.columns(6)
                        with metric_cols[0]: UIComponents.render_metric_card("Master Score", f"{stock['master_score']:.1f}" if pd.notna(stock.get('master_score')) else "N/A", f"Rank #{int(stock['rank'])}" if pd.notna(stock.get('rank')) else "N/A", help_text="Composite score indicating overall strength. Higher is better.")
                        with metric_cols[1]: price_value = f"â‚¹{stock['price']:,.0f}" if pd.notna(stock.get('price')) else "N/A"; ret_1d_value = f"{stock['ret_1d']:+.1f}%" if pd.notna(stock.get('ret_1d')) else None; UIComponents.render_metric_card("Price", price_value, ret_1d_value, help_text="Current stock price and 1-day return.")
                        with metric_cols[2]: UIComponents.render_metric_card("From Low", f"{stock['from_low_pct']:.0f}%" if pd.notna(stock.get('from_low_pct')) else "N/A", help_text="Percentage change from 52-week low. Higher means closer to high.")
                        with metric_cols[3]: ret_30d = stock.get('ret_30d'); UIComponents.render_metric_card("30D Return", f"{ret_30d:+.1f}%" if pd.notna(ret_30d) else "N/A", "â†‘" if pd.notna(ret_30d) and ret_30d > 0 else ("â†“" if pd.notna(ret_30d) and ret_30d < 0 else None), help_text="Percentage return over the last 30 days.")
                        with metric_cols[4]: rvol = stock.get('rvol'); UIComponents.render_metric_card("RVOL", f"{rvol:.1f}x" if pd.notna(rvol) else "N/A", "High" if pd.notna(rvol) and rvol > 2 else "Normal", help_text="Relative Volume: current volume compared to average. Higher indicates unusual activity.")
                        with metric_cols[5]: UIComponents.render_metric_card("Wave State", stock.get('wave_state', 'N/A'), stock.get('category', 'N/A'), help_text="Detected current momentum phase (Forming, Building, Cresting, Breaking).")
                        st.markdown("#### ðŸ“ˆ Score Components"); score_cols_breakdown = st.columns(6)
                        components = [("Position", stock.get('position_score'), CONFIG.POSITION_WEIGHT, "52-week range positioning."), ("Volume", stock.get('volume_score'), CONFIG.VOLUME_WEIGHT, "Multi-timeframe volume patterns."), ("Momentum", stock.get('momentum_score'), CONFIG.MOMENTUM_WEIGHT, "30-day price momentum."), ("Acceleration", stock.get('acceleration_score'), CONFIG.ACCELERATION_WEIGHT, "Momentum acceleration signals."), ("Breakout", stock.get('breakout_score'), CONFIG.BREAKOUT_WEIGHT, "Technical breakout readiness."), ("RVOL", stock.get('rvol_score'), CONFIG.RVOL_WEIGHT, "Real-time relative volume score.")]
                        for i, (name, score, weight, help_text_comp) in enumerate(components):
                            with score_cols_breakdown[i]:
                                if pd.isna(score): color, display_score = "âšª", "N/A"
                                elif score >= 80: color, display_score = "ðŸŸ¢", f"{score:.0f}"
                                elif score >= 60: color, display_score = "ðŸŸ¡", f"{score:.0f}"
                                else: color, display_score = "ðŸ”´", f"{score:.0f}"
                                with st.popover(f"**{name}**", help=help_text_comp):
                                    st.markdown(f"**{name} Score**: {color} {display_score}")
                                    st.markdown(f"Weighted at **{weight:.0%}** of Master Score."); st.write(help_text_comp)
                        if stock.get('patterns'): st.markdown(f"**ðŸŽ¯ Patterns:** {stock['patterns']}")
                        else: st.markdown("**ðŸŽ¯ Patterns:** None detected.")
                        st.markdown("---"); detail_cols_top = st.columns([1, 1]);
                        with detail_cols_top[0]:
                            st.markdown("**ðŸ“Š Classification**")
                            st.text(f"Sector: {stock.get('sector', 'Unknown')}"); st.text(f"Industry: {stock.get('industry', 'Unknown')}"); st.text(f"Category: {stock.get('category', 'Unknown')}")
                            if show_fundamentals:
                                st.markdown("**ðŸ’° Fundamentals**")
                                if 'pe' in stock and pd.notna(stock['pe']): pe_val = stock['pe']; st.text(f"PE Ratio: ðŸŸ¢ {pe_val:.1f}x" if pe_val < 15 else f"PE Ratio: ðŸŸ¡ {pe_val:.1f}x" if pe_val < 25 else f"PE Ratio: ðŸ”´ {pe_val:.1f}x") if pe_val > 0 else st.text("PE Ratio: ðŸ”´ Loss")
                                else: st.text("PE Ratio: N/A")
                                if 'eps_current' in stock and pd.notna(stock['eps_current']): st.text(f"EPS Current: â‚¹{stock['eps_current']:.2f}")
                                else: st.text("EPS Current: N/A")
                                if 'eps_change_pct' in stock and pd.notna(stock['eps_change_pct']): eps_chg = stock['eps_change_pct']; eps_emoji = "ðŸš€" if eps_chg >= 100 else "ðŸ”¥" if eps_chg >= 50 else "ðŸ“ˆ" if eps_chg >= 0 else "ðŸ“‰"; st.text(f"EPS Growth: {eps_emoji} {eps_chg:+.0f}%" if eps_chg >= 100 else f"EPS Growth: {eps_emoji} {eps_chg:+.1f}%")
                                else: st.text("EPS Growth: N/A")
                        with detail_cols_top[1]:
                            st.markdown("**ðŸ“ˆ Performance**")
                            for period, col in [("1 Day", 'ret_1d'), ("7 Days", 'ret_7d'), ("30 Days", 'ret_30d'), ("3 Months", 'ret_3m'), ("6 Months", 'ret_6m'), ("1 Year", 'ret_1y')]:
                                if col in stock.index and pd.notna(stock[col]): st.text(f"{period}: {stock[col]:+.1f}%")
                                else: st.text(f"{period}: N/A")
                        st.markdown("---"); detail_cols_tech = st.columns([1,1])
                        with detail_cols_tech[0]:
                            st.markdown("**ðŸ” Technicals**")
                            if all(col in stock.index and pd.notna(stock[col]) for col in ['low_52w', 'high_52w']): st.text(f"52W Low: â‚¹{stock.get('low_52w', 0):,.0f}"); st.text(f"52W High: â‚¹{stock.get('high_52w', 0):,.0f}")
                            else: st.text("52W Range: N/A")
                            st.text(f"From High: {stock.get('from_high_pct', 'N/A'):.0f}%" if pd.notna(stock.get('from_high_pct')) else "From High: N/A")
                            st.text(f"From Low: {stock.get('from_low_pct', 'N/A'):.0f}%" if pd.notna(stock.get('from_low_pct')) else "From Low: N/A")
                            st.markdown("**ðŸ“Š Trading Position**")
                            tp_col1, tp_col2, tp_col3 = st.columns(3); current_price = stock.get('price', 0); sma_checks = [('sma_20d', '20DMA'), ('sma_50d', '50DMA'), ('sma_200d', '200DMA')]
                            for i, (sma_col, sma_label) in enumerate(sma_checks):
                                display_col = [tp_col1, tp_col2, tp_col3][i]
                                with display_col:
                                    sma_value = stock.get(sma_col);
                                    if pd.notna(sma_value) and sma_value > 0:
                                        if current_price > sma_value: pct_diff = ((current_price - sma_value) / sma_value) * 100; st.markdown(f"**{sma_label}**: <span style='color:green'>â†‘{pct_diff:.1f}%</span>", unsafe_allow_html=True)
                                        else: pct_diff = ((sma_value - current_price) / sma_value) * 100; st.markdown(f"**{sma_label}**: <span style='color:red'>â†“{pct_diff:.1f}%</span>", unsafe_allow_html=True)
                                    else: st.markdown(f"**{sma_label}**: N/A")
                        with detail_cols_tech[1]:
                            st.markdown("**ðŸ“ˆ Trend Analysis**")
                            if 'trend_quality' in stock.index and pd.notna(stock['trend_quality']):
                                tq = stock['trend_quality']
                                if tq >= 80: st.markdown(f"ðŸ”¥ Strong Uptrend ({tq:.0f})", help_text="Price is above all key moving averages, and they are aligned.")
                                elif tq >= 60: st.markdown(f"âœ… Good Uptrend ({tq:.0f})", help_text="Price is above most key moving averages.")
                                elif tq >= 40: st.markdown(f"âž¡ï¸ Neutral Trend ({tq:.0f})", help_text="Price is oscillating around moving averages.")
                                else: st.markdown(f"âš ï¸ Weak/Downtrend ({tq:.0f})", help_text="Price is below most key moving averages.")
                            else: st.markdown("Trend: N/A")
                            st.markdown("---"); st.markdown("#### ðŸŽ¯ Advanced Metrics"); adv_col1, adv_col2 = st.columns(2)
                            with adv_col1:
                                if 'vmi' in stock and pd.notna(stock['vmi']): st.metric("VMI", f"{stock['vmi']:.2f}", help_text="Volume Momentum Index: measures the strength of volume trend across timeframes.")
                                else: st.metric("VMI", "N/A")
                                if 'momentum_harmony' in stock and pd.notna(stock['momentum_harmony']): harmony_val = stock['momentum_harmony']; harmony_emoji = "ðŸŸ¢" if harmony_val >= 3 else "ðŸŸ¡" if harmony_val >= 2 else "ðŸ”´"; st.metric("Harmony", f"{harmony_emoji} {int(harmony_val)}/4", help_text="Momentum Harmony: measures alignment of returns across multiple timeframes (1D, 7D, 30D, 3M). Max 4/4 is perfect alignment.")
                                else: st.metric("Harmony", "N/A")
                            with adv_col2:
                                if 'position_tension' in stock and pd.notna(stock['position_tension']): st.metric("Position Tension", f"{stock['position_tension']:.0f}", help_text="Measures the stock's position within its 52-week range relative to its volatility. Higher implies more 'tension' or readiness for a move.")
                                else: st.metric("Position Tension", "N/A")
                                if 'money_flow_mm' in stock and pd.notna(stock['money_flow_mm']): st.metric("Money Flow", f"â‚¹{stock['money_flow_mm']:.1f}M", help_text="Estimated institutional money flow in millions (Price * Volume * RVOL).")
                                else: st.metric("Money Flow", "N/A")
            else: st.warning("No stocks found matching your search criteria.")
    with tabs[5]:
        st.markdown("### ðŸ“¥ Export Data"); st.markdown("#### ðŸ“‹ Export Templates")
        export_template = st.radio("Choose export template:", options=["Full Analysis (All Data)", "Day Trader Focus", "Swing Trader Focus", "Investor Focus"], key="wd_export_template_radio", help="Select a template based on your trading style")
        template_map = {"Full Analysis (All Data)": "full", "Day Trader Focus": "day_trader", "Swing Trader Focus": "swing_trader", "Investor Focus": "investor"}
        selected_template = template_map[export_template]
        col1, col2 = st.columns(2);
        with col1:
            st.markdown("#### ðŸ“Š Excel Report"); st.markdown("Comprehensive multi-sheet report including: - Top 100 stocks with all scores - Market intelligence dashboard - Sector rotation analysis - Industry rotation analysis (NEW) - Pattern frequency analysis - Wave Radar signals - Summary statistics")
            if st.button("Generate Excel Report", type="primary", use_container_width=True, key="wd_generate_excel"):
                if len(filtered_df) == 0: st.error("No data to export. Please adjust your filters.")
                else:
                    with st.spinner("Creating Excel report..."):
                        try: excel_file = ExportEngine.create_excel_report(filtered_df, template=selected_template)
                        except Exception as e: st.error(f"Error generating Excel report: {e}"); logger.error(f"Excel export error: {e}", exc_info=True); excel_file = None
                        if excel_file: st.download_button(label="ðŸ“¥ Download Excel Report", data=excel_file, file_name=f"wave_detection_report_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key="wd_download_excel_button"); st.success("Excel report generated successfully!")
        with col2:
            st.markdown("#### ðŸ“„ CSV Export"); st.markdown("Enhanced CSV format with: - All ranking scores - Advanced metrics (VMI, Money Flow) - Pattern detections - Wave states - Category classifications - Optimized for further analysis")
            if st.button("Generate CSV Export", use_container_width=True, key="wd_generate_csv"):
                if len(filtered_df) == 0: st.error("No data to export. Please adjust your filters.")
                else:
                    try: csv_data = ExportEngine.create_csv_export(filtered_df)
                    except Exception as e: st.error(f"Error generating CSV: {e}"); logger.error(f"CSV export error: {e}", exc_info=True); csv_data = None
                    if csv_data: st.download_button(label="ðŸ“¥ Download CSV File", data=csv_data, file_name=f"wave_detection_data_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv", key="wd_download_csv_button"); st.success("CSV export generated successfully!")
        st.markdown("---"); st.markdown("#### ðŸ“Š Export Preview")
        export_stats = {"Total Stocks": len(filtered_df), "Average Score": f"{filtered_df['master_score'].mean():.1f}" if not filtered_df.empty else "N/A", "Stocks with Patterns": (filtered_df['patterns'].fillna('') != '').sum() if 'patterns' in filtered_df.columns else 0, "High RVOL (>2x)": (filtered_df['rvol'].fillna(0) > 2).sum() if 'rvol' in filtered_df.columns else 0, "Positive 30D Returns": (filtered_df['ret_30d'].fillna(0) > 0).sum() if 'ret_30d' in filtered_df.columns else 0, "Data Quality": f"{st.session_state.data_quality.get('completeness', 0):.1f}%"}
        stat_cols = st.columns(3);
        for i, (label, value) in enumerate(export_stats.items()): with stat_cols[i % 3]: UIComponents.render_metric_card(label, value)
    with tabs[6]:
        st.markdown("### â„¹ï¸ About Wave Detection Ultimate 3.0 - Final Production Version")
        col1, col2 = st.columns([2, 1])
        with col1:
            st.markdown("""#### ðŸŒŠ Welcome to Wave Detection Ultimate 3.0
            The FINAL production version of the most advanced stock ranking system designed to catch momentum waves early.
            This professional-grade tool combines technical analysis, volume dynamics, advanced metrics, and smart pattern recognition to identify high-potential stocks before they peak.
            #### ðŸŽ¯ Core Features - LOCKED IN PRODUCTION
            **Master Score 3.0** - Proprietary ranking algorithm (DO NOT MODIFY): - **Position Analysis (30%)** - 52-week range positioning - **Volume Dynamics (25%)** - Multi-timeframe volume patterns - **Momentum Tracking (15%)** - 30-day price momentum - **Acceleration Detection (10%)** - Momentum acceleration signals - **Breakout Probability (10%)** - Technical breakout readiness - **RVOL Integration (10%)** - Real-time relative volume
            **Advanced Metrics** - NEW IN FINAL VERSION: - **Money Flow** - Price Ã— Volume Ã— RVOL in millions - **VMI (Volume Momentum Index)** - Weighted volume trend score - **Position Tension** - Range position stress indicator - **Momentum Harmony** - Multi-timeframe alignment (0-4) - **Wave State** - Real-time momentum classification - **Overall Wave Strength** - Composite score for wave filter
            **Wave Radarâ„¢** - Enhanced detection system: - Momentum shift detection with signal counting - Smart money flow tracking by category and **industry (NEW)** - Pattern emergence alerts with distance metrics - Market regime detection (Risk-ON/OFF/Neutral) - Sensitivity controls (Conservative/Balanced/Aggressive)
            **25 Pattern Detection** - Complete set: - 11 Technical patterns - 5 Fundamental patterns (Hybrid mode) - 6 Price range patterns - 3 NEW intelligence patterns (Stealth, Vampire, Perfect Storm)
            #### ðŸ’¡ How to Use
            1. **Data Source** - Google Sheets (default) or CSV upload 2. **Quick Actions** - Instant filtering for common scenarios 3. **Smart Filters** - Interconnected filtering system, including new Wave filters 4. **Display Modes** - Technical or Hybrid (with fundamentals) 5. **Wave Radar** - Monitor early momentum signals 6. **Export Templates** - Customized for trading styles
            #### ðŸ”§ Production Features
            - **Performance Optimized** - Sub-2 second processing - **Memory Efficient** - Handles 2000+ stocks smoothly - **Error Resilient** - Graceful degradation with retry logic - **Data Validation** - Comprehensive quality checks with clipping alerts - **Smart Caching** - Daily invalidation for data freshness - **Mobile Responsive** - Works on all devices
            #### ðŸ“Š Data Processing Pipeline
            1. Load from Google Sheets or CSV 2. Validate and clean all 41 columns (with clipping notifications) 3. Calculate 6 component scores 4. Generate Master Score 3.0 5. Calculate advanced metrics 6. Detect all 25 patterns 7. Classify into tiers 8. Apply smart ranking
            #### ðŸŽ¨ Display Modes
            **Technical Mode** (Default) - Pure momentum analysis - Technical indicators only - Pattern detection - Volume dynamics
            **Hybrid Mode** - All technical features - PE ratio analysis - EPS growth tracking - Fundamental patterns - Value indicators""")
        with col2:
            st.markdown("""
            #### ðŸ“ˆ Pattern Groups
            **Technical Patterns** - ðŸ”¥ CAT LEADER - ðŸ’Ž HIDDEN GEM - ðŸš€ ACCELERATING - ðŸ¦ INSTITUTIONAL - âš¡ VOL EXPLOSION - ðŸŽ¯ BREAKOUT - ðŸ‘‘ MARKET LEADER - ðŸŒŠ MOMENTUM WAVE - ðŸ’° LIQUID LEADER - ðŸ’ª LONG STRENGTH - ðŸ“ˆ QUALITY TREND
            **Range Patterns** - ðŸŽ¯ 52W HIGH APPROACH - ðŸ”„ 52W LOW BOUNCE - ðŸ‘‘ GOLDEN ZONE - ðŸ“Š VOL ACCUMULATION - ðŸ”€ MOMENTUM DIVERGE - ðŸŽ¯ RANGE COMPRESS
            **NEW Intelligence** - ðŸ¤« STEALTH - ðŸ§› VAMPIRE - â›ˆï¸ PERFECT STORM
            **Fundamental** (Hybrid) - ðŸ’Ž VALUE MOMENTUM - ðŸ“Š EARNINGS ROCKET - ðŸ† QUALITY LEADER - âš¡ TURNAROUND - âš ï¸ HIGH PE
            #### âš¡ Performance
            - Initial load: <2 seconds - Filtering: <200ms - Pattern detection: <500ms - Search: <50ms - Export: <1 second
            #### ðŸ”’ Production Status
            **Version**: 3.0.8-FINAL-PRODUCTION
            **Last Updated**: July 2025
            **Status**: PRODUCTION
            **Updates**: LOCKED
            **Testing**: COMPLETE
            **Optimization**: MAXIMUM
            #### ðŸ’¬ Credits
            Developed for professional traders requiring reliable, fast, and comprehensive market analysis.
            This is the FINAL version. No further updates will be made. All features are permanent.
            ---
            **Indian Market Optimized**
            - â‚¹ Currency formatting - IST timezone aware - NSE/BSE categories - Local number formats""")
        st.markdown("---")
        st.markdown("#### ðŸ“Š Current Session Statistics")
        stats_cols = st.columns(4);
        with stats_cols[0]: UIComponents.render_metric_card("Total Stocks Loaded", f"{len(ranked_df):,}" if 'ranked_df' in locals() and ranked_df is not None else "0", help_text="Total number of stocks loaded into the application before any filtering.")
        with stats_cols[1]: UIComponents.render_metric_card("Currently Filtered", f"{len(filtered_df):,}" if 'filtered_df' in locals() and filtered_df is not None else "0", help_text="Number of stocks remaining after applying all selected filters.")
        with stats_cols[2]: data_quality = st.session_state.data_quality.get('completeness', 0); quality_emoji = "ðŸŸ¢" if data_quality > 80 else "ðŸŸ¡" if data_quality > 60 else "ðŸ”´"; UIComponents.render_metric_card("Data Quality", f"{quality_emoji} {data_quality:.1f}%", help_text="Overall completeness percentage of data fields for loaded stocks.")
        with stats_cols[3]: cache_time = datetime.now(timezone.utc) - st.session_state.last_refresh; minutes = int(cache_time.total_seconds() / 60); cache_status = "Fresh" if minutes < 60 else "Stale"; cache_emoji = "ðŸŸ¢" if minutes < 60 else "ðŸ”´"; UIComponents.render_metric_card("Cache Age", f"{cache_emoji} {minutes} min", cache_status, help_text="Time since data was last refreshed from source or cache was cleared. Cache invalidates daily.")
    st.markdown("---")
    st.markdown("""<div style="text-align: center; color: #666; padding: 1rem;">ðŸŒŠ Wave Detection Ultimate 3.0 - Final Production Version<br><small>Professional Stock Ranking System â€¢ All Features Complete â€¢ Performance Optimized â€¢ Permanently Locked</small></div>""", unsafe_allow_html=True)
if __name__ == "__main__":
    try:
        import re
        main()
    except Exception as e:
        st.error(f"Critical Application Error: {str(e)}")
        logger.error(f"Application crashed: {str(e)}", exc_info=True)
        if st.button("ðŸ”„ Restart Application"): st.cache_data.clear(); st.rerun()
        if st.button("ðŸ“§ Report Issue"): st.info("Please take a screenshot and report this error.")
