"""
Wave Detection Ultimate 3.2 - FINAL ULTIMATE PRODUCTION VERSION
===============================================================
The definitive professional stock ranking system. This version consolidates the most robust architectural features, powerful analytics, and polished user experience from all previous iterations.

Version: 3.2.0-FINAL-ULTIMATE
Last Updated: August 2025
Status: PERMANENTLY LOCKED - Feature Complete & Bug-free
"""

# ============================================
# IMPORTS AND SETUP
# ============================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timezone
import logging
from typing import Dict, List, Tuple, Optional, Any, Union, Set
from dataclasses import dataclass, field
from functools import lru_cache, wraps
import time
from io import BytesIO
import warnings
import gc
import re
import hashlib
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from collections import defaultdict
import json

# Suppress warnings for clean output
warnings.filterwarnings('ignore')

# Performance optimizations
np.seterr(all='ignore')
pd.options.mode.chained_assignment = None
pd.options.display.float_format = '{:.2f}'.format

# Set random seed for reproducibility
np.random.seed(42)

# ============================================
# LOGGING CONFIGURATION
# ============================================

# Production logging with performance tracking
log_level = logging.INFO

logging.basicConfig(
    level=log_level,
    format='%(asctime)s | %(name)s | %(levelname)s | %(funcName)s:%(lineno)d | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Performance tracking storage
performance_stats = defaultdict(list)

def log_performance(operation: str, duration: float):
    """Track performance metrics"""
    performance_stats[operation].append(duration)
    if len(performance_stats[operation]) > 100:
        performance_stats[operation] = performance_stats[operation][-100:]
    
    if duration > 1.0:
        logger.warning(f"{operation} took {duration:.2f}s")

# ============================================
# ROBUST SESSION STATE MANAGER
# ============================================

class RobustSessionState:
    """Bulletproof session state management - prevents all KeyErrors"""
    
    STATE_DEFAULTS = {
        # Core states
        'search_query': "",
        'last_refresh': None,
        'data_source': "sheet",
        'sheet_id': "",
        'gid': "",
        'user_preferences': {
            'default_top_n': 50,
            'display_mode': 'Technical',
            'last_filters': {}
        },
        'filters': {},
        'active_filter_count': 0,
        'quick_filter': None,
        'quick_filter_applied': False,
        'show_debug': False,
        'performance_metrics': {},
        'data_quality': {},
        'last_good_data': None,
        'session_id': None,
        'session_start': None,
        'validation_stats': defaultdict(int),
        'trigger_clear': False,
        'wd_current_page_rankings': 0,
        
        # All filter states with proper defaults
        'category_filter': [],
        'sector_filter': [],
        'industry_filter': [],
        'min_score': 0,
        'patterns': [],
        'trend_filter': "All Trends",
        'eps_tier_filter': [],
        'pe_tier_filter': [],
        'price_tier_filter': [],
        'min_eps_change': "",
        'min_pe': "",
        'max_pe': "",
        'require_fundamental_data': False,
        'wave_states_filter': [],
        'wave_strength_range_slider': (0, 100),
        'show_sensitivity_details': False,
        'show_market_regime': True,
        'wave_timeframe_select': "All Waves",
        'wave_sensitivity': "Balanced",
        'export_template_radio': "Full Analysis (All Data)",
        'display_mode_toggle': 0,
        
        # Data states
        'ranked_df': None,
        'data_timestamp': None,
        
        # UI states
        'search_input': ""
    }
    
    @staticmethod
    def safe_get(key: str, default: Any = None) -> Any:
        """Safely get a session state value with fallback"""
        if key not in st.session_state:
            st.session_state[key] = RobustSessionState.STATE_DEFAULTS.get(key, default)
        return st.session_state[key]
    
    @staticmethod
    def safe_set(key: str, value: Any) -> None:
        """Safely set a session state value"""
        st.session_state[key] = value
    
    @staticmethod
    def initialize():
        """Initialize all session state variables"""
        for key, default_value in RobustSessionState.STATE_DEFAULTS.items():
            if key not in st.session_state:
                if key == 'last_refresh' and default_value is None:
                    st.session_state[key] = datetime.now(timezone.utc)
                elif key == 'session_start' and default_value is None:
                    st.session_state[key] = datetime.now(timezone.utc)
                elif key == 'session_id' and default_value is None:
                    st.session_state[key] = hashlib.md5(
                        f"{datetime.now()}{np.random.rand()}".encode()
                    ).hexdigest()[:8]
                else:
                    st.session_state[key] = default_value
    
    @staticmethod
    def clear_filters():
        """Clear all filter states safely"""
        filter_keys = [
            'category_filter', 'sector_filter', 'industry_filter', 'eps_tier_filter',
            'pe_tier_filter', 'price_tier_filter', 'patterns',
            'min_score', 'trend_filter', 'min_eps_change',
            'min_pe', 'max_pe', 'require_fundamental_data',
            'quick_filter', 'quick_filter_applied',
            'wave_states_filter', 'wave_strength_range_slider',
            'show_sensitivity_details', 'show_market_regime',
            'wave_timeframe_select', 'wave_sensitivity', 'wd_current_page_rankings'
        ]
        
        for key in filter_keys:
            if key in RobustSessionState.STATE_DEFAULTS:
                RobustSessionState.safe_set(key, RobustSessionState.STATE_DEFAULTS[key])
        
        RobustSessionState.safe_set('filters', {})
        RobustSessionState.safe_set('active_filter_count', 0)
        RobustSessionState.safe_set('trigger_clear', False)
    
    @staticmethod
    def get_session_info() -> Dict[str, Any]:
        """Get session information"""
        return {
            'session_id': RobustSessionState.safe_get('session_id', 'unknown'),
            'start_time': RobustSessionState.safe_get('session_start', datetime.now()),
            'duration': (datetime.now() - RobustSessionState.safe_get('session_start', datetime.now())).seconds,
            'data_source': RobustSessionState.safe_get('data_source', 'unknown'),
            'stocks_loaded': len(RobustSessionState.safe_get('ranked_df', [])),
            'active_filters': RobustSessionState.safe_get('active_filter_count', 0)
        }

# ============================================
# CONFIGURATION AND CONSTANTS
# ============================================

@dataclass(frozen=True)
class Config:
    """System configuration with validated weights and thresholds"""
    
    # Data source
    DEFAULT_SHEET_URL_TEMPLATE: str = "https://docs.google.com/spreadsheets/d/{sheet_id}/edit?usp=sharing"
    CSV_URL_TEMPLATE: str = "https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}"
    DEFAULT_GID: str = "1823439984"
    
    # Intelligent retry settings
    REQUEST_TIMEOUT: int = 30
    MAX_RETRY_ATTEMPTS: int = 3
    RETRY_BACKOFF_FACTOR: float = 0.5
    RETRY_STATUS_CODES: Tuple[int, ...] = (408, 429, 500, 502, 503, 504)
    
    # Cache settings
    CACHE_TTL: int = 3600
    STALE_DATA_HOURS: int = 24
    
    # Master Score 3.0 weights (total = 100%) - DO NOT MODIFY
    POSITION_WEIGHT: float = 0.30
    VOLUME_WEIGHT: float = 0.25
    MOMENTUM_WEIGHT: float = 0.15
    ACCELERATION_WEIGHT: float = 0.10
    BREAKOUT_WEIGHT: float = 0.10
    RVOL_WEIGHT: float = 0.10
    
    # Display settings
    DEFAULT_TOP_N: int = 50
    AVAILABLE_TOP_N: List[int] = field(default_factory=lambda: [10, 20, 50, 100, 200, 500])
    
    # Critical columns
    CRITICAL_COLUMNS: List[str] = field(default_factory=lambda: ['ticker', 'price', 'volume_1d'])
    
    # Important columns
    IMPORTANT_COLUMNS: List[str] = field(default_factory=lambda: [
        'ret_30d', 'from_low_pct', 'from_high_pct',
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d', 'volume_90d', 'volume_30d', 'volume_7d',
        'ret_7d', 'category', 'sector', 'industry', 'rvol', 'market_cap'
    ])
    
    # All percentage columns
    PERCENTAGE_COLUMNS: List[str] = field(default_factory=lambda: [
        'from_low_pct', 'from_high_pct',
        'ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 
        'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y',
        'eps_change_pct'
    ])
    
    # Volume ratio columns
    VOLUME_RATIO_COLUMNS: List[str] = field(default_factory=lambda: [
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d'
    ])
    
    # Pattern thresholds
    PATTERN_THRESHOLDS: Dict[str, float] = field(default_factory=lambda: {
        "category_leader": 90,
        "hidden_gem": 80,
        "acceleration": 85,
        "institutional": 75,
        "vol_explosion": 95,
        "breakout_ready": 80,
        "market_leader": 95,
        "momentum_wave": 75,
        "liquid_leader": 80,
        "long_strength": 80,
        "52w_high_approach": 90,
        "52w_low_bounce": 85,
        "golden_zone": 85,
        "vol_accumulation": 80,
        "momentum_diverge": 90,
        "range_compress": 75,
        "stealth": 70,
        "vampire": 85,
        "perfect_storm": 80
    })
    
    # Pattern metadata for confidence scoring
    PATTERN_METADATA: Dict[str, Dict[str, str]] = field(default_factory=lambda: {
        'ðŸ”¥ CAT LEADER': {'importance': 'high', 'risk': 'low'},
        'ðŸ’Ž HIDDEN GEM': {'importance': 'high', 'risk': 'medium'},
        'ðŸš€ ACCELERATING': {'importance': 'high', 'risk': 'medium'},
        'ðŸ¦ INSTITUTIONAL': {'importance': 'high', 'risk': 'low'},
        'âš¡ VOL EXPLOSION': {'importance': 'very_high', 'risk': 'high'},
        'ðŸŽ¯ BREAKOUT': {'importance': 'high', 'risk': 'medium'},
        'ðŸ‘‘ MARKET LEADER': {'importance': 'very_high', 'risk': 'low'},
        'ðŸŒŠ MOMENTUM WAVE': {'importance': 'high', 'risk': 'medium'},
        'ðŸ’° LIQUID LEADER': {'importance': 'medium', 'risk': 'low'},
        'ðŸ’ª LONG STRENGTH': {'importance': 'medium', 'risk': 'low'},
        'ðŸ“ˆ QUALITY TREND': {'importance': 'high', 'risk': 'low'},
        'ðŸ’Ž VALUE MOMENTUM': {'importance': 'medium', 'risk': 'low'},
        'ðŸ“Š EARNINGS ROCKET': {'importance': 'high', 'risk': 'medium'},
        'ðŸ† QUALITY LEADER': {'importance': 'high', 'risk': 'low'},
        'âš¡ TURNAROUND': {'importance': 'medium', 'risk': 'high'},
        'âš ï¸ HIGH PE': {'importance': 'low', 'risk': 'high'},
        'ðŸŽ¯ 52W HIGH APPROACH': {'importance': 'high', 'risk': 'medium'},
        'ðŸ”„ 52W LOW BOUNCE': {'importance': 'high', 'risk': 'high'},
        'ðŸ‘‘ GOLDEN ZONE': {'importance': 'high', 'risk': 'low'},
        'ðŸ“Š VOL ACCUMULATION': {'importance': 'medium', 'risk': 'low'},
        'ðŸ”€ MOMENTUM DIVERGE': {'importance': 'high', 'risk': 'high'},
        'ðŸŽ¯ RANGE COMPRESS': {'importance': 'medium', 'risk': 'medium'},
        'ðŸ¤« STEALTH': {'importance': 'high', 'risk': 'medium'},
        'ðŸ§› VAMPIRE': {'importance': 'high', 'risk': 'high'},
        'â›ˆï¸ PERFECT STORM': {'importance': 'very_high', 'risk': 'low'}
    })
    
    # Value bounds for data validation
    VALUE_BOUNDS: Dict[str, Tuple[float, float]] = field(default_factory=lambda: {
        'price': (0.01, 1_000_000),
        'rvol': (0.01, 1_000_000.0),
        'pe': (-10000, 10000),
        'returns': (-99.99, 9999.99),
        'volume': (0, 1e15)
    })
    
    # Performance thresholds
    PERFORMANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        'data_processing': 2.0,
        'filtering': 0.2,
        'pattern_detection': 0.5,
        'export_generation': 1.0,
        'search': 0.05
    })
    
    # Market categories (Indian market specific)
    MARKET_CATEGORIES: List[str] = field(default_factory=lambda: [
        'Mega Cap', 'Large Cap', 'Mid Cap', 'Small Cap', 'Micro Cap'
    ])
    
    # Tier definitions with proper boundaries
    TIERS: Dict[str, Dict[str, Tuple[float, float]]] = field(default_factory=lambda: {
        "eps": {
            "Loss": (-float('inf'), 0),
            "0-5": (0, 5),
            "5-10": (5, 10),
            "10-20": (10, 20),
            "20-50": (20, 50),
            "50-100": (50, 100),
            "100+": (100, float('inf'))
        },
        "pe": {
            "Negative/NA": (-float('inf'), 0),
            "0-10": (0, 10),
            "10-15": (10, 15),
            "15-20": (15, 20),
            "20-30": (20, 30),
            "30-50": (30, 50),
            "50+": (50, float('inf'))
        },
        "price": {
            "0-100": (0, 100),
            "100-250": (100, 250),
            "250-500": (250, 500),
            "500-1000": (500, 1000),
            "1000-2500": (1000, 2500),
            "2500-5000": (2500, 5000),
            "5000+": (5000, float('inf'))
        }
    })

# Global configuration instance
CONFIG = Config()

# ============================================
# PERFORMANCE MONITORING
# ============================================

class PerformanceMonitor:
    """Track and report performance metrics"""
    
    @staticmethod
    def timer(target_time: Optional[float] = None):
        """Performance timing decorator with target comparison"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                try:
                    result = func(*args, **kwargs)
                    elapsed = time.perf_counter() - start
                    
                    log_performance(func.__name__, elapsed)
                    
                    if target_time and elapsed > target_time:
                        logger.warning(f"{func.__name__} took {elapsed:.2f}s (target: {target_time}s)")
                    elif elapsed > 1.0:
                        logger.info(f"{func.__name__} completed in {elapsed:.2f}s")
                    
                    perf_metrics = RobustSessionState.safe_get('performance_metrics', {})
                    perf_metrics[func.__name__] = elapsed
                    RobustSessionState.safe_set('performance_metrics', perf_metrics)
                    
                    return result
                except Exception as e:
                    elapsed = time.perf_counter() - start
                    logger.error(f"{func.__name__} failed after {elapsed:.2f}s: {str(e)}")
                    raise
            return wrapper
        return decorator
    
    @staticmethod
    def get_performance_summary() -> Dict[str, Dict[str, float]]:
        """Get performance statistics"""
        summary = {}
        for op, durations in performance_stats.items():
            if durations:
                summary[op] = {
                    'avg': np.mean(durations),
                    'min': np.min(durations),
                    'max': np.max(durations),
                    'p95': np.percentile(durations, 95) if len(durations) > 1 else durations[0]
                }
        return summary

# ============================================
# DATA VALIDATION AND SANITIZATION
# ============================================

class DataValidator:
    """Comprehensive data validation with tracking"""
    
    def __init__(self):
        self.validation_stats = defaultdict(int)
        self.correction_stats = defaultdict(int)
    
    def reset_stats(self):
        """Reset validation statistics"""
        self.validation_stats.clear()
        self.correction_stats.clear()
    
    def get_validation_report(self) -> Dict[str, Any]:
        """Get validation report"""
        return {
            'validations': dict(self.validation_stats),
            'corrections': dict(self.correction_stats),
            'total_issues': sum(self.correction_stats.values())
        }
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame, required_columns: List[str], 
                          context: str = "") -> Tuple[bool, str]:
        """Validate dataframe structure"""
        if df is None:
            return False, f"{context}: DataFrame is None"
        
        if df.empty:
            return False, f"{context}: DataFrame is empty"
        
        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            return False, f"{context}: Missing required columns: {missing_columns}"
        
        if len(df) < 1:
            return False, f"{context}: No data rows found"
        
        return True, "Validation passed"
    
    def clean_numeric_value(self, value: Any, is_percentage: bool = False, 
                           bounds: Optional[Tuple[float, float]] = None,
                           column_name: str = "") -> float:
        """Clean and validate numeric values with tracking"""
        self.validation_stats[f'{column_name}_total'] += 1
        
        try:
            if pd.isna(value) or value is None:
                self.correction_stats[f'{column_name}_nan'] += 1
                return np.nan
            
            cleaned = str(value).strip()
            
            if cleaned.upper() in ['', 'N/A', 'NA', 'NAN', 'NONE', 'NULL', '-', 
                                  '#N/A', '#ERROR!', '#DIV/0!', 'INF', '-INF']:
                self.correction_stats[f'{column_name}_invalid'] += 1
                return np.nan
            
            cleaned = cleaned.replace('â‚¹', '').replace('$', '').replace(',', '')\
                           .replace(' ', '').replace('%', '')
            
            result = float(cleaned)
            
            if bounds:
                min_val, max_val = bounds
                if result < min_val or result > max_val:
                    self.correction_stats[f'{column_name}_clipped'] += 1
                    result = np.clip(result, min_val, max_val)
            
            if np.isnan(result) or np.isinf(result):
                self.correction_stats[f'{column_name}_infinite'] += 1
                return np.nan
            
            return result
            
        except (ValueError, TypeError, AttributeError):
            self.correction_stats[f'{column_name}_error'] += 1
            return np.nan
    
    @staticmethod
    def sanitize_string(value: Any, default: str = "Unknown") -> str:
        """Sanitize string values"""
        if pd.isna(value) or value is None:
            return default
        
        cleaned = str(value).strip()
        if cleaned.upper() in ['', 'N/A', 'NA', 'NAN', 'NONE', 'NULL', '-']:
            return default
        
        cleaned = ' '.join(cleaned.split())
        
        return cleaned

# Global validator instance
validator = DataValidator()

# ============================================
# SMART CACHING WITH VERSIONING
# ============================================

def get_requests_session() -> requests.Session:
    """Create requests session with retry logic."""
    session = requests.Session()
    
    retry = Retry(
        total=CONFIG.MAX_RETRY_ATTEMPTS,
        read=CONFIG.MAX_RETRY_ATTEMPTS,
        connect=CONFIG.MAX_RETRY_ATTEMPTS,
        backoff_factor=CONFIG.RETRY_BACKOFF_FACTOR,
        status_forcelist=CONFIG.RETRY_STATUS_CODES,
        allowed_methods=["HEAD", "GET", "OPTIONS"]
    )
    
    adapter = HTTPAdapter(
        max_retries=retry,
        pool_connections=10,
        pool_maxsize=20
    )
    
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    session.headers.update({
        'User-Agent': 'Wave Detection Ultimate 3.2',
        'Accept': 'text/csv,application/csv,text/plain',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive'
    })
    
    return session

@st.cache_data(ttl=CONFIG.CACHE_TTL, persist="disk", show_spinner=False)
def load_and_process_data(source_type: str = "sheet", file_data=None, 
                         sheet_id: str = None, gid: str = None,
                         data_version: str = "1.0") -> Tuple[pd.DataFrame, datetime, Dict[str, Any]]:
    """Load and process data with smart caching and versioning."""
    
    start_time = time.perf_counter()
    metadata = {
        'source_type': source_type,
        'data_version': data_version,
        'processing_start': datetime.now(timezone.utc),
        'errors': [],
        'warnings': [],
        'performance': {}
    }
    
    try:
        validator.reset_stats()
        
        if source_type == "upload" and file_data is not None:
            logger.info("Loading data from uploaded CSV")
            try:
                df = pd.read_csv(file_data, low_memory=False)
                metadata['source'] = "User Upload"
            except UnicodeDecodeError:
                for encoding in ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']:
                    try:
                        file_data.seek(0)
                        df = pd.read_csv(file_data, low_memory=False, encoding=encoding)
                        metadata['warnings'].append(f"Used {encoding} encoding")
                        break
                    except:
                        continue
                else:
                    raise ValueError("Unable to decode CSV file")
        else:
            if not sheet_id:
                raise ValueError("Please enter a Google Sheets ID")
            
            sheet_id_match = re.search(r'/d/([a-zA-Z0-9-_]+)', sheet_id)
            if sheet_id_match:
                sheet_id = sheet_id_match.group(1)
            
            if not gid:
                gid = CONFIG.DEFAULT_GID
            
            csv_url = CONFIG.CSV_URL_TEMPLATE.format(sheet_id=sheet_id, gid=gid)
            
            logger.info(f"Loading data from Google Sheets ID: {sheet_id}")
            
            session = get_requests_session()
            
            try:
                response = session.get(csv_url, timeout=CONFIG.REQUEST_TIMEOUT)
                response.raise_for_status()
                
                if len(response.content) < 100:
                    raise ValueError("Response too small, likely an error page")
                
                df = pd.read_csv(BytesIO(response.content), low_memory=False)
                metadata['source'] = "Google Sheets"
                metadata['sheet_id'] = sheet_id
                
            except requests.exceptions.RequestException as e:
                logger.error(f"Failed to load from Google Sheets: {str(e)}")
                
                last_good_data = RobustSessionState.safe_get('last_good_data')
                if last_good_data:
                    logger.info("Using cached data as fallback")
                    df, timestamp, old_metadata = last_good_data
                    metadata['warnings'].append("Using cached data due to load failure")
                    metadata['cache_used'] = True
                    return df, timestamp, metadata
                raise
        
        metadata['performance']['load_time'] = time.perf_counter() - start_time
        
        is_valid, validation_msg = DataValidator.validate_dataframe(
            df, CONFIG.CRITICAL_COLUMNS, "Initial load"
        )
        if not is_valid:
            raise ValueError(validation_msg)
        
        processing_start = time.perf_counter()
        df = DataProcessor.process_dataframe(df, metadata)
        metadata['performance']['processing_time'] = time.perf_counter() - processing_start
        
        scoring_start = time.perf_counter()
        df = RankingEngine.calculate_all_scores(df)
        metadata['performance']['scoring_time'] = time.perf_counter() - scoring_start
        
        pattern_start = time.perf_counter()
        df = PatternDetector.detect_all_patterns_optimized(df)
        metadata['performance']['pattern_time'] = time.perf_counter() - pattern_start
        
        metrics_start = time.perf_counter()
        df = AdvancedMetrics.calculate_all_metrics(df)
        metadata['performance']['metrics_time'] = time.perf_counter() - metrics_start
        
        is_valid, validation_msg = DataValidator.validate_dataframe(
            df, ['master_score', 'rank'], "Final processed"
        )
        if not is_valid:
            raise ValueError(validation_msg)
        
        validation_report = validator.get_validation_report()
        if validation_report['total_issues'] > 0:
            metadata['warnings'].append(
                f"Data quality: {validation_report['total_issues']} issues auto-corrected"
            )
            metadata['validation_report'] = validation_report
        
        timestamp = datetime.now(timezone.utc)
        RobustSessionState.safe_set('last_good_data', (df.copy(), timestamp, metadata))
        
        total_time = time.perf_counter() - start_time
        metadata['performance']['total_time'] = total_time
        metadata['processing_end'] = datetime.now(timezone.utc)
        
        logger.info(
            f"Data processing complete: {len(df)} stocks in {total_time:.2f}s "
            f"(Load: {metadata['performance'].get('load_time', 0):.2f}s, "
            f"Process: {metadata['performance'].get('processing_time', 0):.2f}s, "
            f"Score: {metadata['performance'].get('scoring_time', 0):.2f}s)"
        )
        
        gc.collect()
        
        return df, timestamp, metadata
        
    except Exception as e:
        logger.error(f"Failed to load and process data: {str(e)}")
        metadata['errors'].append(str(e))
        
        last_good_data = RobustSessionState.safe_get('last_good_data')
        if last_good_data:
            df, timestamp, old_metadata = last_good_data
            metadata['warnings'].append("Using previously cached data due to error")
            metadata['cache_used'] = True
            metadata['original_error'] = str(e)
            return df, timestamp, metadata
        
        raise

# ============================================
# DATA PROCESSING ENGINE
# ============================================

class DataProcessor:
    """Handle all data processing with validation and optimization"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def process_dataframe(df: pd.DataFrame, metadata: Dict[str, Any]) -> pd.DataFrame:
        """Complete data processing pipeline"""
        
        df = df.copy()
        initial_count = len(df)
        
        logger.info(f"Processing {initial_count} rows...")
        
        df['ticker'] = df['ticker'].apply(DataValidator.sanitize_string)
        
        numeric_cols = [col for col in df.columns if col not in 
                       ['ticker', 'company_name', 'category', 'sector', 'industry', 'year', 'market_cap']]
        
        for col in numeric_cols:
            if col in df.columns:
                is_pct = col in CONFIG.PERCENTAGE_COLUMNS
                
                if 'volume' in col.lower():
                    bounds = CONFIG.VALUE_BOUNDS['volume']
                elif col == 'rvol':
                    bounds = CONFIG.VALUE_BOUNDS['rvol']
                elif col == 'pe':
                    bounds = CONFIG.VALUE_BOUNDS['pe']
                elif is_pct:
                    bounds = CONFIG.VALUE_BOUNDS['returns']
                else:
                    bounds = CONFIG.VALUE_BOUNDS.get('price', None)
                
                df[col] = df[col].apply(
                    lambda x: validator.clean_numeric_value(x, is_pct, bounds, col)
                )
        
        string_cols = ['ticker', 'company_name', 'category', 'sector', 'industry']
        for col in string_cols:
            if col in df.columns:
                df[col] = df[col].apply(DataValidator.sanitize_string)
        
        if 'industry' not in df.columns and 'sector' in df.columns:
            df['industry'] = df['sector']
            metadata['warnings'].append("Industry column created from sector data")
        
        for col in CONFIG.VOLUME_RATIO_COLUMNS:
            if col in df.columns:
                df[col] = (100 + df[col]) / 100
                df[col] = df[col].clip(0.01, 1000.0)
                df[col] = df[col].fillna(1.0)
        
        if 'rvol' not in df.columns or df['rvol'].isna().all():
            if 'volume_1d' in df.columns and 'volume_90d' in df.columns:
                with np.errstate(divide='ignore', invalid='ignore'):
                    df['rvol'] = np.where(
                        df['volume_90d'] > 0,
                        df['volume_1d'] / df['volume_90d'],
                        1.0
                    )
                metadata['warnings'].append("RVOL calculated from volume data")
        
        df = df.dropna(subset=['ticker', 'price'], how='any')
        df = df[df['price'] > 0.01]
        
        before_dedup = len(df)
        df = df.drop_duplicates(subset=['ticker'], keep='first')
        if before_dedup > len(df):
            metadata['warnings'].append(f"Removed {before_dedup - len(df)} duplicate tickers")
        
        df = DataProcessor._fill_missing_values(df)
        
        df = DataProcessor._add_tier_classifications(df)
        
        removed = initial_count - len(df)
        if removed > 0:
            metadata['warnings'].append(f"Removed {removed} invalid rows during processing")
        
        total_cells = len(df) * len(df.columns)
        missing_cells = df.isna().sum().sum()
        completeness = (1 - missing_cells / total_cells) * 100 if total_cells > 0 else 0
        
        metadata['data_quality'] = {
            'total_rows': len(df),
            'removed_rows': removed,
            'duplicate_tickers': before_dedup - len(df),
            'completeness': completeness,
            'columns_available': list(df.columns)
        }
        
        RobustSessionState.safe_set('data_quality', metadata['data_quality'])
        
        logger.info(f"Processed {len(df)} valid stocks from {initial_count} initial rows")
        
        return df
    
    @staticmethod
    def _fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:
        """Fill missing values with sensible defaults. This is a defensive
        method to ensure no NaN values remain in critical columns before scoring."""
        
        df['from_low_pct'] = df.get('from_low_pct', pd.Series(50.0, index=df.index)).fillna(50.0)
        df['from_high_pct'] = df.get('from_high_pct', pd.Series(-50.0, index=df.index)).fillna(-50.0)
        
        df['rvol'] = df.get('rvol', pd.Series(1.0, index=df.index)).fillna(1.0)
        
        return_cols = [col for col in df.columns if col.startswith('ret_')]
        for col in return_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0.0)
        
        volume_cols = [col for col in df.columns if col.startswith('volume_')]
        for col in volume_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        df['category'] = df.get('category', pd.Series('Unknown', index=df.index)).fillna('Unknown')
        df['sector'] = df.get('sector', pd.Series('Unknown', index=df.index)).fillna('Unknown')
        
        if 'industry' in df.columns:
            df['industry'] = df['industry'].fillna(df['sector'])
        else:
            df['industry'] = df['sector']
        
        return df
    
    @staticmethod
    def _add_tier_classifications(df: pd.DataFrame) -> pd.DataFrame:
        """Add tier classifications for filtering"""
        
        if 'eps_change_pct' in df.columns:
            conditions = [
                df['eps_change_pct'] < 0,
                (df['eps_change_pct'] >= 0) & (df['eps_change_pct'] < 20),
                (df['eps_change_pct'] >= 20) & (df['eps_change_pct'] < 50),
                (df['eps_change_pct'] >= 50) & (df['eps_change_pct'] < 100),
                df['eps_change_pct'] >= 100
            ]
            choices = ['Negative', 'Low (0-20%)', 'Medium (20-50%)', 
                       'High (50-100%)', 'Extreme (>100%)']
            df['eps_tier'] = np.select(conditions, choices, default='Unknown')
        
        if 'pe' in df.columns:
            conditions = [
                df['pe'] < 0,
                (df['pe'] >= 0) & (df['pe'] < 15),
                (df['pe'] >= 15) & (df['pe'] < 25),
                (df['pe'] >= 25) & (df['pe'] < 50),
                df['pe'] >= 50
            ]
            choices = ['Negative/NA', 'Value (<15)', 'Fair (15-25)', 
                       'Growth (25-50)', 'Expensive (>50)']
            df['pe_tier'] = np.select(conditions, choices, default='Unknown')
        
        if 'price' in df.columns:
            conditions = [
                df['price'] < 10,
                (df['price'] >= 10) & (df['price'] < 100),
                (df['price'] >= 100) & (df['price'] < 1000),
                (df['price'] >= 1000) & (df['price'] < 5000),
                df['price'] >= 5000
            ]
            choices = ['Penny (<â‚¹10)', 'Low (â‚¹10-100)', 'Mid (â‚¹100-1000)', 
                       'High (â‚¹1000-5000)', 'Premium (>â‚¹5000)']
            df['price_tier'] = np.select(conditions, choices, default='Unknown')
        
        return df

# ============================================
# ADVANCED METRICS CALCULATOR
# ============================================

class AdvancedMetrics:
    """Calculate advanced metrics and indicators"""
    
    @staticmethod
    def calculate_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all advanced metrics"""
        
        df['market_regime'] = AdvancedMetrics._detect_market_regime(df)

        if all(col in df.columns for col in ['price', 'volume_1d', 'rvol']):
            df['money_flow'] = df['price'] * df['volume_1d'] * df['rvol']
            df['money_flow_mm'] = df['money_flow'] / 1_000_000
        else:
            df['money_flow_mm'] = 0.0
        
        if all(col in df.columns for col in ['vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'vol_ratio_90d_180d']):
            df['vmi'] = (
                df['vol_ratio_1d_90d'] * 4 +
                df['vol_ratio_7d_90d'] * 3 +
                df['vol_ratio_30d_90d'] * 2 +
                df['vol_ratio_90d_180d'] * 1
            ) / 10
        else:
            df['vmi'] = 1.0
        
        if all(col in df.columns for col in ['from_low_pct', 'from_high_pct']):
            df['position_tension'] = df['from_low_pct'] + abs(df['from_high_pct'])
        else:
            df['position_tension'] = 100.0
        
        df['momentum_harmony'] = 0
        
        if 'ret_1d' in df.columns:
            df['momentum_harmony'] += (df['ret_1d'] > 0).astype(int)
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_ret_7d = np.where(df['ret_7d'] != 0, df['ret_7d'] / 7, 0)
                daily_ret_30d = np.where(df['ret_30d'] != 0, df['ret_30d'] / 30, 0)
            
            df['momentum_harmony'] += (daily_ret_7d > daily_ret_30d).astype(int)
        
        if all(col in df.columns for col in ['ret_30d', 'ret_3m']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_ret_30d_comp = np.where(df['ret_30d'] != 0, df['ret_30d'] / 30, 0)
                daily_ret_3m_comp = np.where(df['ret_3m'] != 0, df['ret_3m'] / 90, 0)
            df['momentum_harmony'] += (daily_ret_30d_comp > daily_ret_3m_comp).astype(int)
        
        if 'ret_3m' in df.columns:
            df['momentum_harmony'] += (df['ret_3m'] > 0).astype(int)

        df['smart_money_flow'] = AdvancedMetrics._calculate_smart_money_flow(df)
        
        df['wave_state'] = df.apply(AdvancedMetrics._get_wave_state, axis=1)

        if all(col in df.columns for col in ['momentum_score', 'acceleration_score', 'rvol_score', 'breakout_score']):
            df['overall_wave_strength'] = (
                df['momentum_score'] * 0.3 +
                df['acceleration_score'] * 0.3 +
                df['rvol_score'] * 0.2 +
                df['breakout_score'] * 0.2
            )
        else:
            df['overall_wave_strength'] = 50.0
        
        return df
    
    @staticmethod
    def _get_wave_state(row: pd.Series) -> str:
        """Determine wave state for a stock"""
        signals = 0
        
        if 'momentum_score' in row and row['momentum_score'] > 70:
            signals += 1
        if 'volume_score' in row and row['volume_score'] > 70:
            signals += 1
        if 'acceleration_score' in row and row['acceleration_score'] > 70:
            signals += 1
        if 'rvol' in row and row['rvol'] > 2:
            signals += 1
        
        if signals >= 4:
            return "ðŸŒŠðŸŒŠðŸŒŠ CRESTING"
        elif signals >= 3:
            return "ðŸŒŠðŸŒŠ BUILDING"
        elif signals >= 1:
            return "ðŸŒŠ FORMING"
        else:
            return "ðŸ’¥ BREAKING"
            
    @staticmethod
    def _detect_market_regime(df: pd.DataFrame) -> pd.Series:
        """Detect current market regime with supporting data."""
        if df.empty or 'ret_30d' not in df.columns:
            return pd.Series("ðŸ˜´ RANGE-BOUND", index=df.index)
        
        positive_breadth = (df['ret_30d'] > 0).mean()
        strong_positive = (df['ret_30d'] > 10).mean()
        strong_negative = (df['ret_30d'] < -10).mean()
        
        regime = np.full(len(df), "ðŸ˜´ RANGE-BOUND", dtype=object)
        
        if positive_breadth > 0.6 and strong_positive > 0.3:
            regime = "ðŸ”¥ RISK-ON BULL"
        elif positive_breadth < 0.4 and strong_negative > 0.3:
            regime = "ðŸ›¡ï¸ RISK-OFF DEFENSIVE"
        else:
            regime = "ðŸ˜´ RANGE-BOUND"
        
        return pd.Series(regime, index=df.index)
    
    @staticmethod
    def _calculate_smart_money_flow(df: pd.DataFrame) -> pd.Series:
        """Calculate smart money flow indicator."""
        smart_flow = pd.Series(50, index=df.index, dtype=float)
        
        if all(col in df.columns for col in ['vol_ratio_7d_90d', 'vol_ratio_30d_90d']):
            vol_persistence = (
                (df['vol_ratio_7d_90d'] > 1.2) & 
                (df['vol_ratio_30d_90d'] > 1.1)
            ).astype(float) * 20
            smart_flow += vol_persistence
        
        if 'ret_30d' in df.columns and 'volume_score' in df.columns:
            divergence = np.where(
                (np.abs(df['ret_30d']) < 5) & (df['volume_score'] > 70),
                20, 0
            )
            smart_flow += divergence
        
        if 'liquidity_score' in df.columns:
            institutional = np.where(df['liquidity_score'] > 80, 10, 0)
            smart_flow += institutional
        
        return smart_flow.clip(0, 100)
        
# ============================================
# RANKING ENGINE - OPTIMIZED
# ============================================

class RankingEngine:
    """Calculate all scores and rankings with performance optimization"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def calculate_all_scores(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all scores using vectorized operations"""
        
        if df.empty:
            return df
        
        logger.info(f"Calculating scores for {len(df)} stocks...")
        
        # Calculate individual component scores
        df['position_score'] = RankingEngine._calculate_position_score(df)
        df['volume_score'] = RankingEngine._calculate_volume_score(df)
        df['momentum_score'] = RankingEngine._calculate_momentum_score(df)
        df['acceleration_score'] = RankingEngine._calculate_acceleration_score(df)
        df['breakout_score'] = RankingEngine._calculate_breakout_score(df)
        df['rvol_score'] = RankingEngine._calculate_rvol_score(df)
        
        # Add quality indicators
        df['momentum_quality'] = RankingEngine._calculate_momentum_quality(df)
        df['trend_quality'] = RankingEngine._calculate_trend_quality(df)
        df['long_term_strength'] = RankingEngine._calculate_long_term_strength(df)
        df['liquidity_score'] = RankingEngine._calculate_liquidity_score(df)
        
        # Calculate Master Score 3.0
        scores_matrix = df[['position_score', 'volume_score', 'momentum_score',
                           'acceleration_score', 'breakout_score', 'rvol_score']].fillna(50).values
        
        weights = np.array([
            CONFIG.POSITION_WEIGHT,
            CONFIG.VOLUME_WEIGHT,
            CONFIG.MOMENTUM_WEIGHT,
            CONFIG.ACCELERATION_WEIGHT,
            CONFIG.BREAKOUT_WEIGHT,
            CONFIG.RVOL_WEIGHT
        ])
        
        df['master_score'] = np.dot(scores_matrix, weights).clip(0, 100)
        
        # Add quality bonus (max 5 points)
        if 'momentum_quality' in df.columns:
            quality_bonus = df['momentum_quality'] * 0.05
            df['master_score'] = (df['master_score'] + quality_bonus).clip(0, 100)
        
        if 'smart_money_flow' in df.columns:
            flow_bonus = np.where(df['smart_money_flow'] > 70, 3, 0)
            df['master_score'] = (df['master_score'] + flow_bonus).clip(0, 100)

        df['rank'] = df['master_score'].rank(method='first', ascending=False, na_option='bottom')
        df['rank'] = df['rank'].fillna(len(df) + 1).astype(int)
        
        df['percentile'] = df['master_score'].rank(pct=True, ascending=True, na_option='bottom') * 100
        df['percentile'] = df['percentile'].fillna(0)
        
        df = RankingEngine._calculate_category_ranks(df)
        
        logger.info(f"Ranking complete: {len(df)} stocks processed")
        
        return df
    
    @staticmethod
    def _calculate_momentum_quality(df: pd.DataFrame) -> pd.Series:
        """Calculate momentum quality score"""
        quality_score = pd.Series(50, index=df.index, dtype=float)
        
        if all(col in df.columns for col in ['ret_30d', 'ret_7d', 'ret_1d']):
            all_positive = (df['ret_30d'] > 0) & (df['ret_7d'] > 0) & (df['ret_1d'] > 0)
            quality_score[all_positive] = 80
            
            accelerating = (df['ret_1d'] > df['ret_7d'] / 7) & (df['ret_7d'] / 7 > df['ret_30d'] / 30)
            quality_score[accelerating] = 90
            
            mixed = ((df['ret_30d'] > 0) & (df['ret_7d'] < 0)) | ((df['ret_7d'] > 0) & (df['ret_1d'] < 0))
            quality_score[mixed] = 30
        
        return quality_score.clip(0, 100)
    
    @staticmethod
    def _safe_rank(series: pd.Series, pct: bool = True, ascending: bool = True) -> pd.Series:
        """Safely rank a series with proper edge case handling"""
        if series is None or series.empty:
            return pd.Series(dtype=float)
        
        series = series.replace([np.inf, -np.inf], np.nan)
        
        valid_count = series.notna().sum()
        if valid_count == 0:
            return pd.Series(50, index=series.index)
        
        if pct:
            ranks = series.rank(pct=True, ascending=ascending, na_option='bottom') * 100
            ranks = ranks.fillna(0 if ascending else 100)
        else:
            ranks = series.rank(ascending=ascending, method='min', na_option='bottom')
            ranks = ranks.fillna(valid_count + 1)
        
        return ranks
    
    @staticmethod
    def _calculate_position_score(df: pd.DataFrame) -> pd.Series:
        """Calculate position score from 52-week range"""
        position_score = pd.Series(50, index=df.index, dtype=float)
        
        has_from_low = 'from_low_pct' in df.columns and df['from_low_pct'].notna().any()
        has_from_high = 'from_high_pct' in df.columns and df['from_high_pct'].notna().any()
        
        if not has_from_low and not has_from_high:
            logger.warning("No position data available, using neutral position scores")
            return position_score
        
        from_low = df['from_low_pct'].fillna(50) if has_from_low else pd.Series(50, index=df.index)
        from_high = df['from_high_pct'].fillna(-50) if has_from_high else pd.Series(-50, index=df.index)
        
        if has_from_low:
            rank_from_low = RankingEngine._safe_rank(from_low, pct=True, ascending=True)
        else:
            rank_from_low = pd.Series(50, index=df.index)
        
        if has_from_high:
            rank_from_high = RankingEngine._safe_rank(from_high, pct=True, ascending=False)
        else:
            rank_from_high = pd.Series(50, index=df.index)
        
        position_score = (rank_from_low * 0.6 + rank_from_high * 0.4)
        
        return position_score.clip(0, 100)
    
    @staticmethod
    def _calculate_volume_score(df: pd.DataFrame) -> pd.Series:
        """Calculate comprehensive volume score"""
        volume_score = pd.Series(50, index=df.index, dtype=float)
        
        vol_cols = [
            ('vol_ratio_1d_90d', 0.20),
            ('vol_ratio_7d_90d', 0.20),
            ('vol_ratio_30d_90d', 0.20),
            ('vol_ratio_30d_180d', 0.15),
            ('vol_ratio_90d_180d', 0.25)
        ]
        
        total_weight = 0
        weighted_score = pd.Series(0, index=df.index, dtype=float)
        
        for col, weight in vol_cols:
            if col in df.columns and df[col].notna().any():
                col_rank = RankingEngine._safe_rank(df[col], pct=True, ascending=True)
                weighted_score += col_rank * weight
                total_weight += weight
        
        if total_weight > 0:
            volume_score = weighted_score / total_weight
        else:
            logger.warning("No volume ratio data available, using neutral scores")
        
        return volume_score.clip(0, 100)
    
    @staticmethod
    def _calculate_momentum_score(df: pd.DataFrame) -> pd.Series:
        """Calculate momentum score based on returns"""
        momentum_score = pd.Series(50, index=df.index, dtype=float)
        
        if 'ret_30d' not in df.columns or df['ret_30d'].isna().sum() == 0:
            if 'ret_7d' in df.columns and df['ret_7d'].notna().any():
                ret_7d = df['ret_7d'].fillna(0)
                momentum_score = RankingEngine._safe_rank(ret_7d, pct=True, ascending=True)
                logger.info("Using 7-day returns for momentum score")
            else:
                logger.warning("No return data available for momentum calculation")
            
            return momentum_score.clip(0, 100)
        
        ret_30d = df['ret_30d'].fillna(0)
        momentum_score = RankingEngine._safe_rank(ret_30d, pct=True, ascending=True)
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d']):
            consistency_bonus = pd.Series(0, index=df.index, dtype=float)
            
            all_positive = (df['ret_7d'] > 0) & (df['ret_30d'] > 0)
            consistency_bonus[all_positive] = 5
            
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_ret_7d = np.where(df['ret_7d'] != 0, df['ret_7d'] / 7, 0)
                daily_ret_30d = np.where(df['ret_30d'] != 0, df['ret_30d'] / 30, 0)
            
            accelerating = all_positive & (daily_ret_7d > daily_ret_30d)
            consistency_bonus[accelerating] = 10
            
            momentum_score = (momentum_score + consistency_bonus).clip(0, 100)
        
        return momentum_score
    
    @staticmethod
    def _calculate_acceleration_score(df: pd.DataFrame) -> pd.Series:
        """Calculate if momentum is accelerating"""
        acceleration_score = pd.Series(50, index=df.index, dtype=float)
        
        req_cols = ['ret_1d', 'ret_7d', 'ret_30d']
        available_cols = [col for col in req_cols if col in df.columns]
        
        if len(available_cols) < 2:
            logger.warning("Insufficient return data for acceleration calculation")
            return acceleration_score
        
        ret_1d = df['ret_1d'].fillna(0) if 'ret_1d' in df.columns else pd.Series(0, index=df.index)
        ret_7d = df['ret_7d'].fillna(0) if 'ret_7d' in df.columns else pd.Series(0, index=df.index)
        ret_30d = df['ret_30d'].fillna(0) if 'ret_30d' in df.columns else pd.Series(0, index=df.index)
        
        with np.errstate(divide='ignore', invalid='ignore'):
            avg_daily_1d = ret_1d
            avg_daily_7d = np.where(ret_7d != 0, ret_7d / 7, 0)
            avg_daily_30d = np.where(ret_30d != 0, ret_30d / 30, 0)
        
        if all(col in df.columns for col in req_cols):
            perfect = (avg_daily_1d > avg_daily_7d) & (avg_daily_7d > avg_daily_30d) & (ret_1d > 0)
            acceleration_score[perfect] = 100
            
            good = (~perfect) & (avg_daily_1d > avg_daily_7d) & (ret_1d > 0)
            acceleration_score[good] = 80
            
            moderate = (~perfect) & (~good) & (ret_1d > 0)
            acceleration_score[moderate] = 60
            
            slight_decel = (ret_1d <= 0) & (ret_7d > 0)
            acceleration_score[slight_decel] = 40
            
            strong_decel = (ret_1d <= 0) & (ret_7d <= 0)
            acceleration_score[strong_decel] = 20
        
        return acceleration_score
    
    @staticmethod
    def _calculate_breakout_score(df: pd.DataFrame) -> pd.Series:
        """Calculate breakout probability"""
        breakout_score = pd.Series(50, index=df.index, dtype=float)
        
        if 'from_high_pct' in df.columns:
            distance_from_high = -df['from_high_pct'].fillna(-50)
            distance_factor = (100 - distance_from_high).clip(0, 100)
        else:
            distance_factor = pd.Series(50, index=df.index)
        
        volume_factor = pd.Series(50, index=df.index)
        if 'vol_ratio_7d_90d' in df.columns:
            vol_ratio = df['vol_ratio_7d_90d'].fillna(1.0)
            volume_factor = ((vol_ratio - 1) * 100).clip(0, 100)
        
        trend_factor = pd.Series(0, index=df.index, dtype=float)
        
        if 'price' in df.columns:
            current_price = df['price']
            trend_count = 0
            
            for sma_col, points in [('sma_20d', 33.33), ('sma_50d', 33.33), ('sma_200d', 33.34)]:
                if sma_col in df.columns:
                    above_sma = (current_price > df[sma_col]).fillna(False)
                    trend_factor += above_sma.astype(float) * points
                    trend_count += 1
            
            if trend_count > 0 and trend_count < 3:
                trend_factor = trend_factor * (3 / trend_count)
        
        trend_factor = trend_factor.clip(0, 100)
        
        breakout_score = (
            distance_factor * 0.4 +
            volume_factor * 0.4 +
            trend_factor * 0.2
        )
        
        return breakout_score.clip(0, 100)
    
    @staticmethod
    def _calculate_rvol_score(df: pd.DataFrame) -> pd.Series:
        """Calculate RVOL-based score"""
        if 'rvol' not in df.columns:
            return pd.Series(50, index=df.index)
        
        rvol = df['rvol'].fillna(1.0)
        rvol_score = pd.Series(50, index=df.index, dtype=float)
        
        rvol_score[rvol > 10] = 95
        rvol_score[(rvol > 5) & (rvol <= 10)] = 90
        rvol_score[(rvol > 3) & (rvol <= 5)] = 85
        rvol_score[(rvol > 2) & (rvol <= 3)] = 80
        rvol_score[(rvol > 1.5) & (rvol <= 2)] = 70
        rvol_score[(rvol > 1.2) & (rvol <= 1.5)] = 60
        rvol_score[(rvol > 0.8) & (rvol <= 1.2)] = 50
        rvol_score[(rvol > 0.5) & (rvol <= 0.8)] = 40
        rvol_score[(rvol > 0.3) & (rvol <= 0.5)] = 30
        rvol_score[rvol <= 0.3] = 20
        
        return rvol_score
    
    @staticmethod
    def _calculate_trend_quality(df: pd.DataFrame) -> pd.Series:
        """Calculate trend quality score based on SMA alignment"""
        trend_score = pd.Series(50, index=df.index, dtype=float)
        
        if 'price' not in df.columns:
            return trend_score
        
        current_price = df['price']
        sma_cols = ['sma_20d', 'sma_50d', 'sma_200d']
        available_smas = [col for col in sma_cols if col in df.columns and df[col].notna().any()]
        
        if len(available_smas) == 0:
            return trend_score
        
        if len(available_smas) >= 3:
            perfect_trend = (
                (current_price > df['sma_20d']) & 
                (df['sma_20d'] > df['sma_50d']) & 
                (df['sma_50d'] > df['sma_200d'])
            )
            trend_score[perfect_trend] = 100
            
            strong_trend = (
                (~perfect_trend) &
                (current_price > df['sma_20d']) & 
                (current_price > df['sma_50d']) & 
                (current_price > df['sma_200d'])
            )
            trend_score[strong_trend] = 85
            
            above_count = sum([(current_price > df[sma]).astype(int) for sma in available_smas])
            
            good_trend = (above_count == 2) & (~perfect_trend) & (~strong_trend)
            trend_score[good_trend] = 70
            
            weak_trend = (above_count == 1)
            trend_score[weak_trend] = 40
            
            poor_trend = (above_count == 0)
            trend_score[poor_trend] = 20
        
        return trend_score
    
    @staticmethod
    def _calculate_long_term_strength(df: pd.DataFrame) -> pd.Series:
        """Calculate long-term strength score"""
        strength_score = pd.Series(50, index=df.index, dtype=float)
        
        lt_cols = ['ret_3m', 'ret_6m', 'ret_1y']
        available_cols = [col for col in lt_cols if col in df.columns and df[col].notna().any()]
        
        if not available_cols:
            return strength_score
        
        lt_returns = df[available_cols].fillna(0)
        avg_return = lt_returns.mean(axis=1)
        
        strength_score[avg_return > 100] = 100
        strength_score[(avg_return > 50) & (avg_return <= 100)] = 90
        strength_score[(avg_return > 30) & (avg_return <= 50)] = 80
        strength_score[(avg_return > 15) & (avg_return <= 30)] = 70
        strength_score[(avg_return > 5) & (avg_return <= 15)] = 60
        strength_score[(avg_return > 0) & (avg_return <= 5)] = 50
        strength_score[(avg_return > -10) & (avg_return <= 0)] = 40
        strength_score[(avg_return > -25) & (avg_return <= -10)] = 30
        strength_score[avg_return <= -25] = 20
        
        return strength_score.clip(0, 100)
    
    @staticmethod
    def _calculate_liquidity_score(df: pd.DataFrame) -> pd.Series:
        """Calculate liquidity score based on trading volume"""
        liquidity_score = pd.Series(50, index=df.index, dtype=float)
        
        if 'volume_30d' in df.columns and 'price' in df.columns:
            dollar_volume = df['volume_30d'].fillna(0) * df['price'].fillna(0)
            liquidity_score = RankingEngine._safe_rank(dollar_volume, pct=True, ascending=True)
        
        return liquidity_score.clip(0, 100)
    
    @staticmethod
    def _calculate_category_ranks(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate percentile ranks within each category"""
        df['category_rank'] = 9999
        df['category_percentile'] = 0.0
        
        categories = df['category'].unique()
        
        for category in categories:
            if category != 'Unknown':
                mask = df['category'] == category
                cat_df = df[mask]
                
                if len(cat_df) > 0:
                    cat_ranks = cat_df['master_score'].rank(method='first', ascending=False, na_option='bottom')
                    df.loc[mask, 'category_rank'] = cat_ranks.astype(int)
                    
                    cat_percentiles = cat_df['master_score'].rank(pct=True, ascending=True, na_option='bottom') * 100
                    df.loc[mask, 'category_percentile'] = cat_percentiles
        
        return df

# ============================================
# PATTERN DETECTION ENGINE - FULLY OPTIMIZED
# ============================================

class PatternDetector:
    """Detect all patterns using fully vectorized operations with confidence scoring"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.3)
    def detect_all_patterns_optimized(df: pd.DataFrame) -> pd.DataFrame:
        """Detect all 25 patterns with confidence scoring - O(n) complexity"""
        
        if df.empty:
            df['patterns'] = ''
            df['pattern_confidence'] = 0
            return df
        
        n_stocks = len(df)
        pattern_results = {}
        
        if 'category_percentile' in df.columns:
            pattern_results['ðŸ”¥ CAT LEADER'] = df['category_percentile'] >= CONFIG.PATTERN_THRESHOLDS['category_leader']
        
        if 'category_percentile' in df.columns and 'percentile' in df.columns:
            pattern_results['ðŸ’Ž HIDDEN GEM'] = (
                (df['category_percentile'] >= CONFIG.PATTERN_THRESHOLDS['hidden_gem']) & 
                (df['percentile'] < 70)
            )
        
        if 'acceleration_score' in df.columns:
            pattern_results['ðŸš€ ACCELERATING'] = df['acceleration_score'] >= CONFIG.PATTERN_THRESHOLDS['acceleration']
        
        if 'volume_score' in df.columns and 'vol_ratio_90d_180d' in df.columns:
            pattern_results['ðŸ¦ INSTITUTIONAL'] = (
                (df['volume_score'] >= CONFIG.PATTERN_THRESHOLDS['institutional']) &
                (df['vol_ratio_90d_180d'] > 1.1)
            )
        
        if 'rvol' in df.columns:
            pattern_results['âš¡ VOL EXPLOSION'] = df['rvol'] > 3
        
        if 'breakout_score' in df.columns:
            pattern_results['ðŸŽ¯ BREAKOUT'] = df['breakout_score'] >= CONFIG.PATTERN_THRESHOLDS['breakout_ready']
        
        if 'percentile' in df.columns:
            pattern_results['ðŸ‘‘ MARKET LEADER'] = df['percentile'] >= CONFIG.PATTERN_THRESHOLDS['market_leader']
        
        if 'momentum_score' in df.columns and 'acceleration_score' in df.columns:
            pattern_results['ðŸŒŠ MOMENTUM WAVE'] = (
                (df['momentum_score'] >= CONFIG.PATTERN_THRESHOLDS['momentum_wave']) &
                (df['acceleration_score'] >= 70)
            )
        
        if 'liquidity_score' in df.columns and 'percentile' in df.columns:
            pattern_results['ðŸ’° LIQUID LEADER'] = (
                (df['liquidity_score'] >= CONFIG.PATTERN_THRESHOLDS['liquid_leader']) &
                (df['percentile'] >= CONFIG.PATTERN_THRESHOLDS['liquid_leader'])
            )
        
        if 'long_term_strength' in df.columns:
            pattern_results['ðŸ’ª LONG STRENGTH'] = df['long_term_strength'] >= CONFIG.PATTERN_THRESHOLDS['long_strength']
        
        if 'trend_quality' in df.columns:
            pattern_results['ðŸ“ˆ QUALITY TREND'] = df['trend_quality'] >= 80
        
        if 'pe' in df.columns and 'master_score' in df.columns:
            has_valid_pe = (df['pe'].notna() & (df['pe'] > 0) & (df['pe'] < 10000))
            pattern_results['ðŸ’Ž VALUE MOMENTUM'] = has_valid_pe & (df['pe'] < 15) & (df['master_score'] >= 70)
        
        if 'eps_change_pct' in df.columns and 'acceleration_score' in df.columns:
            has_eps_growth = df['eps_change_pct'].notna()
            extreme_growth = has_eps_growth & (df['eps_change_pct'] > 1000)
            normal_growth = has_eps_growth & (df['eps_change_pct'] > 50) & (df['eps_change_pct'] <= 1000)
            
            pattern_results['ðŸ“Š EARNINGS ROCKET'] = (
                (extreme_growth & (df['acceleration_score'] >= 80)) |
                (normal_growth & (df['acceleration_score'] >= 70))
            )
        
        if all(col in df.columns for col in ['pe', 'eps_change_pct', 'percentile']):
            has_complete_data = (
                df['pe'].notna() & 
                df['eps_change_pct'].notna() & 
                (df['pe'] > 0) &
                (df['pe'] < 10000)
            )
            pattern_results['ðŸ† QUALITY LEADER'] = (
                has_complete_data &
                (df['pe'].between(10, 25)) &
                (df['eps_change_pct'] > 20) &
                (df['percentile'] >= 80)
            )
        
        if 'eps_change_pct' in df.columns and 'volume_score' in df.columns:
            has_eps = df['eps_change_pct'].notna()
            mega_turnaround = has_eps & (df['eps_change_pct'] > 500) & (df['volume_score'] >= 60)
            strong_turnaround = has_eps & (df['eps_change_pct'] > 100) & (df['eps_change_pct'] <= 500) & (df['volume_score'] >= 70)
            
            pattern_results['âš¡ TURNAROUND'] = mega_turnaround | strong_turnaround
        
        if 'pe' in df.columns:
            has_valid_pe = df['pe'].notna() & (df['pe'] > 0)
            pattern_results['âš ï¸ HIGH PE'] = has_valid_pe & (df['pe'] > 100)
        
        if all(col in df.columns for col in ['from_high_pct', 'volume_score', 'momentum_score']):
            pattern_results['ðŸŽ¯ 52W HIGH APPROACH'] = (
                (df['from_high_pct'] > -5) & 
                (df['volume_score'] >= 70) & 
                (df['momentum_score'] >= 60)
            )
        
        if all(col in df.columns for col in ['from_low_pct', 'acceleration_score', 'ret_30d']):
            pattern_results['ðŸ”„ 52W LOW BOUNCE'] = (
                (df['from_low_pct'] < 20) & 
                (df['acceleration_score'] >= 80) & 
                (df['ret_30d'] > 10)
            )
        
        if all(col in df.columns for col in ['from_low_pct', 'from_high_pct', 'trend_quality']):
            pattern_results['ðŸ‘‘ GOLDEN ZONE'] = (
                (df['from_low_pct'] > 60) & 
                (df['from_high_pct'] > -40) & 
                (df['trend_quality'] >= 70)
            )
        
        if all(col in df.columns for col in ['vol_ratio_30d_90d', 'vol_ratio_90d_180d', 'ret_30d']):
            pattern_results['ðŸ“Š VOL ACCUMULATION'] = (
                (df['vol_ratio_30d_90d'] > 1.2) & 
                (df['vol_ratio_90d_180d'] > 1.1) & 
                (df['ret_30d'] > 5)
            )
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'acceleration_score', 'rvol']):
            ret_7d_arr = df['ret_7d'].fillna(0).values
            ret_30d_arr = df['ret_30d'].fillna(0).values
            
            daily_7d_pace = np.where(ret_7d_arr != 0, ret_7d_arr / 7, 0)
            daily_30d_pace = np.where(ret_30d_arr != 0, ret_30d_arr / 30, 0)
            
            pattern_results['ðŸ”€ MOMENTUM DIVERGE'] = (
                (daily_7d_pace > daily_30d_pace * 1.5) & 
                (df['acceleration_score'] >= 85) & 
                (df['rvol'] > 2)
            )
        
        if all(col in df.columns for col in ['high_52w', 'low_52w', 'from_low_pct']):
            high_arr = df['high_52w'].fillna(0).values
            low_arr = df['low_52w'].fillna(0).values
            
            range_pct = np.where(
                low_arr > 0,
                ((high_arr - low_arr) / low_arr) * 100,
                100
            )
            
            pattern_results['ðŸŽ¯ RANGE COMPRESS'] = (range_pct < 50) & (df['from_low_pct'] > 30)
        
        if all(col in df.columns for col in ['vol_ratio_90d_180d', 'vol_ratio_30d_90d', 'from_low_pct', 'ret_7d', 'ret_30d']):
            ret_7d_arr = df['ret_7d'].fillna(0).values
            ret_30d_arr = df['ret_30d'].fillna(0).values
            
            ret_ratio = np.where(ret_30d_arr != 0, ret_7d_arr / (ret_30d_arr / 4), 0)
            
            pattern_results['ðŸ¤« STEALTH'] = (
                (df['vol_ratio_90d_180d'] > 1.1) &
                (df['vol_ratio_30d_90d'].between(0.9, 1.1)) &
                (df['from_low_pct'] > 40) &
                (ret_ratio > 1)
            )
        
        if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'rvol', 'from_high_pct', 'category']):
            ret_1d_arr = df['ret_1d'].fillna(0).values
            ret_7d_arr = df['ret_7d'].fillna(0).values
            
            daily_pace_ratio = np.where(ret_7d_arr != 0, ret_1d_arr / (ret_7d_arr / 7), 0)
            
            pattern_results['ðŸ§› VAMPIRE'] = (
                (daily_pace_ratio > 2) &
                (df['rvol'] > 3) &
                (df['from_high_pct'] > -15) &
                (df['category'].isin(['Small Cap', 'Micro Cap']))
            )
        
        if 'momentum_harmony' in df.columns and 'master_score' in df.columns:
            pattern_results['â›ˆï¸ PERFECT STORM'] = (
                (df['momentum_harmony'] == 4) &
                (df['master_score'] > 80)
            )
        
        pattern_names = list(pattern_results.keys())
        pattern_matrix = np.column_stack([pattern_results[name].values for name in pattern_names])
        
        df['patterns'] = [
            ' | '.join([pattern_names[i] for i, val in enumerate(row) if val])
            for row in pattern_matrix
        ]
        
        df['patterns'] = df['patterns'].fillna('')
        
        df = PatternDetector._calculate_pattern_confidence(df)
        
        return df
    
    @staticmethod
    def _calculate_pattern_confidence(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate confidence scores for detected patterns"""
        
        confidence_scores = []
        
        for idx, row in df.iterrows():
            patterns = row['patterns'].split(' | ') if row['patterns'] else []
            total_confidence = 0
            
            for pattern in patterns:
                if pattern and pattern in CONFIG.PATTERN_METADATA:
                    metadata = CONFIG.PATTERN_METADATA[pattern]
                    
                    importance_weights = {
                        'very_high': 40,
                        'high': 30,
                        'medium': 20,
                        'low': 10
                    }
                    confidence = importance_weights.get(metadata['importance'], 20)
                    
                    risk_multipliers = {
                        'low': 1.2,
                        'medium': 1.0,
                        'high': 0.8,
                        'very_high': 0.6
                    }
                    confidence *= risk_multipliers.get(metadata['risk'], 1.0)
                    
                    total_confidence += confidence
            
            if len(patterns) > 1:
                total_confidence *= (1 + np.log(len(patterns))) / len(patterns)
            
            confidence_scores.append(min(100, total_confidence))
        
        df['pattern_confidence'] = confidence_scores
        
        return df

# ============================================
# MARKET INTELLIGENCE
# ============================================

class MarketIntelligence:
    """Advanced market analysis and regime detection"""
    
    @staticmethod
    def detect_market_regime(df: pd.DataFrame) -> Tuple[str, Dict[str, Any]]:
        """Detect current market regime with supporting data"""
        
        if df.empty:
            return "ðŸ˜´ NO DATA", {}
        
        metrics = {}
        
        if 'category' in df.columns and 'master_score' in df.columns:
            category_scores = df.groupby('category')['master_score'].mean()
            
            micro_small_avg = category_scores[category_scores.index.isin(['Micro Cap', 'Small Cap'])].mean()
            large_mega_avg = category_scores[category_scores.index.isin(['Large Cap', 'Mega Cap'])].mean()
            
            metrics['micro_small_avg'] = micro_small_avg
            metrics['large_mega_avg'] = large_mega_avg
            metrics['category_spread'] = micro_small_avg - large_mega_avg
        else:
            micro_small_avg = 50
            large_mega_avg = 50
        
        if 'ret_30d' in df.columns:
            breadth = len(df[df['ret_30d'] > 0]) / len(df)
            metrics['breadth'] = breadth
        else:
            breadth = 0.5
        
        if 'rvol' in df.columns:
            avg_rvol = df['rvol'].median()
            metrics['avg_rvol'] = avg_rvol
        else:
            avg_rvol = 1.0
        
        if micro_small_avg > large_mega_avg + 10 and breadth > 0.6:
            regime = "ðŸ”¥ RISK-ON BULL"
        elif large_mega_avg > micro_small_avg + 10 and breadth < 0.4:
            regime = "ðŸ›¡ï¸ RISK-OFF DEFENSIVE"
        elif avg_rvol > 1.5 and breadth > 0.5:
            regime = "âš¡ VOLATILE OPPORTUNITY"
        else:
            regime = "ðŸ˜´ RANGE-BOUND"
        
        metrics['regime'] = regime
        
        return regime, metrics
    
    @staticmethod
    def calculate_advance_decline_ratio(df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate advance/decline ratio and related metrics"""
        
        ad_metrics = {}
        
        if 'ret_1d' in df.columns:
            advancing = len(df[df['ret_1d'] > 0])
            declining = len(df[df['ret_1d'] < 0])
            unchanged = len(df[df['ret_1d'] == 0])
            
            ad_metrics['advancing'] = advancing
            ad_metrics['declining'] = declining
            ad_metrics['unchanged'] = unchanged
            
            if declining > 0:
                ad_metrics['ad_ratio'] = advancing / declining
            else:
                ad_metrics['ad_ratio'] = float('inf') if advancing > 0 else 1.0
            
            ad_metrics['ad_line'] = advancing - declining
            ad_metrics['breadth_pct'] = (advancing / len(df)) * 100 if len(df) > 0 else 0
        
        return ad_metrics

    @staticmethod
    def _apply_dynamic_sampling(df_group: pd.DataFrame) -> pd.DataFrame:
        """Helper to apply dynamic sampling based on group size."""
        group_size = len(df_group)
        
        if 1 <= group_size <= 5:
            sample_count = group_size
        elif 6 <= group_size <= 20:
            sample_count = max(1, int(group_size * 0.80))
        elif 21 <= group_size <= 50:
            sample_count = max(1, int(group_size * 0.60))
        elif 51 <= group_size <= 100:
            sample_count = max(1, int(group_size * 0.40))
        else:
            sample_count = min(50, int(group_size * 0.25))
        
        if sample_count > 0:
            return df_group.nlargest(sample_count, 'master_score', keep='first')
        else:
            return pd.DataFrame()

    @staticmethod
    def _calculate_flow_metrics(normalized_df: pd.DataFrame, group_col: str) -> pd.DataFrame:
        """Helper to calculate common flow metrics for sector/industry rotation."""
        agg_dict = {
            'master_score': ['mean', 'median', 'std', 'count'],
            'momentum_score': 'mean',
            'volume_score': 'mean',
            'rvol': 'mean',
            'ret_30d': 'mean',
            'money_flow_mm': 'sum'
        }
        
        available_agg_dict = {
            k: v for k, v in agg_dict.items() if k in normalized_df.columns
        }

        group_metrics = normalized_df.groupby(group_col).agg(available_agg_dict).round(2)
        
        new_columns = []
        for col, funcs in available_agg_dict.items():
            if isinstance(funcs, list):
                for f in funcs:
                    new_columns.append(f"{col}_{f}")
            else:
                new_columns.append(f"{col}_{funcs}")
        
        group_metrics.columns = new_columns
        
        rename_map = {
            'master_score_mean': 'avg_score',
            'master_score_median': 'median_score',
            'master_score_std': 'std_score',
            'master_score_count': 'count',
            'momentum_score_mean': 'avg_momentum',
            'volume_score_mean': 'avg_volume',
            'rvol_mean': 'avg_rvol',
            'ret_30d_mean': 'avg_ret_30d',
            'money_flow_mm_sum': 'total_money_flow'
        }
        group_metrics = group_metrics.rename(columns=rename_map)

        group_metrics['flow_score'] = (
            group_metrics['avg_score'].fillna(0) * 0.3 +
            group_metrics['median_score'].fillna(0) * 0.2 +
            group_metrics['avg_momentum'].fillna(0) * 0.25 +
            group_metrics['avg_volume'].fillna(0) * 0.25
        )
        
        group_metrics['rank'] = group_metrics['flow_score'].rank(ascending=False, method='min')
        
        return group_metrics.sort_values('flow_score', ascending=False)
    
    @staticmethod
    def detect_sector_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect sector rotation patterns with normalized analysis and dynamic sampling.
        """
        if 'sector' not in df.columns or df.empty:
            return pd.DataFrame()
        
        sector_dfs = []
        
        grouped_sectors = df.groupby('sector')
        for sector_name, sector_group_df in grouped_sectors:
            if sector_name != 'Unknown':
                sampled_sector_df = MarketIntelligence._apply_dynamic_sampling(sector_group_df.copy())
                if not sampled_sector_df.empty:
                    sector_dfs.append(sampled_sector_df)
        
        if not sector_dfs:
            return pd.DataFrame()
        
        normalized_df = pd.concat(sector_dfs, ignore_index=True)
        
        sector_metrics = MarketIntelligence._calculate_flow_metrics(normalized_df, 'sector')
        
        original_counts = df.groupby('sector').size().rename('total_stocks')
        sector_metrics = sector_metrics.join(original_counts, how='left')
        sector_metrics['analyzed_stocks'] = sector_metrics['count']
        sector_metrics['sampling_pct'] = (sector_metrics['analyzed_stocks'] / sector_metrics['total_stocks'] * 100).round(1)
        
        return sector_metrics

    @staticmethod
    def detect_industry_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect industry rotation patterns with normalized analysis and dynamic sampling.
        """
        if 'industry' not in df.columns or df.empty:
            return pd.DataFrame()
        
        industry_dfs = []
        
        grouped_industries = df.groupby('industry')
        for industry_name, industry_group_df in grouped_industries:
            if industry_name != 'Unknown':
                sampled_industry_df = MarketIntelligence._apply_dynamic_sampling(industry_group_df.copy())
                if not sampled_industry_df.empty:
                    industry_dfs.append(sampled_industry_df)
        
        if not industry_dfs:
            return pd.DataFrame()
        
        normalized_df = pd.concat(industry_dfs, ignore_index=True)
        
        industry_metrics = MarketIntelligence._calculate_flow_metrics(normalized_df, 'industry')
        
        original_counts = df.groupby('industry').size().rename('total_stocks')
        industry_metrics = industry_metrics.join(original_counts, how='left')
        industry_metrics['analyzed_stocks'] = industry_metrics['count']
        industry_metrics['sampling_pct'] = (industry_metrics['analyzed_stocks'] / industry_metrics['total_stocks'] * 100).round(1)
        
        return industry_metrics

# ============================================
# VISUALIZATION ENGINE
# ============================================

class Visualizer:
    """Create all visualizations with proper error handling"""
    
    @staticmethod
    def create_score_distribution(df: pd.DataFrame) -> go.Figure:
        """Create score distribution chart"""
        fig = go.Figure()
        
        if df.empty:
            fig.add_annotation(
                text="No data available for visualization",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig
        
        scores = [
            ('position_score', 'Position', '#3498db'),
            ('volume_score', 'Volume', '#e74c3c'),
            ('momentum_score', 'Momentum', '#2ecc71'),
            ('acceleration_score', 'Acceleration', '#f39c12'),
            ('breakout_score', 'Breakout', '#9b59b6'),
            ('rvol_score', 'RVOL', '#e67e22')
        ]
        
        for score_col, label, color in scores:
            if score_col in df.columns:
                score_data = df[score_col].dropna()
                if len(score_data) > 0:
                    fig.add_trace(go.Box(
                        y=score_data,
                        name=label,
                        marker_color=color,
                        boxpoints='outliers',
                        hovertemplate=f'{label}<br>Score: %{{y:.1f}}<extra></extra>'
                    ))
        
        fig.update_layout(
            title="Score Component Distribution",
            yaxis_title="Score (0-100)",
            template='plotly_white',
            height=400,
            showlegend=False
        )
        
        return fig

    @staticmethod
    def create_acceleration_profiles(df: pd.DataFrame, n: int = 10) -> go.Figure:
        """Create acceleration profiles showing momentum over time"""
        try:
            accel_df = df.nlargest(min(n, len(df)), 'acceleration_score')
            
            if len(accel_df) == 0:
                fig = go.Figure()
                fig.add_annotation(text="No stocks meet criteria for acceleration profiles.", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)
                return fig
            
            fig = go.Figure()
            
            for _, stock in accel_df.iterrows():
                x_points = ['Start']
                y_points = [0]
                
                if 'ret_30d' in stock.index and pd.notna(stock['ret_30d']):
                    x_points.append('30D')
                    y_points.append(stock['ret_30d'])
                
                if 'ret_7d' in stock.index and pd.notna(stock['ret_7d']):
                    x_points.append('7D')
                    y_points.append(stock['ret_7d'])
                
                if 'ret_1d' in stock.index and pd.notna(stock['ret_1d']):
                    x_points.append('Today')
                    y_points.append(stock['ret_1d'])
                
                if len(x_points) > 1:
                    accel_score = stock.get('acceleration_score', 0)
                    if accel_score >= 85:
                        line_style = dict(width=3, dash='solid')
                        marker_style = dict(size=10, symbol='star', line=dict(color='DarkSlateGrey', width=1))
                    elif accel_score >= 70:
                        line_style = dict(width=2, dash='solid')
                        marker_style = dict(size=8)
                    else:
                        line_style = dict(width=2, dash='dot')
                        marker_style = dict(size=6)
                    
                    fig.add_trace(go.Scatter(
                        x=x_points,
                        y=y_points,
                        mode='lines+markers',
                        name=f"{stock['ticker']} ({accel_score:.0f})",
                        line=line_style,
                        marker=marker_style,
                        hovertemplate=(
                            f"<b>{stock['ticker']}</b><br>" +
                            "%{{x}}: %{{y:.1f}}%<br>" +
                            f"Accel Score: {accel_score:.0f}<extra></extra>"
                        )
                    ))
            
            fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)
            
            fig.update_layout(
                title=f"Acceleration Profiles - Top {len(accel_df)} Momentum Builders",
                xaxis_title="Time Frame",
                yaxis_title="Return %",
                height=400,
                template='plotly_white',
                showlegend=True,
                legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02),
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            logger.error(f"Error creating acceleration profiles: {str(e)}")
            fig = go.Figure()
            fig.add_annotation(text=f"Error generating chart: {e}", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)
            return fig

# ============================================
# FILTER ENGINE - ENHANCED WITH INTERCONNECTION
# ============================================

class FilterEngine:
    """Apply all filters with performance optimization and interconnection"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.2)
    def apply_all_filters(df: pd.DataFrame, filters: Dict[str, Any]) -> pd.DataFrame:
        """Apply all active filters efficiently"""
        
        if df.empty or not filters:
            return df
        
        mask = pd.Series(True, index=df.index)
        
        quick_filter = RobustSessionState.safe_get('quick_filter')
        if quick_filter and RobustSessionState.safe_get('quick_filter_applied', False):
            mask &= FilterEngine._apply_quick_filter_mask(df, quick_filter)
        
        if filters.get('min_score', 0) > 0:
            mask &= df['master_score'] >= filters['min_score']
        
        if filters.get('categories'):
            mask &= df['category'].isin(filters['categories'])
        
        if filters.get('sectors'):
            mask &= df['sector'].isin(filters['sectors'])
        
        if filters.get('industries'):
            mask &= df['industry'].isin(filters['industries'])
        
        if filters.get('patterns'):
            pattern_regex = '|'.join([re.escape(p) for p in filters['patterns']])
            mask &= df['patterns'].str.contains(pattern_regex, case=False, na=False, regex=True)
        
        if filters.get('trend_filter') and filters['trend_filter'] != 'All Trends':
            if 'trend_quality' in df.columns:
                min_trend, max_trend = filters['trend_range']
                mask &= (df['trend_quality'] >= min_trend) & (df['trend_quality'] <= max_trend)
        
        for tier_type in ['eps_tiers', 'pe_tiers', 'price_tiers']:
            if filters.get(tier_type):
                tier_col = tier_type.replace('_tiers', '_tier')
                if tier_col in df.columns:
                    mask &= df[tier_col].isin(filters[tier_type])
        
        if filters.get('min_eps_change') is not None and 'eps_change_pct' in df.columns:
            mask &= df['eps_change_pct'] >= filters['min_eps_change']
        
        if filters.get('min_pe') is not None and 'pe' in df.columns:
            mask &= df['pe'] >= filters['min_pe']
        
        if filters.get('max_pe') is not None and 'pe' in df.columns:
            mask &= df['pe'] <= filters['max_pe']
        
        if filters.get('wave_states') and 'wave_state' in df.columns:
            mask &= df['wave_state'].isin(filters['wave_states'])
        
        if filters.get('wave_strength_range') and 'overall_wave_strength' in df.columns:
            min_val, max_val = filters['wave_strength_range']
            mask &= (df['overall_wave_strength'] >= min_val) & (df['overall_wave_strength'] <= max_val)
        
        if filters.get('require_fundamental_data', False):
            if all(col in df.columns for col in ['pe', 'eps_change_pct']):
                mask &= df['pe'].notna() & df['eps_change_pct'].notna()
        
        filtered_df = df[mask].copy()
        
        logger.info(f"Filters reduced {len(df)} to {len(filtered_df)} stocks")
        
        return filtered_df
    
    @staticmethod
    def _apply_quick_filter_mask(df: pd.DataFrame, filter_type: str) -> pd.Series:
        """Create and return a boolean mask for a quick filter"""
        
        if df.empty:
            return pd.Series(False, index=df.index)
        
        if filter_type == 'top_gainers' and 'momentum_score' in df.columns:
            return df['momentum_score'] >= 80
        
        if filter_type == 'volume_surges' and 'rvol' in df.columns:
            return df['rvol'] >= 3
        
        if filter_type == 'breakout_ready' and 'breakout_score' in df.columns:
            return df['breakout_score'] >= 80
        
        if filter_type == 'hidden_gems' and 'patterns' in df.columns:
            return df['patterns'].str.contains('HIDDEN GEM', na=False)
        
        return pd.Series(True, index=df.index)

    @staticmethod
    def get_filter_options(df: pd.DataFrame, column: str, current_filters: Dict[str, Any]) -> List[str]:
        """Get available filter options with perfect smart interconnection"""
        
        if df.empty or column not in df.columns:
            return []
        
        temp_filters = current_filters.copy()
        
        filter_key_map = {
            'category': 'categories',
            'sector': 'sectors',
            'industry': 'industries',
            'eps_tier': 'eps_tiers',
            'pe_tier': 'pe_tiers',
            'price_tier': 'price_tiers',
            'wave_state': 'wave_states'
        }
        
        if column in filter_key_map:
            temp_filters.pop(filter_key_map[column], None)
        
        if column == 'industry' and 'sectors' in current_filters:
            temp_filters['sectors'] = current_filters['sectors']
        
        filtered_df = FilterEngine.apply_all_filters(df, temp_filters)
        
        values = filtered_df[column].dropna().unique()
        
        values = [v for v in values if str(v).strip() not in ['Unknown', 'unknown', '', 'nan', 'NaN', 'None']]
        
        try:
            values = sorted(values, key=lambda x: float(str(x).replace(',', '')) if str(x).replace(',', '').replace('.', '').isdigit() else x)
        except:
            values = sorted(values, key=str)
        
        return values

# ============================================
# SEARCH ENGINE - ENHANCED
# ============================================

class SearchEngine:
    """Optimized search functionality with exact match priority"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.05)
    def search_stocks(df: pd.DataFrame, query: str) -> pd.DataFrame:
        """Search stocks with exact match prioritization"""
        
        if not query or df.empty:
            return pd.DataFrame()
        
        try:
            query = query.upper().strip()
            
            results = df.copy()
            results['relevance'] = 0
            
            exact_ticker_mask = results['ticker'].str.upper() == query
            results.loc[exact_ticker_mask, 'relevance'] += 1000
            
            ticker_starts_mask = results['ticker'].str.upper().str.startswith(query)
            results.loc[ticker_starts_mask & ~exact_ticker_mask, 'relevance'] += 500
            
            ticker_contains_mask = results['ticker'].str.upper().str.contains(query, na=False, regex=False)
            results.loc[ticker_contains_mask & ~ticker_starts_mask, 'relevance'] += 200
            
            if 'company_name' in results.columns:
                company_upper = results['company_name'].str.upper().fillna('')
                
                company_exact_mask = company_upper == query
                results.loc[company_exact_mask, 'relevance'] += 800
                
                company_starts_mask = company_upper.str.startswith(query)
                results.loc[company_starts_mask & ~company_exact_mask, 'relevance'] += 300
                
                company_contains_mask = company_upper.str.contains(query, na=False, regex=False)
                results.loc[company_contains_mask & ~company_starts_mask, 'relevance'] += 100
                
                def word_match_score(company_name):
                    if pd.isna(company_name):
                        return 0
                    words = str(company_name).upper().split()
                    for word in words:
                        if word.startswith(query):
                            return 50
                    return 0
                
                word_scores = results['company_name'].apply(word_match_score)
                results['relevance'] += word_scores
            
            matches = results[results['relevance'] > 0].copy()
            
            if matches.empty:
                return pd.DataFrame()
            
            matches = matches.sort_values(['relevance', 'master_score'], ascending=[False, False])
            
            return matches.drop('relevance', axis=1)
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            return pd.DataFrame()

# ============================================
# EXPORT ENGINE - ENHANCED
# ============================================

class ExportEngine:
    """Handle all export operations with streaming for large datasets"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def create_excel_report(df: pd.DataFrame, template: str = 'full') -> BytesIO:
        """Create comprehensive Excel report with smart templates"""
        
        output = BytesIO()
        
        templates = {
            'day_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'rvol', 
                           'momentum_score', 'acceleration_score', 'ret_1d', 'ret_7d', 
                           'volume_score', 'vmi', 'wave_state', 'patterns', 'category'],
                'focus': 'Intraday momentum and volume'
            },
            'swing_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 
                           'breakout_score', 'position_score', 'position_tension',
                           'from_high_pct', 'from_low_pct', 'trend_quality', 
                           'momentum_harmony', 'patterns', 'industry'],
                'focus': 'Position and breakout setups'
            },
            'investor': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'pe', 
                           'eps_current', 'eps_change_pct', 'ret_1y', 'ret_3y', 
                           'long_term_strength', 'money_flow_mm', 'category', 'sector', 'industry'],
                'focus': 'Fundamentals and long-term performance'
            },
            'full': {
                'columns': None,
                'focus': 'Complete analysis'
            }
        }
        
        try:
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                workbook = writer.book
                
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#3498db',
                    'font_color': 'white',
                    'border': 1
                })
                
                number_format = workbook.add_format({'num_format': '#,##0.00'})
                percent_format = workbook.add_format({'num_format': '0.0%'})
                currency_format = workbook.add_format({'num_format': 'â‚¹#,##0'})
                
                top_100 = df.nlargest(min(100, len(df)), 'master_score')
                
                if template in templates and templates[template]['columns']:
                    export_cols = [col for col in templates[template]['columns'] if col in top_100.columns]
                else:
                    export_cols = None
                
                if export_cols:
                    top_100_export = top_100[export_cols]
                else:
                    top_100_export = top_100
                
                top_100_export.to_excel(writer, sheet_name='Top 100', index=False)
                
                worksheet = writer.sheets['Top 100']
                for i, col in enumerate(top_100_export.columns):
                    worksheet.write(0, i, col, header_format)
                
                intel_data = []
                
                regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
                intel_data.append({
                    'Metric': 'Market Regime',
                    'Value': regime,
                    'Details': f"Breadth: {regime_metrics.get('breadth', 0):.1%}"
                })
                
                ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
                intel_data.append({
                    'Metric': 'Advance/Decline',
                    'Value': f"{ad_metrics.get('advancing', 0)}/{ad_metrics.get('declining', 0)}",
                    'Details': f"Ratio: {ad_metrics.get('ad_ratio', 1):.2f}"
                })
                
                intel_df = pd.DataFrame(intel_data)
                intel_df.to_excel(writer, sheet_name='Market Intelligence', index=False)
                
                sector_rotation = MarketIntelligence.detect_sector_rotation(df)
                if not sector_rotation.empty:
                    sector_rotation.to_excel(writer, sheet_name='Sector Rotation')
                
                pattern_counts = {}
                for patterns in df['patterns'].dropna():
                    if patterns:
                        for p in patterns.split(' | '):
                            pattern_counts[p] = pattern_counts.get(p, 0) + 1
                
                if pattern_counts:
                    pattern_df = pd.DataFrame(
                        list(pattern_counts.items()),
                        columns=['Pattern', 'Count']
                    ).sort_values('Count', ascending=False)
                    pattern_df.to_excel(writer, sheet_name='Pattern Analysis', index=False)
                
                wave_signals = df[
                    (df['momentum_score'] >= 60) & 
                    (df['acceleration_score'] >= 70) &
                    (df['rvol'] >= 2)
                ].head(50)
                
                if len(wave_signals) > 0:
                    wave_cols = ['ticker', 'company_name', 'master_score', 
                                'momentum_score', 'acceleration_score', 'rvol',
                                'wave_state', 'patterns', 'category', 'industry']
                    available_wave_cols = [col for col in wave_cols if col in wave_signals.columns]
                    
                    wave_signals[available_wave_cols].to_excel(
                        writer, sheet_name='Wave Radar', index=False
                    )
                
                summary_stats = {
                    'Total Stocks': len(df),
                    'Average Master Score': df['master_score'].mean(),
                    'Stocks with Patterns': (df['patterns'] != '').sum(),
                    'High RVOL (>2x)': (df['rvol'] > 2).sum() if 'rvol' in df.columns else 0,
                    'Positive 30D Returns': (df['ret_30d'] > 0).sum() if 'ret_30d' in df.columns else 0,
                    'Template Used': template,
                    'Export Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                summary_df = pd.DataFrame(list(summary_stats.items()), columns=['Metric', 'Value'])
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                
                logger.info(f"Excel report created successfully with {len(writer.sheets)} sheets")
                
        except Exception as e:
            logger.error(f"Error creating Excel report: {str(e)}")
            raise
        
        output.seek(0)
        return output
    
    @staticmethod
    def create_csv_export(df: pd.DataFrame) -> str:
        """Create CSV export efficiently"""
        
        export_cols = [
            'rank', 'ticker', 'company_name', 'master_score',
            'position_score', 'volume_score', 'momentum_score',
            'acceleration_score', 'breakout_score', 'rvol_score',
            'trend_quality', 'price', 'pe', 'eps_current', 'eps_change_pct',
            'from_low_pct', 'from_high_pct',
            'ret_1d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y',
            'rvol', 'vmi', 'money_flow_mm', 'position_tension',
            'momentum_harmony', 'wave_state', 'patterns', 
            'category', 'sector', 'industry', 'eps_tier', 'pe_tier', 'overall_wave_strength'
        ]
        
        available_cols = [col for col in export_cols if col in df.columns]
        
        export_df = df[available_cols].copy()
        
        vol_ratio_cols = [col for col in export_df.columns if 'vol_ratio' in col]
        for col in vol_ratio_cols:
            export_df[col] = (export_df[col] - 1) * 100
        
        return export_df.to_csv(index=False)

# ============================================
# UI COMPONENTS
# ============================================

class UIComponents:
    """Reusable UI components"""
    
    @staticmethod
    def render_metric_card(label: str, value: Any, delta: Optional[str] = None, 
                          help_text: Optional[str] = None) -> None:
        """Render a styled metric card"""
        if help_text:
            st.metric(label, value, delta, help=help_text)
        else:
            st.metric(label, value, delta)

    @staticmethod
    def render_summary_section(df: pd.DataFrame) -> None:
        """Render enhanced summary dashboard"""
        
        if df.empty:
            st.warning("No data available for summary")
            return
        
        st.markdown("### ðŸ“Š Market Pulse")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
            ad_ratio = ad_metrics.get('ad_ratio', 1.0)
            
            if ad_ratio > 2:
                ad_emoji = "ðŸ”¥"
                ad_color = "inverse"
            elif ad_ratio > 1:
                ad_emoji = "ðŸ“ˆ"
                ad_color = "normal"
            else:
                ad_emoji = "ðŸ“‰"
                ad_color = "off"
            
            UIComponents.render_metric_card(
                "A/D Ratio",
                f"{ad_emoji} {ad_ratio:.2f}",
                f"{ad_metrics.get('advancing', 0)}/{ad_metrics.get('declining', 0)}",
                "Advance/Decline Ratio"
            )
        
        with col2:
            high_momentum = len(df[df['momentum_score'] >= 70])
            momentum_pct = (high_momentum / len(df) * 100)
            
            UIComponents.render_metric_card(
                "Momentum Health",
                f"{momentum_pct:.0f}%",
                f"{high_momentum} strong stocks"
            )
        
        with col3:
            avg_rvol = df['rvol'].median() if 'rvol' in df.columns else 1.0
            high_vol_count = len(df[df['rvol'] > 2]) if 'rvol' in df.columns else 0
            
            if avg_rvol > 1.5:
                vol_emoji = "ðŸŒŠ"
            elif avg_rvol > 1.2:
                vol_emoji = "ðŸ’§"
            else:
                vol_emoji = "ðŸœï¸"
            
            UIComponents.render_metric_card(
                "Volume State",
                f"{vol_emoji} {avg_rvol:.1f}x",
                f"{high_vol_count} surges"
            )
        
        with col4:
            risk_factors = 0
            
            if 'from_high_pct' in df.columns and 'momentum_score' in df.columns:
                overextended = len(df[(df['from_high_pct'] >= 0) & (df['momentum_score'] < 50)])
                if overextended > 20:
                    risk_factors += 1
            
            if 'rvol' in df.columns:
                pump_risk = len(df[(df['rvol'] > 10) & (df['master_score'] < 50)])
                if pump_risk > 10:
                    risk_factors += 1
            
            if 'trend_quality' in df.columns:
                downtrends = len(df[df['trend_quality'] < 40])
                if downtrends > len(df) * 0.3:
                    risk_factors += 1
            
            risk_levels = ["ðŸŸ¢ LOW", "ðŸŸ¡ MODERATE", "ðŸŸ  HIGH", "ðŸ”´ EXTREME"]
            risk_level = risk_levels[min(risk_factors, 3)]
            
            UIComponents.render_metric_card(
                "Risk Level",
                risk_level,
                f"{risk_factors} factors"
            )
        
        st.markdown("### ðŸŽ¯ Today's Best Opportunities")
        
        opp_col1, opp_col2, opp_col3 = st.columns(3)
        
        with opp_col1:
            ready_to_run = df[
                (df['momentum_score'] >= 70) & 
                (df['acceleration_score'] >= 70) &
                (df['rvol'] >= 2)
            ].nlargest(5, 'master_score')
            
            st.markdown("**ðŸš€ Ready to Run**")
            if len(ready_to_run) > 0:
                for _, stock in ready_to_run.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"Score: {stock['master_score']:.1f} | RVOL: {stock['rvol']:.1f}x")
            else:
                st.info("No momentum leaders found")
        
        with opp_col2:
            hidden_gems = df[df['patterns'].str.contains('HIDDEN GEM', na=False)].nlargest(5, 'master_score')
            
            st.markdown("**ðŸ’Ž Hidden Gems**")
            if len(hidden_gems) > 0:
                for _, stock in hidden_gems.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"Cat %ile: {stock.get('category_percentile', 0):.0f} | Score: {stock['master_score']:.1f}")
            else:
                st.info("No hidden gems today")
        
        with opp_col3:
            volume_alerts = df[df['rvol'] > 3].nlargest(5, 'master_score')
            
            st.markdown("**âš¡ Volume Alerts**")
            if len(volume_alerts) > 0:
                for _, stock in volume_alerts.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"RVOL: {stock['rvol']:.1f}x | {stock.get('wave_state', 'N/A')}")
            else:
                st.info("No extreme volume detected")
        
        st.markdown("### ðŸ§  Market Intelligence")
        
        intel_col1, intel_col2 = st.columns([2, 1])
        
        with intel_col1:
            sector_rotation = MarketIntelligence.detect_sector_rotation(df)
            
            if not sector_rotation.empty:
                fig = go.Figure()
                
                fig.add_trace(go.Bar(
                    x=sector_rotation.index[:10],
                    y=sector_rotation['flow_score'][:10],
                    text=[f"{val:.1f}" for val in sector_rotation['flow_score'][:10]],
                    textposition='outside',
                    marker_color=['#2ecc71' if score > 60 else '#e74c3c' if score < 40 else '#f39c12' 
                                 for score in sector_rotation['flow_score'][:10]],
                    hovertemplate=(
                        'Sector: %{x}<br>'
                        'Flow Score: %{y:.1f}<br>'
                        'Analyzed: %{customdata[0]} of %{customdata[1]} stocks<br>'
                        'Avg Score: %{customdata[2]:.1f}<br>'
                        'Median Score: %{customdata[3]:.1f}<extra></extra>'
                    ),
                    customdata=np.column_stack((
                        sector_rotation['analyzed_stocks'][:10],
                        sector_rotation['total_stocks'][:10],
                        sector_rotation['avg_score'][:10],
                        sector_rotation['median_score'][:10]
                    ))
                ))
                
                fig.update_layout(
                    title="Sector Rotation Map - Smart Money Flow (Dynamically Sampled)",
                    xaxis_title="Sector",
                    yaxis_title="Flow Score",
                    height=400,
                    template='plotly_white',
                    showlegend=False,
                    xaxis_tickangle=-45
                )
                
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No sector rotation data available for visualization.")
        
        with intel_col2:
            regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
            
            st.markdown(f"**ðŸŽ¯ Market Regime**")
            st.markdown(f"### {regime}")
            
            st.markdown("**ðŸ“¡ Key Signals**")
            
            signals = []
            
            breadth = regime_metrics.get('breadth', 0.5)
            if breadth > 0.6:
                signals.append("âœ… Strong breadth")
            elif breadth < 0.4:
                signals.append("âš ï¸ Weak breadth")
            
            category_spread = regime_metrics.get('category_spread', 0)
            if category_spread > 10:
                signals.append("ðŸ”„ Small caps leading")
            elif category_spread < -10:
                signals.append("ðŸ›¡ï¸ Large caps defensive")
            
            avg_rvol = regime_metrics.get('avg_rvol', 1.0)
            if avg_rvol > 1.5:
                signals.append("ðŸŒŠ High volume activity")
            
            pattern_count = (df['patterns'] != '').sum()
            if pattern_count > len(df) * 0.2:
                signals.append("ðŸŽ¯ Many patterns emerging")
            
            for signal in signals:
                st.write(signal)
            
            st.markdown("**ðŸ’ª Market Strength**")
            
            strength_score = (
                (breadth * 50) +
                (min(avg_rvol, 2) * 25) +
                ((pattern_count / len(df)) * 25)
            )
            
            if strength_score > 70:
                strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢"
            elif strength_score > 50:
                strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢âšª"
            elif strength_score > 30:
                strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢âšªâšª"
            else:
                strength_meter = "ðŸŸ¢ðŸŸ¢âšªâšªâšª"
            
            st.write(strength_meter)
    
    @staticmethod
    def render_pagination_controls(df: pd.DataFrame, display_count: int, page_key: str) -> pd.DataFrame:
        """Renders pagination controls and returns the DataFrame slice for the current page."""
        
        total_rows = len(df)
        if total_rows == 0:
            st.caption("No data to display.")
            return df
        
        session_key = f'wd_current_page_{page_key}'
        
        if session_key not in st.session_state:
            st.session_state[session_key] = 0
            
        current_page = st.session_state[session_key]
        
        total_pages = int(np.ceil(total_rows / display_count))
        
        start_idx = current_page * display_count
        end_idx = min(start_idx + display_count, total_rows)
        
        st.caption(f"Showing {start_idx + 1}-{end_idx} of {total_rows} stocks (Page {current_page + 1} of {total_pages})")
        
        col_prev, col_page_num, col_next = st.columns([1, 0.5, 1])
        
        with col_prev:
            if st.button("â¬…ï¸ Previous Page", disabled=(current_page == 0), key=f'wd_prev_page_{page_key}'):
                st.session_state[session_key] -= 1
                st.rerun()
        with col_page_num:
            pass
        with col_next:
            if st.button("Next Page âž¡ï¸", disabled=(current_page >= total_pages - 1), key=f'wd_next_page_{page_key}'):
                st.session_state[session_key] += 1
                st.rerun()
        
        return df.iloc[start_idx:end_idx]

# ============================================
# MAIN APPLICATION
# ============================================

def main():
    """Main Streamlit application - Final Ultimate Version"""
    
    st.set_page_config(
        page_title="Wave Detection Ultimate 3.2",
        page_icon="ðŸŒŠ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    RobustSessionState.initialize()
    
    st.markdown("""
    <style>
    .main {padding: 0rem 1rem;}
    .stTabs [data-baseweb="tab-list"] {gap: 8px;}
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
    div[data-testid="metric-container"] {
        background-color: rgba(28, 131, 225, 0.1);
        border: 1px solid rgba(28, 131, 225, 0.2);
        padding: 5% 5% 5% 10%;
        border-radius: 5px;
        overflow-wrap: break-word;
    }
    .stAlert {
        padding: 1rem;
        border-radius: 5px;
    }
    div.stButton > button {
        width: 100%;
        transition: all 0.3s ease;
    }
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 10px rgba(0,0,0,0.2);
    }
    @media (max-width: 768px) {
        .stDataFrame {font-size: 12px;}
        div[data-testid="metric-container"] {padding: 3%;}
        .main {padding: 0rem 0.5rem;}
    }
    .stDataFrame > div {overflow-x: auto;}
    .stSpinner > div {
        border-color: #3498db;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    ">
        <h1 style="margin: 0; font-size: 2.5rem;">ðŸŒŠ Wave Detection Ultimate 3.2</h1>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">
            Professional Stock Ranking System â€¢ Final Ultimate Production Version
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    with st.sidebar:
        st.markdown("### ðŸŽ¯ Quick Actions")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ðŸ”„ Refresh Data", type="primary", use_container_width=True):
                st.cache_data.clear()
                RobustSessionState.safe_set('last_refresh', datetime.now(timezone.utc))
                st.rerun()
        
        with col2:
            if st.button("ðŸ§¹ Clear Cache", use_container_width=True):
                st.cache_data.clear()
                gc.collect()
                st.success("Cache cleared!")
                time.sleep(0.5)
                st.rerun()
        
        st.markdown("---")
        st.markdown("### ðŸ“‚ Data Source")
        
        data_source_col1, data_source_col2 = st.columns(2)
        
        with data_source_col1:
            if st.button("ðŸ“Š Google Sheets", 
                        type="primary" if RobustSessionState.safe_get('data_source') == "sheet" else "secondary", 
                        use_container_width=True):
                RobustSessionState.safe_set('data_source', "sheet")
                st.rerun()
        
        with data_source_col2:
            if st.button("ðŸ“ Upload CSV", 
                        type="primary" if RobustSessionState.safe_get('data_source') == "upload" else "secondary", 
                        use_container_width=True):
                RobustSessionState.safe_set('data_source', "upload")
                st.rerun()

        uploaded_file = None
        sheet_id = None
        gid = None
        
        if RobustSessionState.safe_get('data_source') == "upload":
            uploaded_file = st.file_uploader(
                "Choose CSV file", 
                type="csv",
                help="Upload a CSV file with stock data. Must contain 'ticker' and 'price' columns."
            )
            if uploaded_file is None:
                st.info("Please upload a CSV file to continue")
        else:
            st.markdown("#### ðŸ“Š Google Sheets Configuration")
            
            sheet_input = st.text_input(
                "Google Sheets ID or URL",
                value=RobustSessionState.safe_get('sheet_id', ''),
                placeholder="Enter Sheet ID or full URL",
                help="Example: 1OEQ_qxL4lXbO9LlKWDGlDju2yQC1iYvOYeXF3mTQuJM or the full Google Sheets URL"
            )
            
            if sheet_input:
                sheet_id_match = re.search(r'/d/([a-zA-Z0-9-_]+)', sheet_input)
                if sheet_id_match:
                    sheet_id = sheet_id_match.group(1)
                else:
                    sheet_id = sheet_input.strip()
                
                RobustSessionState.safe_set('sheet_id', sheet_id)
            
            gid_input = st.text_input(
                "Sheet Tab GID (Optional)",
                value=RobustSessionState.safe_get('gid', CONFIG.DEFAULT_GID),
                placeholder=f"Default: {CONFIG.DEFAULT_GID}",
                help="The GID identifies specific sheet tab. Found in URL after #gid="
            )
            
            if gid_input:
                gid = gid_input.strip()
            else:
                gid = CONFIG.DEFAULT_GID
            
            if not sheet_id:
                st.warning("Please enter a Google Sheets ID to continue")
        
        data_quality = RobustSessionState.safe_get('data_quality', {})
        if data_quality:
            with st.expander("ðŸ“Š Data Quality", expanded=True):
                col1, col2 = st.columns(2)
                with col1:
                    completeness = data_quality.get('completeness', 0)
                    if completeness > 80:
                        emoji = "ðŸŸ¢"
                    elif completeness > 60:
                        emoji = "ðŸŸ¡"
                    else:
                        emoji = "ðŸ”´"
                    
                    st.metric("Completeness", f"{emoji} {completeness:.1f}%")
                    st.metric("Total Stocks", f"{data_quality.get('total_rows', 0):,}")
                
                with col2:
                    if 'timestamp' in data_quality:
                        age = datetime.now(timezone.utc) - data_quality['timestamp']
                        hours = age.total_seconds() / 3600
                        
                        if hours < 1:
                            freshness = "ðŸŸ¢ Fresh"
                        elif hours < 24:
                            freshness = "ðŸŸ¡ Recent"
                        else:
                            freshness = "ðŸ”´ Stale"
                        
                        st.metric("Data Age", freshness)
                    
                    duplicates = data_quality.get('duplicate_tickers', 0)
                    if duplicates > 0:
                        st.metric("Duplicates", f"âš ï¸ {duplicates}")
        
        perf_metrics = RobustSessionState.safe_get('performance_metrics', {})
        if perf_metrics:
            with st.expander("âš¡ Performance"):
                total_time = sum(perf_metrics.values())
                if total_time < 3:
                    perf_emoji = "ðŸŸ¢"
                elif total_time < 5:
                    perf_emoji = "ðŸŸ¡"
                else:
                    perf_emoji = "ðŸ”´"
                
                st.metric("Load Time", f"{perf_emoji} {total_time:.1f}s")
                
                if len(perf_metrics) > 0:
                    slowest = sorted(perf_metrics.items(), key=lambda x: x[1], reverse=True)[:3]
                    for func_name, elapsed in slowest:
                        if elapsed > 0.001:
                            st.caption(f"{func_name}: {elapsed:.4f}s")
        
        st.markdown("---")
        st.markdown("### ðŸ” Smart Filters")
        
        active_filter_count = 0
        
        if RobustSessionState.safe_get('quick_filter_applied', False):
            active_filter_count += 1
        
        filter_keys_to_check = [
            'category_filter', 'sector_filter', 'industry_filter', 'min_score', 'patterns',
            'trend_filter', 'eps_tier_filter', 'pe_tier_filter', 'price_tier_filter',
            'min_eps_change', 'min_pe', 'max_pe', 'require_fundamental_data',
            'wave_states_filter', 'wave_strength_range_slider'
        ]
        
        for key in filter_keys_to_check:
            value = RobustSessionState.safe_get(key)
            if (isinstance(value, (list, set)) and len(value) > 0) or \
               (isinstance(value, str) and value.strip() not in ["", "All Trends"]) or \
               (isinstance(value, (int, float)) and value != 0) or \
               (isinstance(value, tuple) and value != (0, 100)) or \
               (isinstance(value, bool) and value):
                active_filter_count += 1
        
        RobustSessionState.safe_set('active_filter_count', active_filter_count)
        
        if active_filter_count > 0:
            st.info(f"ðŸ” **{active_filter_count} filter{'s' if active_filter_count > 1 else ''} active**")
        
        if st.button("ðŸ—‘ï¸ Clear All Filters", 
                    use_container_width=True, 
                    type="primary" if active_filter_count > 0 else "secondary"):
            RobustSessionState.clear_filters()
            st.success("âœ… All filters cleared!")
            st.rerun()
        
        st.markdown("---")
        show_debug = st.checkbox("ðŸ› Show Debug Info", 
                               value=RobustSessionState.safe_get('show_debug', False),
                               key="show_debug")
    
    try:
        if RobustSessionState.safe_get('data_source') == "upload" and uploaded_file is None:
            st.warning("Please upload a CSV file to continue")
            st.stop()
        
        if RobustSessionState.safe_get('data_source') == "sheet" and not sheet_id:
            st.warning("Please enter a Google Sheets ID to continue")
            st.stop()
        
        cache_key_prefix = f"{RobustSessionState.safe_get('data_source')}_{sheet_id}_{gid}"
        current_hour_key = datetime.now(timezone.utc).strftime('%Y-%m-%d-%H')
        data_version_hash = hashlib.md5(f"{cache_key_prefix}_{current_hour_key}".encode()).hexdigest()

        with st.spinner("ðŸ“¥ Loading and processing data..."):
            try:
                if RobustSessionState.safe_get('data_source') == "upload" and uploaded_file is not None:
                    ranked_df, data_timestamp, metadata = load_and_process_data(
                        "upload", file_data=uploaded_file, data_version=data_version_hash
                    )
                else:
                    ranked_df, data_timestamp, metadata = load_and_process_data(
                        "sheet", 
                        sheet_id=sheet_id,
                        gid=gid,
                        data_version=data_version_hash
                    )
                
                RobustSessionState.safe_set('ranked_df', ranked_df)
                RobustSessionState.safe_set('data_timestamp', data_timestamp)
                RobustSessionState.safe_set('last_refresh', datetime.now(timezone.utc))
                
                if metadata.get('warnings'):
                    for warning in metadata['warnings']:
                        st.warning(warning)
                
                if metadata.get('errors'):
                    for error in metadata['errors']:
                        st.error(error)
                
            except Exception as e:
                logger.error(f"Failed to load data: {str(e)}")
                
                last_good_data = RobustSessionState.safe_get('last_good_data')
                if last_good_data:
                    ranked_df, data_timestamp, metadata = last_good_data
                    st.warning("Failed to load fresh data, using cached version.")
                else:
                    st.error(f"âŒ Error: {str(e)}")
                    st.info("Common issues:\n- Invalid Google Sheets ID\n- Sheet not publicly accessible\n- Network connectivity\n- Invalid CSV format")
                    st.stop()
    except Exception as e:
        st.error(f"âŒ Critical Error: {str(e)}")
        with st.expander("ðŸ” Error Details"):
            st.code(str(e))
        st.stop()
    
    st.markdown("### âš¡ Quick Actions")
    qa_col1, qa_col2, qa_col3, qa_col4, qa_col5 = st.columns(5)
    
    quick_filter_applied = RobustSessionState.safe_get('quick_filter_applied', False)
    quick_filter = RobustSessionState.safe_get('quick_filter', None)
    
    with qa_col1:
        if st.button("ðŸ“ˆ Top Gainers", use_container_width=True):
            RobustSessionState.safe_set('quick_filter', 'top_gainers')
            RobustSessionState.safe_set('quick_filter_applied', True)
            st.rerun()
    
    with qa_col2:
        if st.button("ðŸ”¥ Volume Surges", use_container_width=True):
            RobustSessionState.safe_set('quick_filter', 'volume_surges')
            RobustSessionState.safe_set('quick_filter_applied', True)
            st.rerun()
    
    with qa_col3:
        if st.button("ðŸŽ¯ Breakout Ready", use_container_width=True):
            RobustSessionState.safe_set('quick_filter', 'breakout_ready')
            RobustSessionState.safe_set('quick_filter_applied', True)
            st.rerun()
    
    with qa_col4:
        if st.button("ðŸ’Ž Hidden Gems", use_container_width=True):
            RobustSessionState.safe_set('quick_filter', 'hidden_gems')
            RobustSessionState.safe_set('quick_filter_applied', True)
            st.rerun()
    
    with qa_col5:
        if st.button("ðŸŒŠ Show All", use_container_width=True):
            RobustSessionState.safe_set('quick_filter', None)
            RobustSessionState.safe_set('quick_filter_applied', False)
            st.rerun()
    
    if quick_filter:
        if quick_filter == 'top_gainers':
            ranked_df_display = ranked_df[ranked_df['momentum_score'] >= 80]
            st.info(f"Showing {len(ranked_df_display)} stocks with momentum score â‰¥ 80")
        elif quick_filter == 'volume_surges':
            ranked_df_display = ranked_df[ranked_df['rvol'] >= 3]
            st.info(f"Showing {len(ranked_df_display)} stocks with RVOL â‰¥ 3x")
        elif quick_filter == 'breakout_ready':
            ranked_df_display = ranked_df[ranked_df['breakout_score'] >= 80]
            st.info(f"Showing {len(ranked_df_display)} stocks with breakout score â‰¥ 80")
        elif quick_filter == 'hidden_gems':
            ranked_df_display = ranked_df[ranked_df['patterns'].str.contains('HIDDEN GEM', na=False)]
            st.info(f"Showing {len(ranked_df_display)} hidden gem stocks")
    else:
        ranked_df_display = ranked_df
    
    with st.sidebar:
        filters = {}
        
        st.markdown("### ðŸ“Š Display Mode")
        display_mode = st.radio(
            "Choose your view:",
            options=["Technical", "Hybrid (Technical + Fundamentals)"],
            index=0 if RobustSessionState.safe_get('user_preferences', {}).get('display_mode', 'Technical') == 'Technical' else 1,
            help="Technical: Pure momentum analysis | Hybrid: Adds PE & EPS data",
            key="display_mode_toggle"
        )
        
        user_prefs = RobustSessionState.safe_get('user_preferences', {})
        user_prefs['display_mode'] = display_mode
        RobustSessionState.safe_set('user_preferences', user_prefs)
        show_fundamentals = (display_mode == "Hybrid (Technical + Fundamentals)")
        
        st.markdown("---")
        
        categories = FilterEngine.get_filter_options(ranked_df_display, 'category', filters)
        selected_categories = st.multiselect(
            "Market Cap Category", options=categories, default=RobustSessionState.safe_get('category_filter', []),
            placeholder="Select categories (empty = All)", key="category_filter"
        )
        filters['categories'] = selected_categories
        
        sectors = FilterEngine.get_filter_options(ranked_df_display, 'sector', filters)
        selected_sectors = st.multiselect(
            "Sector", options=sectors, default=RobustSessionState.safe_get('sector_filter', []),
            placeholder="Select sectors (empty = All)", key="sector_filter"
        )
        filters['sectors'] = selected_sectors
        
        if 'industry' in ranked_df_display.columns:
            temp_df = ranked_df_display.copy()
            if selected_sectors:
                temp_df = temp_df[temp_df['sector'].isin(selected_sectors)]
            industries = FilterEngine.get_filter_options(temp_df, 'industry', filters)
            
            selected_industries = st.multiselect(
                "Industry", options=industries, default=RobustSessionState.safe_get('industry_filter', []),
                placeholder="Select industries (empty = All)", key="industry_filter"
            )
            filters['industries'] = selected_industries
        
        filters['min_score'] = st.slider(
            "Minimum Master Score", min_value=0, max_value=100, value=RobustSessionState.safe_get('min_score', 0), step=5,
            help="Filter stocks by minimum score", key="min_score"
        )
        
        all_patterns = set()
        for patterns_str in ranked_df_display['patterns'].dropna():
            if patterns_str:
                all_patterns.update(patterns_str.split(' | '))
        
        if all_patterns:
            filters['patterns'] = st.multiselect(
                "Patterns", options=sorted(all_patterns), default=RobustSessionState.safe_get('patterns', []),
                placeholder="Select patterns (empty = All)", help="Filter by specific patterns", key="patterns"
            )
        
        st.markdown("#### ðŸ“ˆ Trend Strength")
        trend_options = {
            "All Trends": (0, 100), "ðŸ”¥ Strong Uptrend (80+)": (80, 100),
            "âœ… Good Uptrend (60-79)": (60, 79), "âž¡ï¸ Neutral Trend (40-59)": (40, 59),
            "âš ï¸ Weak/Downtrend (<40)": (0, 39)
        }
        default_trend_key = RobustSessionState.safe_get('trend_filter', "All Trends")
        try: current_trend_index = list(trend_options.keys()).index(default_trend_key)
        except ValueError: current_trend_index = 0
        filters['trend_filter'] = st.selectbox(
            "Trend Quality", options=list(trend_options.keys()), index=current_trend_index,
            key="trend_filter", help="Filter stocks by trend strength based on SMA alignment"
        )
        filters['trend_range'] = trend_options[filters['trend_filter']]

        st.markdown("#### ðŸŒŠ Wave Filters")
        wave_states_options = FilterEngine.get_filter_options(ranked_df_display, 'wave_state', filters)
        filters['wave_states'] = st.multiselect(
            "Wave State", options=wave_states_options, default=RobustSessionState.safe_get('wave_states_filter', []),
            placeholder="Select wave states (empty = All)", help="Filter by the detected 'Wave State'", key="wave_states_filter"
        )

        if 'overall_wave_strength' in ranked_df_display.columns:
            min_strength = float(ranked_df_display['overall_wave_strength'].min())
            max_strength = float(ranked_df_display['overall_wave_strength'].max())
            slider_min_val, slider_max_val = 0, 100
            default_range_value = (int(min_strength), int(max_strength)) if pd.notna(min_strength) and pd.notna(max_strength) else (0, 100)
            current_slider_value = RobustSessionState.safe_get('wave_strength_range_slider', default_range_value)
            current_slider_value = (max(slider_min_val, min(slider_max_val, current_slider_value[0])),
                                    max(slider_min_val, min(slider_max_val, current_slider_value[1])))
            filters['wave_strength_range'] = st.slider(
                "Overall Wave Strength", min_value=slider_min_val, max_value=slider_max_val,
                value=current_slider_value, step=1, help="Filter by the calculated 'Overall Wave Strength' score",
                key="wave_strength_range_slider"
            )
        else: filters['wave_strength_range'] = (0, 100)
        
        with st.expander("ðŸ”§ Advanced Filters"):
            for tier_type, col_name in [('eps_tiers', 'eps_tier'), ('pe_tiers', 'pe_tier'), ('price_tiers', 'price_tier')]:
                if col_name in ranked_df_display.columns:
                    tier_options = FilterEngine.get_filter_options(ranked_df_display, col_name, filters)
                    selected_tiers = st.multiselect(
                        f"{col_name.replace('_', ' ').title()}", options=tier_options,
                        default=RobustSessionState.safe_get(f'{col_name}_filter', []),
                        placeholder=f"Select {col_name.replace('_', ' ')}s (empty = All)", key=f"{col_name}_filter"
                    )
                    filters[tier_type] = selected_tiers
            
            if 'eps_change_pct' in ranked_df_display.columns:
                eps_change_input = st.text_input("Min EPS Change %", value=RobustSessionState.safe_get('min_eps_change', ""), placeholder="e.g. -50", key="min_eps_change")
                if eps_change_input.strip():
                    try: filters['min_eps_change'] = float(eps_change_input)
                    except ValueError: st.error("Please enter a valid number for EPS change"); filters['min_eps_change'] = None
                else: filters['min_eps_change'] = None
            
            if show_fundamentals and 'pe' in ranked_df_display.columns:
                st.markdown("**ðŸ” Fundamental Filters**")
                col1, col2 = st.columns(2)
                with col1:
                    min_pe_input = st.text_input("Min PE Ratio", value=RobustSessionState.safe_get('min_pe', ""), placeholder="e.g. 10", key="min_pe")
                    if min_pe_input.strip():
                        try: filters['min_pe'] = float(min_pe_input)
                        except ValueError: st.error("Invalid Min PE"); filters['min_pe'] = None
                    else: filters['min_pe'] = None
                with col2:
                    max_pe_input = st.text_input("Max PE Ratio", value=RobustSessionState.safe_get('max_pe', ""), placeholder="e.g. 30", key="max_pe")
                    if max_pe_input.strip():
                        try: filters['max_pe'] = float(max_pe_input)
                        except ValueError: st.error("Invalid Max PE"); filters['max_pe'] = None
                    else: filters['max_pe'] = None
                filters['require_fundamental_data'] = st.checkbox(
                    "Only show stocks with PE and EPS data", value=RobustSessionState.safe_get('require_fundamental_data', False), key="require_fundamental_data"
                )
    
    filtered_df = FilterEngine.apply_all_filters(ranked_df, filters)
    filtered_df = filtered_df.sort_values('rank')
    
    user_prefs = RobustSessionState.safe_get('user_preferences', {})
    user_prefs['last_filters'] = filters
    RobustSessionState.safe_set('user_preferences', user_prefs)
    
    if show_debug:
        with st.sidebar.expander("ðŸ› Debug Info", expanded=True):
            st.write("**Active Filters:**")
            for key, value in filters.items():
                if value is not None and value != [] and value != 0 and (not (isinstance(value, tuple) and value == (0,100))):
                    st.write(f"â€¢ {key}: {value}")
            st.write(f"\n**Filter Result:**")
            st.write(f"Before: {len(ranked_df)} stocks")
            st.write(f"After: {len(filtered_df)} stocks")
            perf_metrics = RobustSessionState.safe_get('performance_metrics', {})
            if perf_metrics:
                st.write(f"\n**Performance:**")
                for func, time_taken in perf_metrics.items():
                    if time_taken > 0.001: st.write(f"â€¢ {func}: {time_taken:.4f}s")
    
    active_filter_count = RobustSessionState.safe_get('active_filter_count', 0)
    if active_filter_count > 0 or quick_filter_applied:
        filter_status_col1, filter_status_col2 = st.columns([5, 1])
        with filter_status_col1:
            if quick_filter:
                quick_filter_names = {'top_gainers': 'ðŸ“ˆ Top Gainers', 'volume_surges': 'ðŸ”¥ Volume Surges', 'breakout_ready': 'ðŸŽ¯ Breakout Ready', 'hidden_gems': 'ðŸ’Ž Hidden Gems'}
                filter_display = quick_filter_names.get(quick_filter, 'Filtered')
                if active_filter_count > 1:
                    st.info(f"**Viewing:** {filter_display} + {active_filter_count - 1} other filter{'s' if active_filter_count > 2 else ''} | **{len(filtered_df):,} stocks** shown")
                else:
                    st.info(f"**Viewing:** {filter_display} | **{len(filtered_df):,} stocks** shown")
        with filter_status_col2:
            if st.button("Clear Filters", type="secondary"):
                RobustSessionState.safe_set('trigger_clear', True)
                st.rerun()
    
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    
    with col1:
        total_stocks = len(filtered_df)
        total_original = len(ranked_df)
        pct_of_all = (total_stocks/total_original*100) if total_original > 0 else 0
        UIComponents.render_metric_card("Total Stocks", f"{total_stocks:,}", f"{pct_of_all:.0f}% of {total_original:,}")
    with col2:
        if not filtered_df.empty and 'master_score' in filtered_df.columns:
            avg_score = filtered_df['master_score'].mean()
            std_score = filtered_df['master_score'].std()
            UIComponents.render_metric_card("Avg Score", f"{avg_score:.1f}", f"Ïƒ={std_score:.1f}")
        else: UIComponents.render_metric_card("Avg Score", "N/A")
    with col3:
        if show_fundamentals and 'pe' in filtered_df.columns:
            valid_pe = filtered_df['pe'].notna() & (filtered_df['pe'] > 0) & (filtered_df['pe'] < 10000)
            pe_coverage = valid_pe.sum()
            pe_pct = (pe_coverage / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
            if pe_coverage > 0:
                median_pe = filtered_df.loc[valid_pe, 'pe'].median()
                UIComponents.render_metric_card("Median PE", f"{median_pe:.1f}x", f"{pe_pct:.0f}% have data")
            else: UIComponents.render_metric_card("PE Data", "Limited", "No PE data")
        else:
            if not filtered_df.empty and 'master_score' in filtered_df.columns:
                min_score, max_score = filtered_df['master_score'].min(), filtered_df['master_score'].max()
                UIComponents.render_metric_card("Score Range", f"{min_score:.1f}-{max_score:.1f}")
            else: UIComponents.render_metric_card("Score Range", "N/A")
    with col4:
        if show_fundamentals and 'eps_change_pct' in filtered_df.columns:
            valid_eps_change = filtered_df['eps_change_pct'].notna()
            growth_count, strong_count, mega_growth = positive_eps_growth.sum(), strong_growth.sum(), mega_growth.sum()
            UIComponents.render_metric_card("EPS Growth +ve", f"{growth_count}", f"{strong_count} >50% | {mega_growth} >100%")
        else:
            accelerating = (filtered_df['acceleration_score'] >= 80).sum() if 'acceleration_score' in filtered_df.columns else 0
            UIComponents.render_metric_card("Accelerating", f"{accelerating}")
    with col5:
        high_rvol = (filtered_df['rvol'] > 2).sum() if 'rvol' in filtered_df.columns else 0
        UIComponents.render_metric_card("High RVOL", f"{high_rvol}")
    with col6:
        if 'trend_quality' in filtered_df.columns:
            strong_trends = (filtered_df['trend_quality'] >= 80).sum()
            total = len(filtered_df)
            UIComponents.render_metric_card("Strong Trends", f"{strong_trends}", f"{strong_trends/total*100:.0f}%" if total > 0 else "0%")
        else:
            with_patterns = (filtered_df['patterns'] != '').sum()
            UIComponents.render_metric_card("With Patterns", f"{with_patterns}")
    
    tabs = st.tabs(["ðŸ“Š Summary", "ðŸ† Rankings", "ðŸŒŠ Wave Radar", "ðŸ“Š Analysis", "ðŸ” Search", "ðŸ“¥ Export", "â„¹ï¸ About"])
    
    with tabs[0]:
        st.markdown("### ðŸ“Š Executive Summary Dashboard")
        if not filtered_df.empty:
            UIComponents.render_summary_section(filtered_df)
            st.markdown("---")
            st.markdown("#### ðŸ’¾ Download Clean Processed Data")
            download_cols = st.columns(3)
            with download_cols[0]:
                st.markdown("**ðŸ“Š Current View Data**")
                st.write(f"Includes {len(filtered_df)} stocks matching current filters")
                csv_filtered = ExportEngine.create_csv_export(filtered_df)
                st.download_button(label="ðŸ“¥ Download Filtered Data (CSV)", data=csv_filtered, file_name=f"wave_detection_filtered_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")
            with download_cols[1]:
                st.markdown("**ðŸ† Top 100 Stocks**")
                st.write("Elite stocks ranked by Master Score")
                top_100 = filtered_df.nlargest(100, 'master_score')
                csv_top100 = ExportEngine.create_csv_export(top_100)
                st.download_button(label="ðŸ“¥ Download Top 100 (CSV)", data=csv_top100, file_name=f"wave_detection_top100_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")
            with download_cols[2]:
                st.markdown("**ðŸŽ¯ Pattern Stocks Only**")
                pattern_stocks = filtered_df[filtered_df['patterns'] != '']
                st.write(f"Includes {len(pattern_stocks)} stocks with patterns")
                if len(pattern_stocks) > 0:
                    csv_patterns = ExportEngine.create_csv_export(pattern_stocks)
                    st.download_button(label="ðŸ“¥ Download Pattern Stocks (CSV)", data=csv_patterns, file_name=f"wave_detection_patterns_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")
                else: st.info("No stocks with patterns in current filter")
        else: st.warning("No data available for summary. Please adjust filters.")
    
    with tabs[1]:
        st.markdown("### ðŸ† Top Ranked Stocks")
        col1, col2, col3 = st.columns([2, 2, 6])
        with col1:
            user_prefs = RobustSessionState.safe_get('user_preferences', {})
            display_count = st.selectbox("Show top", options=CONFIG.AVAILABLE_TOP_N, index=CONFIG.AVAILABLE_TOP_N.index(user_prefs.get('default_top_n', CONFIG.DEFAULT_TOP_N)))
            user_prefs['default_top_n'] = display_count
            RobustSessionState.safe_set('user_preferences', user_prefs)
        with col2:
            sort_options = ['Rank', 'Master Score', 'RVOL', 'Momentum', 'Money Flow']
            if 'trend_quality' in filtered_df.columns: sort_options.append('Trend')
            sort_by = st.selectbox("Sort by", options=sort_options, index=0)
        
        display_df = filtered_df.copy()
        
        if sort_by == 'Master Score': display_df = display_df.sort_values('master_score', ascending=False)
        elif sort_by == 'RVOL': display_df = display_df.sort_values('rvol', ascending=False)
        elif sort_by == 'Momentum': display_df = display_df.sort_values('momentum_score', ascending=False)
        elif sort_by == 'Money Flow' and 'money_flow_mm' in display_df.columns: display_df = display_df.sort_values('money_flow_mm', ascending=False)
        elif sort_by == 'Trend' and 'trend_quality' in display_df.columns: display_df = display_df.sort_values('trend_quality', ascending=False)
        
        if not display_df.empty:
            if 'trend_quality' in display_df.columns:
                def get_trend_indicator(score):
                    if pd.isna(score): return "âž–"
                    elif score >= 80: return "ðŸ”¥"
                    elif score >= 60: return "âœ…"
                    elif score >= 40: return "âž¡ï¸"
                    else: return "âš ï¸"
                display_df['trend_indicator'] = display_df['trend_quality'].apply(get_trend_indicator)
            
            display_cols = {'rank': 'Rank', 'ticker': 'Ticker', 'company_name': 'Company', 'master_score': 'Score', 'wave_state': 'Wave'}
            if 'trend_indicator' in display_df.columns: display_cols['trend_indicator'] = 'Trend'
            display_cols['price'] = 'Price'
            if show_fundamentals:
                if 'pe' in display_df.columns: display_cols['pe'] = 'PE'
                if 'eps_change_pct' in display_df.columns: display_cols['eps_change_pct'] = 'EPS Î”%'
            display_cols.update({
                'from_low_pct': 'From Low', 'ret_30d': '30D Ret', 'rvol': 'RVOL', 'vmi': 'VMI', 'patterns': 'Patterns',
                'category': 'Category', 'industry': 'Industry'
            })
            
            format_rules = {
                'master_score': '{:.1f}', 'price': 'â‚¹{:,.0f}', 'from_low_pct': '{:.0f}%',
                'ret_30d': '{:+.1f}%', 'rvol': '{:.1f}x', 'vmi': '{:.2f}'
            }
            def format_pe(value):
                try: val = float(value); return 'Loss' if val <= 0 else '>10K' if val > 10000 else f"{val:.0f}" if val > 1000 else f"{val:.1f}"
                except: return '-'
            def format_eps_change(value):
                try: val = float(value); return f"{val/1000:+.1f}K%" if abs(val) >= 1000 else f"{val:+.0f}%" if abs(val) >= 100 else f"{val:+.1f}%"
                except: return '-'
            
            for col, fmt in format_rules.items():
                if col in display_df.columns: display_df[col] = display_df[col].apply(lambda x: fmt.format(x) if pd.notna(x) and isinstance(x, (int, float)) else '-')
            if show_fundamentals:
                if 'pe' in display_df.columns: display_df['pe'] = display_df['pe'].apply(format_pe)
                if 'eps_change_pct' in display_df.columns: display_df['eps_change_pct'] = display_df['eps_change_pct'].apply(format_eps_change)
            
            available_display_cols = [c for c in display_cols.keys() if c in display_df.columns]
            display_df = display_df[available_display_cols]
            display_df.columns = [display_cols[c] for c in available_display_cols]
            
            paginated_df = UIComponents.render_pagination_controls(display_df, display_count, 'rankings')
            st.dataframe(paginated_df, use_container_width=True, height=min(600, len(paginated_df) * 35 + 50), hide_index=True)
            
            with st.expander("ðŸ“Š Quick Statistics"):
                stat_cols = st.columns(4)
                with stat_cols[0]:
                    st.markdown("**Score Distribution**")
                    scores_data = filtered_df['master_score'].dropna()
                    if not scores_data.empty:
                        st.text(f"Max: {scores_data.max():.1f}"); st.text(f"Min: {scores_data.min():.1f}"); st.text(f"Mean: {scores_data.mean():.1f}")
                        st.text(f"Median: {scores_data.median():.1f}"); st.text(f"Q1: {scores_data.quantile(0.25):.1f}"); st.text(f"Q3: {scores_data.quantile(0.75):.1f}")
                        st.text(f"Std: {scores_data.std():.1f}")
                    else: st.text("No valid scores.")
                with stat_cols[1]:
                    st.markdown("**Returns (30D)**")
                    returns_data = filtered_df['ret_30d'].dropna()
                    if not returns_data.empty:
                        st.text(f"Max: {returns_data.max():.1f}%"); st.text(f"Min: {returns_data.min():.1f}%"); st.text(f"Avg: {returns_data.mean():.1f}%")
                        st.text(f"Positive: {(returns_data > 0).sum()}")
                    else: st.text("No 30D return data available")
                with stat_cols[2]:
                    if show_fundamentals:
                        st.markdown("**Fundamentals**")
                        valid_pe = filtered_df['pe'].dropna()
                        valid_pe = valid_pe[(valid_pe > 0) & (valid_pe < 10000)]
                        st.text(f"Median PE: {valid_pe.median():.1f}x" if not valid_pe.empty else "No valid PE.")
                        valid_eps = filtered_df['eps_change_pct'].dropna()
                        st.text(f"Positive EPS: {(valid_eps > 0).sum()}" if not valid_eps.empty else "No valid EPS change.")
                    else:
                        st.markdown("**Volume**")
                        rvol_data = filtered_df['rvol'].dropna()
                        st.text(f"Max: {rvol_data.max():.1f}x" if not rvol_data.empty else "No valid RVOL.")
                        st.text(f"Avg: {rvol_data.mean():.1f}x" if not rvol_data.empty else "")
                        st.text(f">2x: {(rvol_data > 2).sum()}" if not rvol_data.empty else "")
                with stat_cols[3]:
                    st.markdown("**Trend Distribution**")
                    if 'trend_quality' in filtered_df.columns:
                        trend_data = filtered_df['trend_quality'].dropna()
                        if not trend_data.empty:
                            total = len(trend_data)
                            st.text(f"Avg Trend Score: {trend_data.mean():.1f}")
                            st.text(f"Above All SMAs: {(trend_data >= 85).sum()}")
                            st.text(f"In Uptrend (60+): {(trend_data >= 60).sum()}")
                            st.text(f"In Downtrend (<40): {(trend_data < 40).sum()}")
                        else: st.text("No valid trend data.")
                    else: st.text("No trend data available")
        else: st.warning("No stocks match the selected filters.")
    
    with tabs[2]:
        st.markdown("### ðŸŒŠ Wave Radar - Early Momentum Detection System")
        st.markdown("*Catch waves as they form, not after they've peaked!*")
        radar_col1, radar_col2, radar_col3, radar_col4 = st.columns([2, 2, 2, 1])
        with radar_col1:
            wave_timeframe = st.selectbox("Wave Detection Timeframe", options=["All Waves", "Intraday Surge", "3-Day Buildup", "Weekly Breakout", "Monthly Trend"], index=["All Waves", "Intraday Surge", "3-Day Buildup", "Weekly Breakout", "Monthly Trend"].index(RobustSessionState.safe_get('wave_timeframe_select', "All Waves")), key="wave_timeframe_select")
        with radar_col2:
            sensitivity = st.select_slider("Detection Sensitivity", options=["Conservative", "Balanced", "Aggressive"], value=RobustSessionState.safe_get('wave_sensitivity', "Balanced"), key="wave_sensitivity")
            show_sensitivity_details = st.checkbox("Show thresholds", value=RobustSessionState.safe_get('show_sensitivity_details', False), key="show_sensitivity_details")
        with radar_col3:
            show_market_regime = st.checkbox("ðŸ“Š Market Regime Analysis", value=RobustSessionState.safe_get('show_market_regime', True), key="show_market_regime")
        wave_filtered_df = filtered_df.copy()
        with radar_col4:
            if not wave_filtered_df.empty and 'overall_wave_strength' in wave_filtered_df.columns:
                wave_strength_score = wave_filtered_df['overall_wave_strength'].mean()
                wave_emoji, wave_color = ("ðŸŒŠðŸ”¥", "ðŸŸ¢") if wave_strength_score > 70 else ("ðŸŒŠ", "ðŸŸ¡") if wave_strength_score > 50 else ("ðŸ’¤", "ðŸ”´")
                UIComponents.render_metric_card("Wave Strength", f"{wave_emoji} {wave_strength_score:.0f}%", f"{wave_color} Market")
            else: UIComponents.render_metric_card("Wave Strength", "N/A", "Data not available")
        if show_sensitivity_details:
            with st.expander("ðŸ“Š Current Sensitivity Thresholds", expanded=True):
                if sensitivity == "Conservative": st.markdown("... Conservative Settings ...")
                elif sensitivity == "Balanced": st.markdown("... Balanced Settings ...")
                else: st.markdown("... Aggressive Settings ...")
                st.info("ðŸ’¡ **Tip**: Start with Balanced, then adjust based on market conditions and your risk tolerance.")
        
        if wave_timeframe != "All Waves":
            if wave_timeframe == "Intraday Surge": wave_filtered_df = wave_filtered_df[(wave_filtered_df['rvol'] >= 2.5) & (wave_filtered_df['ret_1d'] > 2)]
            elif wave_timeframe == "3-Day Buildup": wave_filtered_df = wave_filtered_df[(wave_filtered_df['ret_3d'] > 5) & (wave_filtered_df['vol_ratio_7d_90d'] > 1.5)]
            elif wave_timeframe == "Weekly Breakout": wave_filtered_df = wave_filtered_df[(wave_filtered_df['ret_7d'] > 8) & (wave_filtered_df['vol_ratio_7d_90d'] > 2.0)]
            elif wave_timeframe == "Monthly Trend": wave_filtered_df = wave_filtered_df[(wave_filtered_df['ret_30d'] > 15) & (wave_filtered_df['price'] > wave_filtered_df['sma_20d'])]

        if not wave_filtered_df.empty:
            st.markdown("#### ðŸš€ Momentum Shifts - Stocks Entering Strength")
            if sensitivity == "Conservative": momentum_threshold, acceleration_threshold, min_rvol = 60, 70, 3.0
            elif sensitivity == "Balanced": momentum_threshold, acceleration_threshold, min_rvol = 50, 60, 2.0
            else: momentum_threshold, acceleration_threshold, min_rvol = 40, 50, 1.5
            
            momentum_shifts = wave_filtered_df[(wave_filtered_df['momentum_score'] >= momentum_threshold) & (wave_filtered_df['acceleration_score'] >= acceleration_threshold)].copy()
            if len(momentum_shifts) > 0:
                momentum_shifts['signal_count'] = ((momentum_shifts['momentum_score'] >= momentum_threshold).astype(int) + (momentum_shifts['acceleration_score'] >= acceleration_threshold).astype(int) + (momentum_shifts['rvol'] >= min_rvol).astype(int))
                top_shifts = momentum_shifts.nlargest(20, 'signal_count')
                st.dataframe(top_shifts, use_container_width=True, hide_index=True)
            else: st.info(f"No momentum shifts detected in {wave_timeframe} timeframe.")
            
            st.markdown("#### ðŸš€ Acceleration Profiles - Momentum Building Over Time")
            accel_threshold = 85 if sensitivity == "Conservative" else 70 if sensitivity == "Balanced" else 60
            accelerating_stocks = wave_filtered_df[wave_filtered_df['acceleration_score'] >= accel_threshold].nlargest(10, 'acceleration_score')
            if len(accelerating_stocks) > 0:
                fig_accel = Visualizer.create_acceleration_profiles(accelerating_stocks, n=10)
                st.plotly_chart(fig_accel, use_container_width=True)
            else: st.info(f"No stocks meet the acceleration threshold ({accel_threshold}+) for {sensitivity} sensitivity.")
        else: st.warning("No data available for Wave Radar analysis.")
    
    with tabs[3]:
        st.markdown("### ðŸ“Š Market Analysis")
        if not filtered_df.empty:
            st.markdown("---")
            st.markdown("### ðŸ“ˆ Performance Analysis")
            perf_tabs = st.tabs(["ðŸ¢ Sector Performance", "ðŸ­ Industry Performance"])
            
            with perf_tabs[0]:
                sector_overview_df = MarketIntelligence.detect_sector_rotation(filtered_df)
                if not sector_overview_df.empty:
                    fig_sector = go.Figure(data=go.Bar(x=sector_overview_df.index[:15], y=sector_overview_df['flow_score'][:15]))
                    st.plotly_chart(fig_sector, use_container_width=True)
                    st.dataframe(sector_overview_df, use_container_width=True)
                else: st.info("No sector data available.")
            
            with perf_tabs[1]:
                industry_overview_df = MarketIntelligence.detect_industry_rotation(filtered_df)
                if not industry_overview_df.empty:
                    fig_industry = go.Figure(data=go.Bar(x=industry_overview_df.index[:15], y=industry_overview_df['flow_score'][:15]))
                    st.plotly_chart(fig_industry, use_container_width=True)
                    st.dataframe(industry_overview_df, use_container_width=True)
                else: st.info("No industry data available.")
        else: st.info("No data available for analysis.")
    
    with tabs[4]:
        st.markdown("### ðŸ” Advanced Stock Search")
        search_query = st.text_input("Search stocks", placeholder="Enter ticker or company name...", key="search_input")
        if search_query:
            search_results = SearchEngine.search_stocks(filtered_df, search_query)
            if not search_results.empty:
                st.success(f"Found {len(search_results)} matching stock(s)")
                for _, stock in search_results.iterrows():
                    st.expander(f"ðŸ“Š {stock['ticker']} - {stock['company_name']}").write(stock)
            else: st.warning("No stocks found matching your search criteria.")

    with tabs[5]:
        st.markdown("### ðŸ“¥ Export Data")
        export_template = st.radio("Choose export template:", options=["Full Analysis (All Data)", "Day Trader Focus", "Swing Trader Focus", "Investor Focus"], key="export_template_radio")
        template_map = {"Full Analysis (All Data)": "full", "Day Trader Focus": "day_trader", "Swing Trader Focus": "swing_trader", "Investor Focus": "investor"}
        selected_template = template_map[export_template]
        if st.button("Generate Excel Report"):
            excel_file = ExportEngine.create_excel_report(filtered_df, template=selected_template)
            st.download_button(label="Download Excel Report", data=excel_file, file_name=f"wave_detection_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    
    with tabs[6]:
        st.markdown("### â„¹ï¸ About Wave Detection Ultimate 3.2 - Final Ultimate Production Version")
        st.markdown("... [About content] ...")
        st.markdown("#### ðŸ“Š Current Session Statistics")
        stats_cols = st.columns(4)
        with stats_cols[0]: UIComponents.render_metric_card("Total Stocks Loaded", f"{len(ranked_df):,}")
        with stats_cols[1]: UIComponents.render_metric_card("Currently Filtered", f"{len(filtered_df):,}")
        with stats_cols[2]:
            data_quality = RobustSessionState.safe_get('data_quality', {}).get('completeness', 0)
            UIComponents.render_metric_card("Data Quality", f"{'ðŸŸ¢' if data_quality > 80 else 'ðŸŸ¡' if data_quality > 60 else 'ðŸ”´'} {data_quality:.1f}%")
        with stats_cols[3]:
            last_refresh = RobustSessionState.safe_get('last_refresh', datetime.now(timezone.utc))
            minutes = int((datetime.now(timezone.utc) - last_refresh).total_seconds() / 60)
            UIComponents.render_metric_card("Cache Age", f"{'ðŸŸ¢' if minutes < 60 else 'ðŸ”´'} {minutes} min", "Fresh" if minutes < 60 else "Stale")

    st.markdown("---")
    st.markdown("""<div style="text-align: center; color: #666; padding: 1rem;">ðŸŒŠ Wave Detection Ultimate 3.2 - Final Ultimate Production Version<br><small>Professional Stock Ranking System â€¢ All Features Complete â€¢ Performance Maximized â€¢ Permanently Locked</small></div>""", unsafe_allow_html=True)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"Critical Application Error: {str(e)}")
        logger.error(f"Application crashed: {str(e)}", exc_info=True)
        if st.button("ðŸ”„ Restart Application"): st.cache_data.clear(); st.rerun()
        if st.button("ðŸ“§ Report Issue"): st.info("Please take a screenshot and report this error.")
