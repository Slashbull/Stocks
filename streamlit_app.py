"""
Wave Detection Ultimate 3.0 - FINAL ENHANCED PRODUCTION VERSION
===============================================================
Professional Stock Ranking System with Advanced Analytics
All bugs fixed, optimized for Streamlit Community Cloud
Enhanced with all valuable features from previous versions

Version: 3.0.7-FINAL-COMPLETE
Last Updated: July 2025
Status: PRODUCTION READY - Feature Complete
"""

# ============================================
# IMPORTS AND SETUP
# ============================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timezone
import logging
from typing import Dict, List, Tuple, Optional, Any, Union, Set
from dataclasses import dataclass, field
from functools import lru_cache, wraps
import time
from io import BytesIO
import warnings
import gc
import hashlib # For cache invalidation
import requests # For robust data loading
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Suppress warnings for clean output
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# ============================================
# LOGGING CONFIGURATION
# ============================================

# Production logging with proper formatting
log_level = logging.INFO

logging.basicConfig(
    level=log_level,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================
# CONFIGURATION AND CONSTANTS
# ============================================

@dataclass(frozen=True)
class Config:
    """System configuration with validated weights and thresholds"""
    
    # Data source - GID retained, base URL is dynamic
    DEFAULT_GID: str = "1823439984" 
    
    # Cache settings optimized for Streamlit Community Cloud
    CACHE_TTL: int = 3600  # 1 hour (daily invalidation overrides this effectively)
    STALE_DATA_HOURS: int = 24 # Not directly used for cache invalidation key, but as a conceptual limit
    
    # Master Score 3.0 weights (total = 100%) - DO NOT MODIFY
    POSITION_WEIGHT: float = 0.30
    VOLUME_WEIGHT: float = 0.25
    MOMENTUM_WEIGHT: float = 0.15
    ACCELERATION_WEIGHT: float = 0.10
    BREAKOUT_WEIGHT: float = 0.10
    RVOL_WEIGHT: float = 0.10
    
    # Display settings
    DEFAULT_TOP_N: int = 50
    AVAILABLE_TOP_N: List[int] = field(default_factory=lambda: [10, 20, 50, 100, 200, 500])
    
    # Critical columns (app fails without these)
    CRITICAL_COLUMNS: List[str] = field(default_factory=lambda: ['ticker', 'price', 'volume_1d'])
    
    # Important columns (degraded experience without)
    IMPORTANT_COLUMNS: List[str] = field(default_factory=lambda: [
        'ret_30d', 'from_low_pct', 'from_high_pct',
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d'
    ])
    
    # All percentage columns for consistent handling
    PERCENTAGE_COLUMNS: List[str] = field(default_factory=lambda: [
        'from_low_pct', 'from_high_pct',
        'ret_1d', 'ret_3d', 'ret_7d', 'ret_30d', 
        'ret_3m', 'ret_6m', 'ret_1y', 'ret_3y', 'ret_5y',
        'eps_change_pct'
    ])
    
    # Volume ratio columns
    VOLUME_RATIO_COLUMNS: List[str] = field(default_factory=lambda: [
        'vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d',
        'vol_ratio_1d_180d', 'vol_ratio_7d_180d', 'vol_ratio_30d_180d',
        'vol_ratio_90d_180d'
    ])
    
    # Pattern thresholds
    PATTERN_THRESHOLDS: Dict[str, float] = field(default_factory=lambda: {
        "category_leader": 90,
        "hidden_gem": 80,
        "acceleration": 85,
        "institutional": 75,
        "vol_explosion": 95,
        "breakout_ready": 80,
        "market_leader": 95,
        "momentum_wave": 75,
        "liquid_leader": 80,
        "long_strength": 80,
        "52w_high_approach": 90,
        "52w_low_bounce": 85,
        "golden_zone": 85,
        "vol_accumulation": 80,
        "momentum_diverge": 90,
        "range_compress": 75,
        "stealth": 70,
        "vampire": 85,
        "perfect_storm": 80
    })
    
    # Value bounds for data validation
    VALUE_BOUNDS: Dict[str, Tuple[float, float]] = field(default_factory=lambda: {
        'price': (0.01, 1_000_000),
        'rvol': (0.001, 1_000_000.0),  # Allow extreme RVOL values, but >0
        'pe': (-10000, 10000),
        'returns': (-99.99, 9999.99),  # Percentage bounds
        'volume': (0, 1e12)
    })
    
    # Performance thresholds (for logging warnings)
    PERFORMANCE_TARGETS: Dict[str, float] = field(default_factory=lambda: {
        'data_processing': 2.0,  # seconds
        'filtering': 0.2,
        'pattern_detection': 0.5,
        'export_generation': 1.0,
        'search': 0.05
    })
    
    # Market categories (Indian market specific)
    MARKET_CATEGORIES: List[str] = field(default_factory=lambda: [
        'Mega Cap', 'Large Cap', 'Mid Cap', 'Small Cap', 'Micro Cap'
    ])
    
    # Tier definitions with proper boundaries
    TIERS: Dict[str, Dict[str, Tuple[float, float]]] = field(default_factory=lambda: {
        "eps": {
            "Loss": (-float('inf'), 0),
            "0-5": (0, 5),
            "5-10": (5, 10),
            "10-20": (10, 20),
            "20-50": (20, 50),
            "50-100": (50, 100),
            "100+": (100, float('inf'))
        },
        "pe": {
            "Negative/NA": (-float('inf'), 0),
            "0-10": (0, 10),
            "10-15": (10, 15),
            "15-20": (15, 20),
            "20-30": (20, 30),
            "30-50": (30, 50),
            "50+": (50, float('inf'))
        },
        "price": {
            "0-100": (0, 100),
            "100-250": (100, 250),
            "250-500": (250, 500),
            "500-1000": (500, 1000),
            "1000-2500": (1000, 2500),
            "2500-5000": (2500, 5000),
            "5000+": (5000, float('inf'))
        }
    })

# Global configuration instance
CONFIG = Config()

# ============================================
# PERFORMANCE MONITORING
# ============================================

class PerformanceMonitor:
    """Track and report performance metrics"""
    
    @staticmethod
    def timer(target_time: Optional[float] = None):
        """Performance timing decorator with target comparison"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                try:
                    result = func(*args, **kwargs)
                    elapsed = time.perf_counter() - start
                    
                    # Log if exceeds target
                    if target_time and elapsed > target_time:
                        logger.warning(f"{func.__name__} took {elapsed:.2f}s (target: {target_time}s)")
                    elif elapsed > 1.0:
                        logger.info(f"{func.__name__} completed in {elapsed:.2f}s")
                    
                    # Store timing
                    if 'performance_metrics' not in st.session_state:
                        st.session_state.performance_metrics = {}
                    st.session_state.performance_metrics[func.__name__] = elapsed
                    
                    return result
                except Exception as e:
                    elapsed = time.perf_counter() - start
                    logger.error(f"{func.__name__} failed after {elapsed:.2f}s: {str(e)}")
                    raise
            return wrapper
        return decorator

# ============================================
# DATA VALIDATION AND SANITIZATION
# ============================================

class DataValidator:
    """Comprehensive data validation and sanitization"""
    
    # Store clipping statistics
    _clipping_counts: Dict[str, int] = {}

    @staticmethod
    def get_clipping_counts() -> Dict[str, int]:
        """Returns the current clipping counts and resets them."""
        counts = DataValidator._clipping_counts.copy()
        DataValidator._clipping_counts.clear() # Reset after retrieval for the session
        return counts

    @staticmethod
    def validate_dataframe(df: pd.DataFrame, required_cols: List[str], context: str) -> Tuple[bool, str]:
        """Validate dataframe structure and data quality"""
        if df is None:
            return False, f"{context}: DataFrame is None"
        
        if df.empty:
            return False, f"{context}: DataFrame is empty"
        
        # Check critical columns
        missing_critical = [col for col in CONFIG.CRITICAL_COLUMNS if col not in df.columns]
        if missing_critical:
            return False, f"{context}: Missing critical columns: {missing_critical}"
        
        # Check for duplicate tickers
        duplicates = df['ticker'].duplicated().sum()
        if duplicates > 0:
            logger.warning(f"{context}: Found {duplicates} duplicate tickers")
        
        # Calculate data quality metrics
        total_cells = len(df) * len(df.columns)
        filled_cells = df.notna().sum().sum()
        completeness = (filled_cells / total_cells * 100) if total_cells > 0 else 0
        
        if completeness < 50:
            logger.warning(f"{context}: Low data completeness ({completeness:.1f}%)")
        
        # Store quality metrics
        if 'data_quality' not in st.session_state:
            st.session_state.data_quality = {}
        
        st.session_state.data_quality.update({
            'completeness': completeness,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'duplicate_tickers': duplicates,
            'context': context,
            'timestamp': datetime.now(timezone.utc)
        })
        
        logger.info(f"{context}: Validated {len(df)} rows, {len(df.columns)} columns, {completeness:.1f}% complete")
        return True, "Valid"
    
    @staticmethod
    def clean_numeric_value(value: Any, col_name: str, is_percentage: bool = False, bounds: Optional[Tuple[float, float]] = None) -> Optional[float]:
        """Clean and convert numeric values with bounds checking and clipping notification"""
        if pd.isna(value) or value == '' or value is None:
            return np.nan
        
        try:
            cleaned = str(value).strip()
            
            if cleaned.upper() in ['', '-', 'N/A', 'NA', 'NAN', 'NONE', '#VALUE!', '#ERROR!', '#DIV/0!', 'INF', '-INF']:
                return np.nan
            
            cleaned = cleaned.replace('â‚¹', '').replace('$', '').replace(',', '').replace(' ', '').replace('%', '')
            result = float(cleaned)
            
            # Apply bounds if specified with logging for clipping
            if bounds:
                min_val, max_val = bounds
                original_result = result
                
                if result < min_val:
                    result = min_val
                    logger.warning(f"Value clipped for column '{col_name}': Original {original_result:.2f} clipped to min {min_val:.2f}.")
                    DataValidator._clipping_counts[col_name] = DataValidator._clipping_counts.get(col_name, 0) + 1
                elif result > max_val:
                    result = max_val
                    logger.warning(f"Value clipped for column '{col_name}': Original {original_result:.2f} clipped to max {max_val:.2f}.")
                    DataValidator._clipping_counts[col_name] = DataValidator._clipping_counts.get(col_name, 0) + 1
            
            if np.isnan(result) or np.isinf(result):
                return np.nan
            
            return result
            
        except (ValueError, TypeError, AttributeError):
            return np.nan
    
    @staticmethod
    def sanitize_string(value: Any, default: str = "Unknown") -> str:
        """Sanitize string values"""
        if pd.isna(value) or value is None:
            return default
        
        cleaned = str(value).strip()
        if cleaned.upper() in ['', 'N/A', 'NA', 'NAN', 'NONE', 'NULL', '-']:
            return default
        
        cleaned = ' '.join(cleaned.split())
        
        return cleaned

# ============================================
# SMART CACHING WITH VERSIONING
# ============================================

def get_requests_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504, 429), # Add 429 for rate limiting
    session=None,
) -> requests.Session:
    """Configures a requests session with retry logic for robust HTTP requests."""
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

@st.cache_data(ttl=CONFIG.CACHE_TTL, persist="disk", show_spinner=False)
def load_and_process_data(source_type: str = "sheet", file_data=None, 
                         data_version: str = "1.0") -> Tuple[pd.DataFrame, datetime, Dict[str, Any]]:
    """
    Load and process data with smart caching and versioning.
    Derives Spreadsheet ID directly from session state.
    """
    
    start_time = time.perf_counter()
    metadata = {
        'source_type': source_type,
        'data_version': data_version, # This data_version will contain the hash for cache invalidation
        'processing_start': datetime.now(timezone.utc),
        'errors': [],
        'warnings': []
    }
    
    try:
        # Load data based on source
        if source_type == "upload" and file_data is not None:
            logger.info("Loading data from uploaded CSV")
            df = pd.read_csv(file_data, low_memory=False)
            metadata['source'] = "User Upload"
        else:
            # Dynamic Spreadsheet ID Determination and strict validation
            user_provided_id = st.session_state.get('user_spreadsheet_id')
            
            if user_provided_id is None or not (len(user_provided_id) == 44 and user_provided_id.isalnum()):
                # This should ideally be caught by UI validation before calling load_and_process_data
                # But as a failsafe, ensure data loading is prevented.
                error_msg = "A valid 44-character alphanumeric Google Spreadsheet ID is required to load data."
                logger.critical(error_msg)
                raise ValueError(error_msg)
            
            final_spreadsheet_id_to_use = user_provided_id
            logger.info(f"Using user-provided Spreadsheet ID: {final_spreadsheet_id_to_use}")
            
            # Construct CSV export URL using the base URL and GID
            # Base URL is https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}
            base_url = f"https://docs.google.com/spreadsheets/d/{final_spreadsheet_id_to_use}"
            csv_url = f"{base_url}/export?format=csv&gid={CONFIG.DEFAULT_GID}"
            
            logger.info(f"Attempting to load data from Google Sheets with Spreadsheet ID: {final_spreadsheet_id_to_use}, GID: {CONFIG.DEFAULT_GID}")
            
            try:
                # Use requests with retry for robust fetching
                session = get_requests_retry_session()
                response = session.get(csv_url)
                response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
                
                df = pd.read_csv(BytesIO(response.content), low_memory=False)
                metadata['source'] = f"Google Sheets (ID: {final_spreadsheet_id_to_use}, GID: {CONFIG.DEFAULT_GID})"
            except requests.exceptions.RequestException as req_e:
                error_msg = f"Network or HTTP error loading Google Sheet (ID: {final_spreadsheet_id_to_use}): {req_e}"
                logger.error(error_msg)
                metadata['errors'].append(error_msg)
                
                # Try to use cached data as fallback
                if 'last_good_data' in st.session_state:
                    logger.info("Using cached data as fallback due to network/HTTP error.")
                    df, timestamp, old_metadata = st.session_state.last_good_data
                    metadata['warnings'].append("Using cached data due to load failure")
                    metadata['cache_used'] = True
                    return df, timestamp, metadata
                raise ValueError(error_msg) from req_e
            except Exception as e:
                error_msg = f"Failed to load CSV from Google Sheet (ID: {final_spreadsheet_id_to_use}): {str(e)}"
                logger.error(error_msg)
                metadata['errors'].append(error_msg)
                
                # Try to use cached data as fallback
                if 'last_good_data' in st.session_state:
                    logger.info("Using cached data as fallback due to CSV parsing error.")
                    df, timestamp, old_metadata = st.session_state.last_good_data
                    metadata['warnings'].append("Using cached data due to load failure")
                    metadata['cache_used'] = True
                    return df, timestamp, metadata
                raise ValueError(error_msg) from e
        
        # Validate loaded data
        is_valid, validation_msg = DataValidator.validate_dataframe(df, CONFIG.CRITICAL_COLUMNS, "Initial load")
        if not is_valid:
            raise ValueError(validation_msg)
        
        # Process the data
        df = DataProcessor.process_dataframe(df, metadata)
        
        # Calculate all scores and rankings
        df = RankingEngine.calculate_all_scores(df)
        
        # Detect patterns
        df = PatternDetector.detect_all_patterns(df)
        
        # Add advanced metrics
        df = AdvancedMetrics.calculate_all_metrics(df)
        
        # Final validation
        is_valid, validation_msg = DataValidator.validate_dataframe(df, ['master_score', 'rank'], "Final processed")
        if not is_valid:
            raise ValueError(validation_msg)
        
        # Store as last good data
        timestamp = datetime.now(timezone.utc)
        st.session_state.last_good_data = (df.copy(), timestamp, metadata)
        
        # Record processing time
        processing_time = time.perf_counter() - start_time
        metadata['processing_time'] = processing_time
        metadata['processing_end'] = datetime.now(timezone.utc)
        
        logger.info(f"Data processing complete: {len(df)} stocks in {processing_time:.2f}s")
        
        # Get and report clipping counts
        clipping_info = DataValidator.get_clipping_counts()
        if clipping_info:
            logger.warning(f"Data clipping occurred: {clipping_info}")
            metadata['warnings'].append(f"Some numeric values were clipped: {clipping_info}. See logs for details.")

        # Clean up memory
        gc.collect()
        
        return df, timestamp, metadata
        
    except Exception as e:
        logger.error(f"Failed to load and process data: {str(e)}")
        metadata['errors'].append(str(e))
        raise

# ============================================
# DATA PROCESSING ENGINE
# ============================================

class DataProcessor:
    """Handle all data processing with validation and optimization"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def process_dataframe(df: pd.DataFrame, metadata: Dict[str, Any]) -> pd.DataFrame:
        """Complete data processing pipeline"""
        
        df = df.copy()
        initial_count = len(df)
        
        # Process numeric columns with vectorization
        numeric_cols_to_process = [col for col in df.columns if col not in ['ticker', 'company_name', 'category', 'sector', 'industry', 'year', 'market_cap']]
        
        for col in numeric_cols_to_process:
            if col in df.columns:
                is_pct = col in CONFIG.PERCENTAGE_COLUMNS
                
                # Determine bounds for specific columns
                bounds = None
                if 'volume' in col.lower():
                    bounds = CONFIG.VALUE_BOUNDS['volume']
                elif col == 'rvol':
                    bounds = CONFIG.VALUE_BOUNDs['rvol']
                elif col == 'pe':
                    bounds = CONFIG.VALUE_BOUNDS['pe']
                elif is_pct:
                    bounds = CONFIG.VALUE_BOUNDS['returns']
                elif col == 'price':
                    bounds = CONFIG.VALUE_BOUNDS['price']
                
                # Apply vectorized cleaning
                df[col] = df[col].apply(lambda x: DataValidator.clean_numeric_value(x, col, is_pct, bounds))
        
        # Process categorical columns
        string_cols = ['ticker', 'company_name', 'category', 'sector', 'industry']
        for col in string_cols:
            if col in df.columns:
                df[col] = df[col].apply(DataValidator.sanitize_string)
        
        # Fix volume ratios (vectorized)
        for col in CONFIG.VOLUME_RATIO_COLUMNS:
            if col in df.columns:
                df[col] = df[col].astype(float) # Ensure float type
                # Convert percentage change to ratio: (100 + change%) / 100
                df[col] = (100 + df[col]) / 100
                df[col] = df[col].clip(0.01, 1000.0)  # Reasonable bounds, allow high
                df[col] = df[col].fillna(1.0) # Fill with 1.0 (no change) if NaN
        
        # Validate critical data
        df = df.dropna(subset=['ticker', 'price'], how='any')
        df = df[df['price'] > 0.01]  # Minimum valid price
        
        # Remove duplicates (keep first)
        before_dedup = len(df)
        df = df.drop_duplicates(subset=['ticker'], keep='first')
        if before_dedup > len(df):
            metadata['warnings'].append(f"Removed {before_dedup - len(df)} duplicate tickers")
        
        # Fill missing values with NaN where appropriate, do not use arbitrary defaults
        # Ranking functions and other calculations will handle NaN explicitly.
        df = DataProcessor._add_tier_classifications(df)
        
        removed = initial_count - len(df)
        if removed > 0:
            metadata['warnings'].append(f"Removed {removed} invalid rows during processing")
        
        logger.info(f"Processed {len(df)} valid stocks from {initial_count} initial rows")
        
        return df
    
    @staticmethod
    def _add_tier_classifications(df: pd.DataFrame) -> pd.DataFrame:
        """Add tier classifications with proper boundary handling"""
        
        def classify_tier(value: float, tier_dict: Dict[str, Tuple[float, float]]) -> str:
            """Classify value into tier with fixed boundary logic"""
            if pd.isna(value):
                return "Unknown"
            
            for tier_name, (min_val, max_val) in tier_dict.items():
                if min_val < value <= max_val: # Standard interval (exclusive lower, inclusive upper)
                    return tier_name
                # Special handling for first tier to include lower bound if it's -inf or 0
                if tier_name == list(tier_dict.keys())[0] and (min_val == -float('inf') or min_val == 0) and value == min_val:
                    return tier_name
            return "Unknown"
        
        # Add tier columns
        if 'eps_current' in df.columns:
            df['eps_tier'] = df['eps_current'].apply(
                lambda x: classify_tier(x, CONFIG.TIERS['eps'])
            )
        
        if 'pe' in df.columns:
            df['pe_tier'] = df['pe'].apply(
                lambda x: "Negative/NA" if pd.isna(x) or x <= 0 
                else classify_tier(x, CONFIG.TIERS['pe'])
            )
        
        if 'price' in df.columns:
            df['price_tier'] = df['price'].apply(
                lambda x: classify_tier(x, CONFIG.TIERS['price'])
            )
        
        return df

# ============================================
# ADVANCED METRICS CALCULATOR
# ============================================

class AdvancedMetrics:
    """Calculate advanced metrics and indicators"""
    
    @staticmethod
    def calculate_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all advanced metrics"""
        
        # Money Flow (in millions)
        if all(col in df.columns for col in ['price', 'volume_1d', 'rvol']):
            # Ensure numeric, fill NA before calculation, then div by 1M
            df['money_flow'] = df['price'].fillna(0) * df['volume_1d'].fillna(0) * df['rvol'].fillna(1.0)
            df['money_flow_mm'] = df['money_flow'] / 1_000_000
        else:
            df['money_flow_mm'] = np.nan # Use NaN if cols are missing
        
        # Volume Momentum Index (VMI)
        if all(col in df.columns for col in ['vol_ratio_1d_90d', 'vol_ratio_7d_90d', 'vol_ratio_30d_90d', 'vol_ratio_90d_180d']):
            # Fill NaN with 1.0 (no change) before calculation for ratios
            df['vmi'] = (
                df['vol_ratio_1d_90d'].fillna(1.0) * 4 +
                df['vol_ratio_7d_90d'].fillna(1.0) * 3 +
                df['vol_ratio_30d_90d'].fillna(1.0) * 2 +
                df['vol_ratio_90d_180d'].fillna(1.0) * 1
            ) / 10
        else:
            df['vmi'] = np.nan # Use NaN if cols are missing
        
        # Position Tension
        if all(col in df.columns for col in ['from_low_pct', 'from_high_pct']):
            # Fill NaN with 50 and -50 for these specific calculations, if not already filled earlier
            df['position_tension'] = df['from_low_pct'].fillna(50) + abs(df['from_high_pct'].fillna(-50))
        else:
            df['position_tension'] = np.nan # Use NaN if cols are missing
        
        # Momentum Harmony (0-4)
        df['momentum_harmony'] = 0
        
        if 'ret_1d' in df.columns:
            df['momentum_harmony'] += (df['ret_1d'].fillna(0) > 0).astype(int)
        
        if all(col in df.columns for col in ['ret_7d', 'ret_30d']):
            with np.errstate(divide='ignore', invalid='ignore'):
                # Use np.nan for division by zero, then fill with 0 for comparison if needed
                daily_ret_7d = np.where(df['ret_7d'].fillna(0) != 0, df['ret_7d'].fillna(0) / 7, np.nan)
                daily_ret_30d = np.where(df['ret_30d'].fillna(0) != 0, df['ret_30d'].fillna(0) / 30, np.nan)
            
            # Compare non-NaN values
            df['momentum_harmony'] += ((daily_ret_7d.fillna(-np.inf) > daily_ret_30d.fillna(-np.inf))).astype(int)
        
        if all(col in df.columns for col in ['ret_30d', 'ret_3m']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_ret_30d_comp = np.where(df['ret_30d'].fillna(0) != 0, df['ret_30d'].fillna(0) / 30, np.nan)
                daily_ret_3m_comp = np.where(df['ret_3m'].fillna(0) != 0, df['ret_3m'].fillna(0) / 90, np.nan)
            df['momentum_harmony'] += ((daily_ret_30d_comp.fillna(-np.inf) > daily_ret_3m_comp.fillna(-np.inf))).astype(int)
        
        if 'ret_3m' in df.columns:
            df['momentum_harmony'] += (df['ret_3m'].fillna(0) > 0).astype(int)
        
        # Wave State
        df['wave_state'] = df.apply(AdvancedMetrics._get_wave_state, axis=1)

        # Overall Wave Strength (for filtering)
        # Fill NA with neutral score (50) for this calculation
        score_cols = ['momentum_score', 'acceleration_score', 'rvol_score', 'breakout_score']
        if all(col in df.columns for col in score_cols):
            df['overall_wave_strength'] = (
                df['momentum_score'].fillna(50) * 0.3 +
                df['acceleration_score'].fillna(50) * 0.3 +
                df['rvol_score'].fillna(50) * 0.2 +
                df['breakout_score'].fillna(50) * 0.2
            )
        else:
            df['overall_wave_strength'] = np.nan # Use NaN if scores are missing
        
        return df
    
    @staticmethod
    def _get_wave_state(row: pd.Series) -> str:
        """Determine wave state for a stock"""
        signals = 0
        
        # Use .get with default 0 or 50 to handle potential missing columns gracefully
        if row.get('momentum_score', 0) > 70:
            signals += 1
        if row.get('volume_score', 0) > 70:
            signals += 1
        if row.get('acceleration_score', 0) > 70:
            signals += 1
        if row.get('rvol', 0) > 2:
            signals += 1
        
        if signals >= 4:
            return "ðŸŒŠðŸŒŠðŸŒŠ CRESTING"
        elif signals >= 3:
            return "ðŸŒŠðŸŒŠ BUILDING"
        elif signals >= 1:
            return "ðŸŒŠ FORMING"
        else:
            return "ðŸ’¥ BREAKING"

# ============================================
# RANKING ENGINE - OPTIMIZED
# ============================================

class RankingEngine:
    """Core ranking calculations - optimized with numpy"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def calculate_all_scores(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate all component scores and master score"""
        
        if df.empty:
            return df
        
        logger.info("Starting optimized ranking calculations...")
        
        # Calculate component scores, filling NaN with a neutral 50 for scoring purposes
        # This handles cases where underlying data for a score component might be missing
        df['position_score'] = RankingEngine._calculate_position_score(df).fillna(50)
        df['volume_score'] = RankingEngine._calculate_volume_score(df).fillna(50)
        df['momentum_score'] = RankingEngine._calculate_momentum_score(df).fillna(50)
        df['acceleration_score'] = RankingEngine._calculate_acceleration_score(df).fillna(50)
        df['breakout_score'] = RankingEngine._calculate_breakout_score(df).fillna(50)
        df['rvol_score'] = RankingEngine._calculate_rvol_score(df).fillna(50)
        
        # Calculate auxiliary scores, also filling NaN with neutral 50
        df['trend_quality'] = RankingEngine._calculate_trend_quality(df).fillna(50)
        df['long_term_strength'] = RankingEngine._calculate_long_term_strength(df).fillna(50)
        df['liquidity_score'] = RankingEngine._calculate_liquidity_score(df).fillna(50)
        
        # Calculate master score using numpy (DO NOT MODIFY FORMULA)
        scores_matrix = np.column_stack([
            df['position_score'],
            df['volume_score'],
            df['momentum_score'],
            df['acceleration_score'],
            df['breakout_score'],
            df['rvol_score']
        ])
        
        weights = np.array([
            CONFIG.POSITION_WEIGHT,
            CONFIG.VOLUME_WEIGHT,
            CONFIG.MOMENTUM_WEIGHT,
            CONFIG.ACCELERATION_WEIGHT,
            CONFIG.BREAKOUT_WEIGHT,
            CONFIG.RVOL_WEIGHT
        ])
        
        df['master_score'] = np.dot(scores_matrix, weights).clip(0, 100)
        
        # Calculate ranks
        # na_option='bottom' ensures NaNs are ranked last.
        df['rank'] = df['master_score'].rank(method='first', ascending=False, na_option='bottom')
        df['rank'] = df['rank'].fillna(len(df) + 1).astype(int)
        
        df['percentile'] = df['master_score'].rank(pct=True, ascending=True, na_option='bottom') * 100
        df['percentile'] = df['percentile'].fillna(0) # Percentile of 0 for NaNs
        
        # Calculate category-specific ranks
        df = RankingEngine._calculate_category_ranks(df)
        
        logger.info(f"Ranking complete: {len(df)} stocks processed")
        
        return df
    
    @staticmethod
    def _safe_rank(series: pd.Series, pct: bool = True, ascending: bool = True) -> pd.Series:
        """Safely rank a series with proper edge case handling for NaNs and Infs."""
        if series is None or series.empty:
            return pd.Series(np.nan, dtype=float) # Return NaN for empty/None series
        
        # Replace inf values with NaN for robust ranking
        series = series.replace([np.inf, -np.inf], np.nan)
        
        # Count valid values for fallback
        valid_count = series.notna().sum()
        if valid_count == 0:
            return pd.Series(np.nan, index=series.index) # All NaNs, return NaN series
        
        # Rank with proper parameters
        # na_option='bottom' (or 'top' for ascending=False) ensures NaNs are pushed to the end.
        if pct:
            ranks = series.rank(pct=True, ascending=ascending, na_option='bottom') * 100
        else:
            ranks = series.rank(ascending=ascending, method='min', na_option='bottom')
        
        return ranks
    
    @staticmethod
    def _calculate_position_score(df: pd.DataFrame) -> pd.Series:
        """Calculate position score from 52-week range (DO NOT MODIFY LOGIC)"""
        position_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        has_from_low = 'from_low_pct' in df.columns and df['from_low_pct'].notna().any()
        has_from_high = 'from_high_pct' in df.columns and df['from_high_pct'].notna().any()
        
        if not has_from_low and not has_from_high:
            logger.warning("No position data available, position scores will be NaN.")
            return position_score
        
        from_low = df['from_low_pct'] if has_from_low else pd.Series(np.nan, index=df.index)
        from_high = df['from_high_pct'] if has_from_high else pd.Series(np.nan, index=df.index)
        
        # Only rank if there are non-NaN values
        rank_from_low = RankingEngine._safe_rank(from_low, pct=True, ascending=True) if has_from_low else pd.Series(np.nan, index=df.index)
        rank_from_high = RankingEngine._safe_rank(from_high, pct=True, ascending=False) if has_from_high else pd.Series(np.nan, index=df.index)
        
        # Combine using .fillna(50) for calculation to provide a neutral score if a component is NaN
        position_score = (rank_from_low.fillna(50) * 0.6 + rank_from_high.fillna(50) * 0.4)
        
        # If both components were NaN, the result will be 50. Ensure we preserve NaN if no data was ever present.
        if not (has_from_low or has_from_high):
            position_score = pd.Series(np.nan, index=df.index)

        return position_score.clip(0, 100)
    
    @staticmethod
    def _calculate_volume_score(df: pd.DataFrame) -> pd.Series:
        """Calculate comprehensive volume score"""
        volume_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        vol_cols = [
            ('vol_ratio_1d_90d', 0.20),
            ('vol_ratio_7d_90d', 0.20),
            ('vol_ratio_30d_90d', 0.20),
            ('vol_ratio_30d_180d', 0.15),
            ('vol_ratio_90d_180d', 0.25)
        ]
        
        total_weight = 0
        weighted_score = pd.Series(0.0, index=df.index, dtype=float) # Start with 0.0
        
        for col, weight in vol_cols:
            if col in df.columns and df[col].notna().any():
                col_rank = RankingEngine._safe_rank(df[col], pct=True, ascending=True)
                # Only add to weighted_score if col_rank is not NaN
                weighted_score += col_rank.fillna(0) * weight # Treat NaN ranks as 0 for sum
                total_weight += weight
        
        if total_weight > 0:
            volume_score = weighted_score / total_weight
            # Propagate NaN if any of the components that contributed were NaN for a specific row
            nan_mask = df[[col for col, _ in vol_cols if col in df.columns]].isna().all(axis=1)
            volume_score[nan_mask] = np.nan
        else:
            logger.warning("No valid volume ratio data available, volume scores will be NaN.")
        
        return volume_score.clip(0, 100)
    
    @staticmethod
    def _calculate_momentum_score(df: pd.DataFrame) -> pd.Series:
        """Calculate momentum score based on returns, propagating NaN properly."""
        momentum_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        has_ret_30d = 'ret_30d' in df.columns and df['ret_30d'].notna().any()
        has_ret_7d = 'ret_7d' in df.columns and df['ret_7d'].notna().any()
        
        if not has_ret_30d and not has_ret_7d:
            logger.warning("No return data available for momentum calculation, scores will be NaN.")
            return momentum_score
        
        ret_30d = df['ret_30d'] if has_ret_30d else pd.Series(np.nan, index=df.index)
        ret_7d = df['ret_7d'] if has_ret_7d else pd.Series(np.nan, index=df.index)
        
        # Primary: 30-day returns if available, else 7-day, otherwise NaN
        if has_ret_30d:
            momentum_score = RankingEngine._safe_rank(ret_30d, pct=True, ascending=True)
        elif has_ret_7d: # Fallback to 7-day only if 30-day is not available
            momentum_score = RankingEngine._safe_rank(ret_7d, pct=True, ascending=True)
            logger.info("Using 7-day returns for momentum score due to missing 30-day data.")
        
        # Add consistency bonus only if both 7-day and 30-day data are available for comparison
        if has_ret_7d and has_ret_30d:
            consistency_bonus = pd.Series(0, index=df.index, dtype=float)
            
            # Both positive for non-NaN returns
            all_positive = (ret_7d.fillna(-1) > 0) & (ret_30d.fillna(-1) > 0)
            consistency_bonus[all_positive] = 5
            
            # Accelerating returns: use NaN for division by zero to propagate missingness
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_ret_7d = np.where(ret_7d.fillna(0) != 0, ret_7d.fillna(0) / 7, np.nan)
                daily_ret_30d = np.where(ret_30d.fillna(0) != 0, ret_30d.fillna(0) / 30, np.nan)
            
            # Compare only where both daily rates are not NaN
            accelerating_mask = all_positive & (daily_ret_7d.fillna(-np.inf) > daily_ret_30d.fillna(-np.inf))
            consistency_bonus[accelerating_mask] = 10
            
            # Apply bonus, but preserve NaN from initial momentum_score if it was NaN
            momentum_score = momentum_score.mask(momentum_score.isna(), other=np.nan) # Ensure NaNs are preserved
            momentum_score = (momentum_score.fillna(50) + consistency_bonus).clip(0, 100) # Fill for sum, then clip
        
        return momentum_score
    
    @staticmethod
    def _calculate_acceleration_score(df: pd.DataFrame) -> pd.Series:
        """Calculate if momentum is accelerating, handling NaNs from division properly."""
        acceleration_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        req_cols = ['ret_1d', 'ret_7d', 'ret_30d']
        available_cols = [col for col in req_cols if col in df.columns and df[col].notna().any()]
        
        if len(available_cols) < 2:
            logger.warning("Insufficient return data for acceleration calculation, scores will be NaN.")
            return acceleration_score
        
        ret_1d = df['ret_1d'] if 'ret_1d' in df.columns else pd.Series(np.nan, index=df.index)
        ret_7d = df['ret_7d'] if 'ret_7d' in df.columns else pd.Series(np.nan, index=df.index)
        ret_30d = df['ret_30d'] if 'ret_30d' in df.columns else pd.Series(np.nan, index=df.index)
        
        with np.errstate(divide='ignore', invalid='ignore'):
            # Use np.nan for division by zero
            avg_daily_1d = ret_1d
            avg_daily_7d = np.where(ret_7d.fillna(0) != 0, ret_7d.fillna(0) / 7, np.nan)
            avg_daily_30d = np.where(ret_30d.fillna(0) != 0, ret_30d.fillna(0) / 30, np.nan)
        
        # Conditions for scoring, only apply if all relevant components are not NaN for that row
        has_all_data = ret_1d.notna() & avg_daily_7d.notna() & avg_daily_30d.notna()

        # Initialize scores for rows that have all data, others remain NaN
        acceleration_score.loc[has_all_data] = 50.0 # Start with neutral for valid rows

        if has_all_data.any():
            # Perfect acceleration
            perfect = has_all_data & (avg_daily_1d > avg_daily_7d) & (avg_daily_7d > avg_daily_30d) & (ret_1d > 0)
            acceleration_score.loc[perfect] = 100
            
            # Good acceleration
            good = has_all_data & (~perfect) & (avg_daily_1d > avg_daily_7d) & (ret_1d > 0)
            acceleration_score.loc[good] = 80
            
            # Moderate
            moderate = has_all_data & (~perfect) & (~good) & (ret_1d > 0)
            acceleration_score.loc[moderate] = 60
            
            # Deceleration
            slight_decel = has_all_data & (ret_1d <= 0) & (ret_7d > 0)
            acceleration_score.loc[slight_decel] = 40
            
            strong_decel = has_all_data & (ret_1d <= 0) & (ret_7d <= 0)
            acceleration_score.loc[strong_decel] = 20
        
        return acceleration_score.clip(0, 100)
    
    @staticmethod
    def _calculate_breakout_score(df: pd.DataFrame) -> pd.Series:
        """Calculate breakout probability, propagating NaN."""
        breakout_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        # Factor 1: Distance from high (40% weight)
        if 'from_high_pct' in df.columns and df['from_high_pct'].notna().any():
            distance_from_high = -df['from_high_pct'] # More negative is further from high
            # Normalize to 0-100 where 0% from high (distance 0) is 100 score, -100% (distance 100) is 0 score
            distance_factor = (100 - distance_from_high.fillna(100)).clip(0, 100) 
        else:
            distance_factor = pd.Series(np.nan, index=df.index)
        
        # Factor 2: Volume surge (40% weight)
        if 'vol_ratio_7d_90d' in df.columns and df['vol_ratio_7d_90d'].notna().any():
            vol_ratio = df['vol_ratio_7d_90d']
            volume_factor = ((vol_ratio.fillna(1.0) - 1) * 100).clip(0, 100) # 1.0 ratio means 0 score, 2.0 ratio means 100 score
        else:
            volume_factor = pd.Series(np.nan, index=df.index)
        
        # Factor 3: Trend support (20% weight)
        trend_factor = pd.Series(np.nan, index=df.index, dtype=float)
        if 'price' in df.columns:
            current_price = df['price']
            sma_cols = ['sma_20d', 'sma_50d', 'sma_200d']
            
            # Calculate sum of conditions where price is above SMA, for rows where SMA is available
            conditions_sum = pd.Series(0, index=df.index, dtype=float)
            valid_sma_count = pd.Series(0, index=df.index, dtype=int)

            for sma_col in sma_cols:
                if sma_col in df.columns and df[sma_col].notna().any():
                    # Only add if both price and SMA are not NaN
                    has_data = current_price.notna() & df[sma_col].notna()
                    conditions_sum.loc[has_data] += (current_price.loc[has_data] > df[sma_col].loc[has_data]).astype(float)
                    valid_sma_count.loc[has_data] += 1
            
            # Only calculate trend_factor for rows where at least one SMA was considered
            trend_factor.loc[valid_sma_count > 0] = (conditions_sum.loc[valid_sma_count > 0] / valid_sma_count.loc[valid_sma_count > 0]) * 100
        
        trend_factor = trend_factor.clip(0, 100)
        
        # Combine factors. Fill NaNs with 50 for the combination, then propagate NaNs for rows
        # where all three factors were NaN.
        combined_score = (
            distance_factor.fillna(50) * 0.4 +
            volume_factor.fillna(50) * 0.4 +
            trend_factor.fillna(50) * 0.2
        )
        
        # Ensure that if all input factors were NaN, the output is NaN
        all_nan_mask = distance_factor.isna() & volume_factor.isna() & trend_factor.isna()
        combined_score.loc[all_nan_mask] = np.nan

        return combined_score.clip(0, 100)
    
    @staticmethod
    def _calculate_rvol_score(df: pd.DataFrame) -> pd.Series:
        """Calculate RVOL-based score, propagating NaN."""
        if 'rvol' not in df.columns or df['rvol'].isna().all():
            return pd.Series(np.nan, index=df.index) # All NaN if no RVOL data
        
        rvol = df['rvol']
        rvol_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        # Only apply scoring to non-NaN RVOL values
        has_rvol = rvol.notna()
        
        rvol_score.loc[has_rvol & (rvol > 10)] = 95
        rvol_score.loc[has_rvol & (rvol > 5) & (rvol <= 10)] = 90
        rvol_score.loc[has_rvol & (rvol > 3) & (rvol <= 5)] = 85
        rvol_score.loc[has_rvol & (rvol > 2) & (rvol <= 3)] = 80
        rvol_score.loc[has_rvol & (rvol > 1.5) & (rvol <= 2)] = 70
        rvol_score.loc[has_rvol & (rvol > 1.2) & (rvol <= 1.5)] = 60
        rvol_score.loc[has_rvol & (rvol > 0.8) & (rvol <= 1.2)] = 50
        rvol_score.loc[has_rvol & (rvol > 0.5) & (rvol <= 0.8)] = 40
        rvol_score.loc[has_rvol & (rvol > 0.3) & (rvol <= 0.5)] = 30
        rvol_score.loc[has_rvol & (rvol <= 0.3)] = 20
        
        return rvol_score.clip(0, 100)
    
    @staticmethod
    def _calculate_trend_quality(df: pd.DataFrame) -> pd.Series:
        """Calculate trend quality score based on SMA alignment, propagating NaN."""
        trend_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        if 'price' not in df.columns or df['price'].isna().all():
            return trend_score
        
        current_price = df['price']
        sma_cols = ['sma_20d', 'sma_50d', 'sma_200d']
        
        # Initialize a count of SMAs that price is above for each row
        above_sma_count = pd.Series(0, index=df.index, dtype=int)
        
        # Track rows where at least one SMA comparison was possible
        rows_with_any_sma_data = pd.Series(False, index=df.index, dtype=bool)

        for sma_col in sma_cols:
            if sma_col in df.columns and df[sma_col].notna().any():
                # Only consider rows where both current_price and SMA are not NaN
                valid_comparison_mask = current_price.notna() & df[sma_col].notna()
                
                above_sma_count.loc[valid_comparison_mask] += (current_price.loc[valid_comparison_mask] > df[sma_col].loc[valid_comparison_mask]).astype(int)
                rows_with_any_sma_data.loc[valid_comparison_mask] = True
        
        # Only assign scores to rows where at least one SMA comparison was made
        rows_to_score = df.index[rows_with_any_sma_data]
        
        if len(rows_to_score) > 0:
            # Initialize scores for these rows to neutral if no specific condition met
            trend_score.loc[rows_to_score] = 50.0

            # Perfect trend alignment (all 3 SMAs and price > SMA20 > SMA50 > SMA200)
            if all(col in df.columns for col in sma_cols):
                perfect_trend = (
                    (current_price > df['sma_20d']) & 
                    (df['sma_20d'] > df['sma_50d']) & 
                    (df['sma_50d'] > df['sma_200d'])
                ).fillna(False) # Treat NaNs in comparison as False
                trend_score.loc[perfect_trend] = 100
                
                # Strong trend (price above all 3 SMAs, but not necessarily perfectly aligned)
                strong_trend = (
                    (~perfect_trend) & # Not perfect, but
                    (current_price > df['sma_20d']) & 
                    (current_price > df['sma_50d']) & 
                    (current_price > df['sma_200d'])
                ).fillna(False)
                trend_score.loc[strong_trend] = 85
            
            # Good trend (price above 2 SMAs)
            good_trend = rows_with_any_sma_data & (above_sma_count == 2) & ~trend_score.notna() # Only for rows not yet scored
            trend_score.loc[good_trend] = 70
            
            # Weak trend (price above 1 SMA)
            weak_trend = rows_with_any_sma_data & (above_sma_count == 1) & ~trend_score.notna()
            trend_score.loc[weak_trend] = 40
            
            # Poor trend (price above 0 SMAs considered)
            poor_trend = rows_with_any_sma_data & (above_sma_count == 0) & ~trend_score.notna()
            trend_score.loc[poor_trend] = 20

        return trend_score.clip(0, 100)
    
    @staticmethod
    def _calculate_long_term_strength(df: pd.DataFrame) -> pd.Series:
        """Calculate long-term strength score, propagating NaN."""
        strength_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        lt_cols = ['ret_3m', 'ret_6m', 'ret_1y']
        available_cols = [col for col in lt_cols if col in df.columns and df[col].notna().any()]
        
        if not available_cols:
            return strength_score
        
        # Calculate average long-term return, treating missing values as 0 for this avg
        lt_returns = df[available_cols].fillna(0)
        avg_return = lt_returns.mean(axis=1)
        
        # Mask for rows where at least one long-term return was available to calculate avg_return meaningfully
        has_any_lt_data = df[available_cols].notna().any(axis=1)
        
        # Categorize based on average return for rows with data
        strength_score.loc[has_any_lt_data & (avg_return > 100)] = 100
        strength_score.loc[has_any_lt_data & (avg_return > 50) & (avg_return <= 100)] = 90
        strength_score.loc[has_any_lt_data & (avg_return > 30) & (avg_return <= 50)] = 80
        strength_score.loc[has_any_lt_data & (avg_return > 15) & (avg_return <= 30)] = 70
        strength_score.loc[has_any_lt_data & (avg_return > 5) & (avg_return <= 15)] = 60
        strength_score.loc[has_any_lt_data & (avg_return > 0) & (avg_return <= 5)] = 50
        strength_score.loc[has_any_lt_data & (avg_return > -10) & (avg_return <= 0)] = 40
        strength_score.loc[has_any_lt_data & (avg_return > -25) & (avg_return <= -10)] = 30
        strength_score.loc[has_any_lt_data & (avg_return <= -25)] = 20
        
        return strength_score.clip(0, 100)
    
    @staticmethod
    def _calculate_liquidity_score(df: pd.DataFrame) -> pd.Series:
        """Calculate liquidity score based on trading volume, propagating NaN."""
        liquidity_score = pd.Series(np.nan, index=df.index, dtype=float) # Default to NaN
        
        if 'volume_30d' in df.columns and 'price' in df.columns:
            # Calculate dollar volume, handling NaNs
            dollar_volume = df['volume_30d'].fillna(0) * df['price'].fillna(0)
            
            # Only rank if dollar_volume is not NaN and positive (meaningful volume)
            has_valid_dollar_volume = dollar_volume.notna() & (dollar_volume > 0)
            
            if has_valid_dollar_volume.any():
                liquidity_score.loc[has_valid_dollar_volume] = RankingEngine._safe_rank(
                    dollar_volume.loc[has_valid_dollar_volume], pct=True, ascending=True
                )
        
        return liquidity_score.clip(0, 100)
    
    @staticmethod
    def _calculate_category_ranks(df: pd.DataFrame) -> pd.DataFrame:
        """Calculate percentile ranks within each category"""
        df['category_rank'] = np.nan # Default to NaN
        df['category_percentile'] = np.nan # Default to NaN
        
        categories = df['category'].dropna().unique() # Only iterate non-NaN categories
        
        for category in categories:
            mask = df['category'] == category
            cat_df = df[mask]
            
            if len(cat_df) > 0 and 'master_score' in cat_df.columns and cat_df['master_score'].notna().any():
                # Calculate ranks for non-NaN master_scores within category
                cat_ranks = cat_df['master_score'].rank(method='first', ascending=False, na_option='bottom')
                df.loc[mask, 'category_rank'] = cat_ranks.astype(int)
                
                # Calculate percentiles for non-NaN master_scores within category
                cat_percentiles = cat_df['master_score'].rank(pct=True, ascending=True, na_option='bottom') * 100
                df.loc[mask, 'category_percentile'] = cat_percentiles
        
        return df

# ============================================
# PATTERN DETECTION ENGINE - OPTIMIZED
# ============================================

class PatternDetector:
    """Detect all patterns using vectorized operations"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.5)
    def detect_all_patterns(df: pd.DataFrame) -> pd.DataFrame:
        """Detect all 25 patterns efficiently using vectorized numpy operations."""
        
        if df.empty:
            df['patterns'] = [''] * len(df)
            return df

        # Get all pattern definitions as (name, mask) tuples
        patterns_with_masks = PatternDetector._get_all_pattern_definitions(df)
        
        # Prepare for vectorized processing
        num_patterns = len(patterns_with_masks)
        if num_patterns == 0:
            df['patterns'] = [''] * len(df)
            return df

        # Create a boolean matrix for all patterns
        # Use df.index to ensure alignment
        pattern_matrix = pd.DataFrame(False, index=df.index, columns=[name for name, _ in patterns_with_masks])
        
        # Populate the boolean matrix efficiently
        for pattern_name, mask in patterns_with_masks:
            if mask is not None and not mask.empty:
                # Align mask to DataFrame index. If mask has different index, reindex will align.
                pattern_matrix[pattern_name] = mask.reindex(df.index, fill_value=False)
        
        # Convert the boolean matrix back to a list of pattern strings for each row
        # This is the most efficient way to generate the combined string column.
        df['patterns'] = pattern_matrix.apply(
            lambda row: ' | '.join(row.index[row].tolist()), axis=1
        )
        
        return df
    
    @staticmethod
    def _get_all_pattern_definitions(df: pd.DataFrame) -> List[Tuple[str, pd.Series]]:
        """
        Get all pattern definitions with masks.
        Ensures all patterns return (name, mask) consistently.
        A pattern's mask should be True for stocks that qualify and False otherwise.
        Missing data should be handled within the mask definition by `fillna(False)` or `notna()`.
        """
        patterns = [] 
        
        # Helper to safely get column, defaulting to False Series if not present or all NaN
        def get_col_safe(col_name: str) -> pd.Series:
            return df[col_name].fillna(False) if col_name in df.columns and df[col_name].notna().any() else pd.Series(False, index=df.index)

        # 1. Category Leader
        if 'category_percentile' in df.columns:
            mask = df['category_percentile'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['category_leader']
            patterns.append(('ðŸ”¥ CAT LEADER', mask))
        
        # 2. Hidden Gem
        if 'category_percentile' in df.columns and 'percentile' in df.columns:
            mask = (
                (df['category_percentile'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['hidden_gem']) & 
                (df['percentile'].fillna(100) < 70) # Assume low percentile for NaN for hidden gem to avoid false positives
            )
            patterns.append(('ðŸ’Ž HIDDEN GEM', mask))
        
        # 3. Accelerating
        if 'acceleration_score' in df.columns:
            mask = df['acceleration_score'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['acceleration']
            patterns.append(('ðŸš€ ACCELERATING', mask))
        
        # 4. Institutional
        if 'volume_score' in df.columns and 'vol_ratio_90d_180d' in df.columns:
            mask = (
                (df['volume_score'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['institutional']) &
                (df['vol_ratio_90d_180d'].fillna(0) > 1.1)
            )
            patterns.append(('ðŸ¦ INSTITUTIONAL', mask))
        
        # 5. Volume Explosion
        if 'rvol' in df.columns:
            mask = df['rvol'].fillna(0) > 3
            patterns.append(('âš¡ VOL EXPLOSION', mask))
        
        # 6. Breakout Ready
        if 'breakout_score' in df.columns:
            mask = df['breakout_score'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['breakout_ready']
            patterns.append(('ðŸŽ¯ BREAKOUT', mask))
        
        # 7. Market Leader
        if 'percentile' in df.columns:
            mask = df['percentile'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['market_leader']
            patterns.append(('ðŸ‘‘ MARKET LEADER', mask))
        
        # 8. Momentum Wave
        if 'momentum_score' in df.columns and 'acceleration_score' in df.columns:
            mask = (
                (df['momentum_score'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['momentum_wave']) &
                (df['acceleration_score'].fillna(0) >= 70)
            )
            patterns.append(('ðŸŒŠ MOMENTUM WAVE', mask))
        
        # 9. Liquid Leader
        if 'liquidity_score' in df.columns and 'percentile' in df.columns:
            mask = (
                (df['liquidity_score'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['liquid_leader']) &
                (df['percentile'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['liquid_leader'])
            )
            patterns.append(('ðŸ’° LIQUID LEADER', mask))
        
        # 10. Long-term Strength
        if 'long_term_strength' in df.columns:
            mask = df['long_term_strength'].fillna(0) >= CONFIG.PATTERN_THRESHOLDS['long_strength']
            patterns.append(('ðŸ’ª LONG STRENGTH', mask))
        
        # 11. Quality Trend
        if 'trend_quality' in df.columns:
            mask = df['trend_quality'].fillna(0) >= 80
            patterns.append(('ðŸ“ˆ QUALITY TREND', mask))
        
        # 12. Value Momentum (Fundamental)
        # Ensure 'pe' is notna, >0 and <10000 to be considered valid
        has_valid_pe = get_col_safe('pe').notna() & (get_col_safe('pe') > 0) & (get_col_safe('pe') < 10000)
        if 'pe' in df.columns and 'master_score' in df.columns:
            mask = has_valid_pe & (df['pe'].fillna(0) < 15) & (df['master_score'].fillna(0) >= 70)
            patterns.append(('ðŸ’Ž VALUE MOMENTUM', mask))
        
        # 13. Earnings Rocket
        if 'eps_change_pct' in df.columns and 'acceleration_score' in df.columns:
            has_eps_growth = get_col_safe('eps_change_pct').notna()
            extreme_growth = has_eps_growth & (df['eps_change_pct'].fillna(0) > 1000)
            normal_growth = has_eps_growth & (df['eps_change_pct'].fillna(0) > 50) & (df['eps_change_pct'].fillna(0) <= 1000)
            
            mask = (
                (extreme_growth & (df['acceleration_score'].fillna(0) >= 80)) |
                (normal_growth & (df['acceleration_score'].fillna(0) >= 70))
            )
            patterns.append(('ðŸ“Š EARNINGS ROCKET', mask))
        
        # 14. Quality Leader
        if all(col in df.columns for col in ['pe', 'eps_change_pct', 'percentile']):
            has_complete_data = (
                get_col_safe('pe').notna() & 
                get_col_safe('eps_change_pct').notna() & 
                (get_col_safe('pe') > 0) &
                (get_col_safe('pe') < 10000)
            )
            mask = (
                has_complete_data &
                (df['pe'].fillna(0).between(10, 25)) &
                (df['eps_change_pct'].fillna(0) > 20) &
                (df['percentile'].fillna(0) >= 80)
            )
            patterns.append(('ðŸ† QUALITY LEADER', mask))
        
        # 15. Turnaround Play
        if 'eps_change_pct' in df.columns and 'volume_score' in df.columns:
            has_eps = get_col_safe('eps_change_pct').notna()
            mega_turnaround = has_eps & (df['eps_change_pct'].fillna(0) > 500) & (df['volume_score'].fillna(0) >= 60)
            strong_turnaround = has_eps & (df['eps_change_pct'].fillna(0) > 100) & (df['eps_change_pct'].fillna(0) <= 500) & (df['volume_score'].fillna(0) >= 70)
            
            mask = mega_turnaround | strong_turnaround
            patterns.append(('âš¡ TURNAROUND', mask))
        
        # 16. High PE Warning
        if 'pe' in df.columns:
            has_valid_pe = get_col_safe('pe').notna() & (get_col_safe('pe') > 0)
            mask = has_valid_pe & (df['pe'].fillna(0) > 100)
            patterns.append(('âš ï¸ HIGH PE', mask))
        
        # 17. 52W High Approach
        if all(col in df.columns for col in ['from_high_pct', 'volume_score', 'momentum_score']):
            mask = (
                (df['from_high_pct'].fillna(-100) > -5) & # Fill with a very low value to exclude NaNs
                (df['volume_score'].fillna(0) >= 70) & 
                (df['momentum_score'].fillna(0) >= 60)
            )
            patterns.append(('ðŸŽ¯ 52W HIGH APPROACH', mask))
        
        # 18. 52W Low Bounce
        if all(col in df.columns for col in ['from_low_pct', 'acceleration_score', 'ret_30d']):
            mask = (
                (df['from_low_pct'].fillna(100) < 20) & # Fill with a high value to exclude NaNs
                (df['acceleration_score'].fillna(0) >= 80) & 
                (df['ret_30d'].fillna(0) > 10)
            )
            patterns.append(('ðŸ”„ 52W LOW BOUNCE', mask))
        
        # 19. Golden Zone
        if all(col in df.columns for col in ['from_low_pct', 'from_high_pct', 'trend_quality']):
            mask = (
                (df['from_low_pct'].fillna(0) > 60) & 
                (df['from_high_pct'].fillna(0) > -40) & 
                (df['trend_quality'].fillna(0) >= 70)
            )
            patterns.append(('ðŸ‘‘ GOLDEN ZONE', mask))
        
        # 20. Volume Accumulation
        if all(col in df.columns for col in ['vol_ratio_30d_90d', 'vol_ratio_90d_180d', 'ret_30d']):
            mask = (
                (df['vol_ratio_30d_90d'].fillna(0) > 1.2) & 
                (df['vol_ratio_90d_180d'].fillna(0) > 1.1) & 
                (df['ret_30d'].fillna(0) > 5)
            )
            patterns.append(('ðŸ“Š VOL ACCUMULATION', mask))
        
        # 21. Momentum Divergence
        if all(col in df.columns for col in ['ret_7d', 'ret_30d', 'acceleration_score', 'rvol']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_7d_pace = np.where(df['ret_7d'].fillna(0) != 0, df['ret_7d'].fillna(0) / 7, np.nan)
                daily_30d_pace = np.where(df['ret_30d'].fillna(0) != 0, df['ret_30d'].fillna(0) / 30, np.nan)
            
            # Ensure both paces are valid for comparison
            mask = (
                daily_7d_pace.notna() & daily_30d_pace.notna() &
                (daily_7d_pace > daily_30d_pace * 1.5) & 
                (df['acceleration_score'].fillna(0) >= 85) & 
                (df['rvol'].fillna(0) > 2)
            )
            patterns.append(('ðŸ”€ MOMENTUM DIVERGE', mask))
        
        # 22. Range Compression
        if all(col in df.columns for col in ['high_52w', 'low_52w', 'from_low_pct']):
            with np.errstate(divide='ignore', invalid='ignore'):
                # Calculate range_pct, filling with a high value if division by zero for low_52w
                range_pct = np.where(
                    df['low_52w'].fillna(0) > 0,
                    ((df['high_52w'].fillna(0) - df['low_52w'].fillna(0)) / df['low_52w'].fillna(0)) * 100,
                    100 # High value if low_52w is 0 or NaN, indicating extreme range or missing data
                )
            # Ensure from_low_pct is not NaN
            mask = (range_pct < 50) & (df['from_low_pct'].fillna(0) > 30)
            patterns.append(('ðŸŽ¯ RANGE COMPRESS', mask))
        
        # 23. Stealth Accumulator (NEW)
        if all(col in df.columns for col in ['vol_ratio_90d_180d', 'vol_ratio_30d_90d', 'from_low_pct', 'ret_7d', 'ret_30d']):
            with np.errstate(divide='ignore', invalid='ignore'):
                ret_ratio = np.where(df['ret_30d'].fillna(0) != 0, df['ret_7d'].fillna(0) / (df['ret_30d'].fillna(0) / 4), np.nan)
            
            mask = (
                df['vol_ratio_90d_180d'].fillna(0) > 1.1) & \
                (df['vol_ratio_30d_90d'].fillna(0).between(0.9, 1.1)) & \
                (df['from_low_pct'].fillna(0) > 40) & \
                (ret_ratio.notna() & (ret_ratio > 1) # Ensure ret_ratio is not NaN and valid
            )
            patterns.append(('ðŸ¤« STEALTH', mask))
        
        # 24. Momentum Vampire (NEW)
        if all(col in df.columns for col in ['ret_1d', 'ret_7d', 'rvol', 'from_high_pct', 'category']):
            with np.errstate(divide='ignore', invalid='ignore'):
                daily_pace_ratio = np.where(df['ret_7d'].fillna(0) != 0, df['ret_1d'].fillna(0) / (df['ret_7d'].fillna(0) / 7), np.nan)
            
            mask = (
                daily_pace_ratio.notna() & (daily_pace_ratio > 2) & # Ensure daily_pace_ratio is not NaN and valid
                (df['rvol'].fillna(0) > 3) &
                (df['from_high_pct'].fillna(0) > -15) &
                (df['category'].isin(['Small Cap', 'Micro Cap']))
            )
            patterns.append(('ðŸ§› VAMPIRE', mask))
        
        # 25. Perfect Storm (NEW)
        if 'momentum_harmony' in df.columns and 'master_score' in df.columns:
            mask = (
                (df['momentum_harmony'].fillna(0) == 4) & # Fill with 0 for comparison
                (df['master_score'].fillna(0) > 80)
            )
            patterns.append(('â›ˆï¸ PERFECT STORM', mask))
        
        return patterns

# ============================================
# MARKET INTELLIGENCE
# ============================================

class MarketIntelligence:
    """Advanced market analysis and regime detection"""
    
    @staticmethod
    def detect_market_regime(df: pd.DataFrame) -> Tuple[str, Dict[str, Any]]:
        """Detect current market regime with supporting data"""
        
        if df.empty:
            return "ðŸ˜´ NO DATA", {}
        
        # Calculate key metrics
        metrics = {}
        
        # Category performance (use fillna(0) for mean calculation if missing)
        if 'category' in df.columns and 'master_score' in df.columns:
            category_scores = df.groupby('category')['master_score'].mean().fillna(0)
            
            micro_small_avg = category_scores[category_scores.index.isin(['Micro Cap', 'Small Cap'])].mean()
            large_mega_avg = category_scores[category_scores.index.isin(['Large Cap', 'Mega Cap'])].mean()
            
            metrics['micro_small_avg'] = micro_small_avg
            metrics['large_mega_avg'] = large_mega_avg
            metrics['category_spread'] = micro_small_avg - large_mega_avg
        else:
            micro_small_avg = 50
            large_mega_avg = 50
        
        # Market breadth (use fillna(0) for ret_30d)
        if 'ret_30d' in df.columns:
            breadth = len(df[df['ret_30d'].fillna(0) > 0]) / len(df) if len(df) > 0 else 0
            metrics['breadth'] = breadth
        else:
            breadth = 0.5
        
        # Average RVOL (use median to be robust to outliers, fillna(1.0) for rvol)
        if 'rvol' in df.columns:
            avg_rvol = df['rvol'].fillna(1.0).median()
            metrics['avg_rvol'] = avg_rvol
        else:
            avg_rvol = 1.0
        
        # Determine regime
        if micro_small_avg > large_mega_avg + 10 and breadth > 0.6:
            regime = "ðŸ”¥ RISK-ON BULL"
        elif large_mega_avg > micro_small_avg + 10 and breadth < 0.4:
            regime = "ðŸ›¡ï¸ RISK-OFF DEFENSIVE"
        elif avg_rvol > 1.5 and breadth > 0.5:
            regime = "âš¡ VOLATILE OPPORTUNITY"
        else:
            regime = "ðŸ˜´ RANGE-BOUND"
        
        metrics['regime'] = regime
        
        return regime, metrics
    
    @staticmethod
    def calculate_advance_decline_ratio(df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate advance/decline ratio and related metrics"""
        
        ad_metrics = {}
        
        if 'ret_1d' in df.columns:
            # Fill NaN for ret_1d to treat them as unchanged
            advancing = len(df[df['ret_1d'].fillna(0) > 0])
            declining = len(df[df['ret_1d'].fillna(0) < 0])
            unchanged = len(df[df['ret_1d'].fillna(0) == 0])
            
            ad_metrics['advancing'] = advancing
            ad_metrics['declining'] = declining
            ad_metrics['unchanged'] = unchanged
            
            if declining > 0:
                ad_metrics['ad_ratio'] = advancing / declining
            else:
                ad_metrics['ad_ratio'] = float('inf') if advancing > 0 else 1.0 # If no declines, ratio is infinite or 1.0
            
            ad_metrics['ad_line'] = advancing - declining
            ad_metrics['breadth_pct'] = (advancing / len(df)) * 100 if len(df) > 0 else 0
        
        return ad_metrics
    
    @staticmethod
    def _apply_dynamic_sampling(df_group: pd.DataFrame) -> pd.DataFrame:
        """Helper to apply dynamic sampling based on group size."""
        group_size = len(df_group)
        
        if 1 <= group_size <= 5:
            sample_count = group_size # Use all (100%)
        elif 6 <= group_size <= 20:
            sample_count = max(1, int(group_size * 0.80)) # Use 80%
        elif 21 <= group_size <= 50:
            sample_count = max(1, int(group_size * 0.60)) # Use 60%
        elif 51 <= group_size <= 100:
            sample_count = max(1, int(group_size * 0.40)) # Use 40%
        else: # group_size > 100
            sample_count = min(50, int(group_size * 0.25)) # Use 25%, max 50 stocks
        
        if sample_count > 0:
            # Sort by master_score and take the dynamic 'N'
            # Fillna(0) for master_score for consistent sorting of NaNs
            return df_group.nlargest(sample_count, 'master_score', keep='first')
        else:
            return pd.DataFrame() # No stocks selected after sampling

    @staticmethod
    def _calculate_flow_metrics(normalized_df: pd.DataFrame, group_col: str) -> pd.DataFrame:
        """Helper to calculate common flow metrics for sector/industry rotation."""
        # Use .get with a default lambda to handle potentially missing columns for aggregation
        agg_dict = {
            'master_score': ['mean', 'median', 'std', 'count'],
            'momentum_score': 'mean',
            'volume_score': 'mean',
            'rvol': 'mean',
            'ret_30d': 'mean',
            'money_flow_mm': 'sum'
        }
        
        # Filter agg_dict to only include columns present in normalized_df
        available_agg_dict = {
            k: v for k, v in agg_dict.items() if k in normalized_df.columns
        }

        group_metrics = normalized_df.groupby(group_col).agg(available_agg_dict).round(2)
        
        # Flatten column names
        # Create a new list of columns that matches the actual aggregation output
        new_columns = []
        for col, funcs in available_agg_dict.items():
            if isinstance(funcs, list):
                for f in funcs:
                    new_columns.append(f"{col}_{f}")
            else:
                new_columns.append(f"{col}_{funcs}")
        
        group_metrics.columns = new_columns
        
        # Rename to clean names for display
        rename_map = {
            'master_score_mean': 'avg_score',
            'master_score_median': 'median_score',
            'master_score_std': 'std_score',
            'master_score_count': 'count',
            'momentum_score_mean': 'avg_momentum',
            'volume_score_mean': 'avg_volume',
            'rvol_mean': 'avg_rvol',
            'ret_30d_mean': 'avg_ret_30d',
            'money_flow_mm_sum': 'total_money_flow'
        }
        group_metrics = group_metrics.rename(columns=rename_map)

        # Calculate flow score with median for robustness
        # Fill NaN with 0 for score calculation
        group_metrics['flow_score'] = (
            group_metrics['avg_score'].fillna(0) * 0.3 +
            group_metrics['median_score'].fillna(0) * 0.2 +
            group_metrics['avg_momentum'].fillna(0) * 0.25 +
            group_metrics['avg_volume'].fillna(0) * 0.25
        )
        
        # Rank groups
        group_metrics['rank'] = group_metrics['flow_score'].rank(ascending=False, method='min') # Use min for ties
        
        return group_metrics.sort_values('flow_score', ascending=False)

    @staticmethod
    def detect_sector_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect sector rotation patterns with normalized analysis and dynamic sampling.
        Uses dynamic sampling based on sector size.
        """
        if 'sector' not in df.columns or df.empty:
            return pd.DataFrame()
        
        sector_dfs = []
        
        # Group by sector and apply dynamic sampling
        grouped_sectors = df.groupby('sector')
        for sector_name, sector_group_df in grouped_sectors:
            if sector_name != 'Unknown':
                sampled_sector_df = MarketIntelligence._apply_dynamic_sampling(sector_group_df.copy())
                if not sampled_sector_df.empty:
                    sector_dfs.append(sampled_sector_df)
        
        if not sector_dfs:
            return pd.DataFrame()
        
        normalized_df = pd.concat(sector_dfs, ignore_index=True)
        
        # Calculate sector metrics on normalized data
        sector_metrics = MarketIntelligence._calculate_flow_metrics(normalized_df, 'sector')
        
        # Add original sector size for reference (from the original full dataframe)
        original_counts = df.groupby('sector').size().rename('total_stocks')
        sector_metrics = sector_metrics.join(original_counts, how='left')
        sector_metrics['analyzed_stocks'] = sector_metrics['count']
        
        return sector_metrics

    @staticmethod
    def detect_industry_rotation(df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect industry rotation patterns with normalized analysis and dynamic sampling.
        Mirrors detect_sector_rotation logic for consistency.
        """
        if 'industry' not in df.columns or df.empty:
            return pd.DataFrame()
        
        industry_dfs = []
        
        # Group by industry and apply dynamic sampling
        grouped_industries = df.groupby('industry')
        for industry_name, industry_group_df in grouped_industries:
            if industry_name != 'Unknown':
                sampled_industry_df = MarketIntelligence._apply_dynamic_sampling(industry_group_df.copy())
                if not sampled_industry_df.empty:
                    industry_dfs.append(sampled_industry_df)
        
        if not industry_dfs:
            return pd.DataFrame()
        
        normalized_df = pd.concat(industry_dfs, ignore_index=True)
        
        # Calculate industry metrics on normalized data
        industry_metrics = MarketIntelligence._calculate_flow_metrics(normalized_df, 'industry')
        
        # Add original industry size for reference
        original_counts = df.groupby('industry').size().rename('total_stocks')
        industry_metrics = industry_metrics.join(original_counts, how='left')
        industry_metrics['analyzed_stocks'] = industry_metrics['count']
        
        return industry_metrics

# ============================================
# VISUALIZATION ENGINE
# ============================================

class Visualizer:
    """Create all visualizations with proper error handling"""
    
    @staticmethod
    def create_score_distribution(df: pd.DataFrame) -> go.Figure:
        """Create score distribution chart"""
        fig = go.Figure()
        
        if df.empty:
            fig.add_annotation(
                text="No data available for visualization",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig
        
        # Score components to visualize
        scores = [
            ('position_score', 'Position', '#3498db'),
            ('volume_score', 'Volume', '#e74c3c'),
            ('momentum_score', 'Momentum', '#2ecc71'),
            ('acceleration_score', 'Acceleration', '#f39c12'),
            ('breakout_score', 'Breakout', '#9b59b6'),
            ('rvol_score', 'RVOL', '#e67e22')
        ]
        
        for score_col, label, color in scores:
            if score_col in df.columns:
                # Drop NaNs before plotting, but check if there's data left
                score_data = df[score_col].dropna()
                if len(score_data) > 0:
                    fig.add_trace(go.Box(
                        y=score_data,
                        name=label,
                        marker_color=color,
                        boxpoints='outliers',
                        hovertemplate=f'{label}<br>Score: %{{y:.1f}}<extra></extra>'
                    ))
        
        fig.update_layout(
            title="Score Component Distribution",
            yaxis_title="Score (0-100)",
            template='plotly_white',
            height=400,
            showlegend=False
        )
        
        return fig

    @staticmethod
    def create_acceleration_profiles(df: pd.DataFrame, n: int = 10) -> go.Figure:
        """Create acceleration profiles showing momentum over time"""
        try:
            # Filter for stocks with sufficient return data for plotting
            plot_df = df.dropna(subset=['ret_1d', 'ret_7d', 'ret_30d'], how='any')
            if plot_df.empty:
                logger.info("No stocks with complete return data for acceleration profiles.")
                fig = go.Figure()
                fig.add_annotation(
                    text="No complete return data available for this chart.",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False
                )
                return fig
            
            # Get top accelerating stocks
            accel_df = plot_df.nlargest(min(n, len(plot_df)), 'acceleration_score')
            
            if len(accel_df) == 0:
                fig = go.Figure()
                fig.add_annotation(
                    text="No stocks meet criteria for acceleration profiles.",
                    xref="paper", yref="paper",
                    x=0.5, y=0.5, showarrow=False
                )
                return fig
            
            fig = go.Figure()
            
            # Create lines for each stock
            for _, stock in accel_df.iterrows():
                # Build timeline data using only available and non-NaN data
                x_points = ['Start']
                y_points = [0] # Starting point for all profiles
                
                # Check for NaNs and append only valid data points
                if 'ret_30d' in stock.index and pd.notna(stock['ret_30d']):
                    x_points.append('30D')
                    y_points.append(stock['ret_30d'])
                
                if 'ret_7d' in stock.index and pd.notna(stock['ret_7d']):
                    x_points.append('7D')
                    y_points.append(stock['ret_7d'])
                
                if 'ret_1d' in stock.index and pd.notna(stock['ret_1d']):
                    x_points.append('Today')
                    y_points.append(stock['ret_1d'])
                
                if len(x_points) > 1:  # Only plot if we have at least one return data point + Start
                    # Determine line style based on acceleration
                    accel_score = stock.get('acceleration_score', 0) # Default to 0 if missing
                    if accel_score >= 85:
                        line_style = dict(width=3, dash='solid')
                        marker_style = dict(size=10, symbol='star', line=dict(color='DarkSlateGrey', width=1))
                    elif accel_score >= 70:
                        line_style = dict(width=2, dash='solid')
                        marker_style = dict(size=8)
                    else:
                        line_style = dict(width=2, dash='dot')
                        marker_style = dict(size=6)
                    
                    fig.add_trace(go.Scatter(
                        x=x_points,
                        y=y_points,
                        mode='lines+markers',
                        name=f"{stock['ticker']} ({accel_score:.0f})",
                        line=line_style,
                        marker=marker_style,
                        hovertemplate=(
                            f"<b>{stock['ticker']}</b><br>" +
                            "%{x}: %{y:.1f}%<br>" +
                            f"Accel Score: {accel_score:.0f}<extra></extra>"
                        )
                    ))
            
            # Add zero line
            fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)
            
            fig.update_layout(
                title=f"Acceleration Profiles - Top {len(accel_df)} Momentum Builders",
                xaxis_title="Time Frame",
                yaxis_title="Return %",
                height=400,
                template='plotly_white',
                showlegend=True,
                legend=dict(
                    orientation="v",
                    yanchor="top",
                    y=1,
                    xanchor="left",
                    x=1.02
                ),
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            logger.error(f"Error creating acceleration profiles: {str(e)}")
            fig = go.Figure()
            fig.add_annotation(
                text=f"Error generating chart: {e}",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig

# ============================================
# FILTER ENGINE - OPTIMIZED
# ============================================

class FilterEngine:
    """Handle all filtering operations efficiently"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.2)
    def apply_filters(df: pd.DataFrame, filters: Dict[str, Any]) -> pd.DataFrame:
        """Apply all filters with optimized performance using numpy.logical_and.reduce"""
        
        if df.empty:
            return df
        
        # List to store all individual boolean masks
        masks = []
        
        # Category filter
        categories = filters.get('categories', [])
        if categories and 'All' not in categories and 'category' in df.columns:
            masks.append(df['category'].isin(categories))
        
        # Sector filter
        sectors = filters.get('sectors', [])
        if sectors and 'All' not in sectors and 'sector' in df.columns:
            masks.append(df['sector'].isin(sectors))

        # Industry filter
        industries = filters.get('industries', [])
        if industries and 'All' not in industries:
            if 'industry' in df.columns:
                masks.append(df['industry'].isin(industries))
            else:
                logger.warning("Industry column not found in data for filtering.")
        
        # Minimum Master Score filter
        min_score = filters.get('min_score', 0)
        if min_score > 0 and 'master_score' in df.columns:
            masks.append(df['master_score'] >= min_score)
        
        # EPS change filter (handle None/empty string for min_eps_change)
        min_eps_change = filters.get('min_eps_change')
        if min_eps_change is not None and 'eps_change_pct' in df.columns:
            # Include NaNs in the mask if they satisfy the condition (e.g., if min_eps_change is very low)
            # Or, if only valid EPS is desired, filter out NaNs explicitly before this.
            # For now, treat NaN as not satisfying the condition.
            masks.append(df['eps_change_pct'].notna() & (df['eps_change_pct'] >= min_eps_change))
        
        # Pattern filter
        patterns = filters.get('patterns', [])
        if patterns and 'patterns' in df.columns:
            # Ensure patterns column is string type and handle NaNs
            pattern_column_str = df['patterns'].fillna('').astype(str)
            pattern_regex = '|'.join(patterns)
            masks.append(pattern_column_str.str.contains(pattern_regex, case=False, regex=True))
        
        # Trend filter
        trend_range = filters.get('trend_range')
        if filters.get('trend_filter') != 'All Trends' and trend_range and 'trend_quality' in df.columns:
            min_trend, max_trend = trend_range
            masks.append(df['trend_quality'].notna() & (df['trend_quality'] >= min_trend) & (df['trend_quality'] <= max_trend))
        
        # PE filters
        min_pe = filters.get('min_pe')
        if min_pe is not None and 'pe' in df.columns:
            # Include valid positive PEs only
            masks.append(df['pe'].notna() & (df['pe'] > 0) & (df['pe'] >= min_pe))
        
        max_pe = filters.get('max_pe')
        if max_pe is not None and 'pe' in df.columns:
            # Include valid positive PEs only
            masks.append(df['pe'].notna() & (df['pe'] > 0) & (df['pe'] <= max_pe))
        
        # Apply tier filters
        for tier_type_key, col_name_suffix in [
            ('eps_tiers', 'eps_tier'),
            ('pe_tiers', 'pe_tier'),
            ('price_tiers', 'price_tier')
        ]:
            tier_values = filters.get(tier_type_key, [])
            col_name = col_name_suffix # Full column name is already like 'eps_tier'
            if tier_values and 'All' not in tier_values and col_name in df.columns:
                masks.append(df[col_name].isin(tier_values))
        
        # Data completeness filter
        if filters.get('require_fundamental_data', False):
            if 'pe' in df.columns and 'eps_change_pct' in df.columns:
                masks.append(df['pe'].notna() & (df['pe'] > 0) & df['eps_change_pct'].notna())
            else:
                logger.warning("Fundamental columns (PE, EPS) not found for 'require_fundamental_data' filter.")
        
        # Wave State filter
        wave_states = filters.get('wave_states', [])
        if wave_states and 'All' not in wave_states and 'wave_state' in df.columns:
            masks.append(df['wave_state'].isin(wave_states))

        # Wave Strength filter
        wave_strength_range = filters.get('wave_strength_range')
        # Only apply if the range is not the default (0, 100)
        if wave_strength_range and wave_strength_range != (0, 100) and 'overall_wave_strength' in df.columns:
            min_ws, max_ws = wave_strength_range
            masks.append(df['overall_wave_strength'].notna() & (df['overall_wave_strength'] >= min_ws) & (df['overall_wave_strength'] <= max_ws))

        # Apply all collected masks using numpy.logical_and.reduce for efficiency
        if masks:
            combined_mask = np.logical_and.reduce(masks)
            filtered_df = df[combined_mask].copy()
        else:
            filtered_df = df.copy() # No filters applied, return full copy
        
        logger.info(f"Filters reduced {len(df)} to {len(filtered_df)} stocks")
        
        return filtered_df
    
    @staticmethod
    def get_filter_options(df: pd.DataFrame, column: str, current_filters: Dict[str, Any]) -> List[str]:
        """Get available filter options with smart interconnection"""
        
        if df.empty or column not in df.columns:
            return []
        
        temp_filters = current_filters.copy()
        
        # Map filter column names to their keys in the filters dictionary
        filter_key_map = {
            'category': 'categories',
            'sector': 'sectors',
            'industry': 'industries', 
            'eps_tier': 'eps_tiers',
            'pe_tier': 'pe_tiers',
            'price_tier': 'price_tiers',
            'wave_state': 'wave_states' 
        }
        
        # Remove the current column's filter to see all its options based on *other* active filters
        if column in filter_key_map:
            temp_filters.pop(filter_key_map[column], None)
        
        # Apply remaining filters to get the interconnected options
        filtered_df = FilterEngine.apply_filters(df, temp_filters)
        
        # Get unique values, drop NaNs, and exclude problematic strings
        values = filtered_df[column].dropna().astype(str).unique()
        values = [v for v in values if v.strip().lower() not in ['unknown', '', 'nan', 'n/a', 'none', '-']]
        
        return sorted(values)

# ============================================
# SEARCH ENGINE - OPTIMIZED
# ============================================

class SearchEngine:
    """Optimized search functionality"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=0.05)
    def search_stocks(df: pd.DataFrame, query: str) -> pd.DataFrame:
        """Search stocks with optimized performance"""
        
        if not query or df.empty:
            return pd.DataFrame()
        
        try:
            query_upper = query.upper().strip()
            
            # Initialize masks for different match types
            mask_ticker_exact = pd.Series(False, index=df.index)
            mask_ticker_contains = pd.Series(False, index=df.index)
            mask_company_contains = pd.Series(False, index=df.index)
            mask_company_word_match = pd.Series(False, index=df.index)

            # Method 1: Direct ticker match (exact)
            if 'ticker' in df.columns:
                mask_ticker_exact = (df['ticker'].str.upper() == query_upper).fillna(False)
                if mask_ticker_exact.any():
                    return df[mask_ticker_exact].copy() # Return exact match immediately

            # If no exact ticker match, proceed with broader search
            # Ensure columns exist and handle NaNs for string operations
            ticker_upper = df['ticker'].str.upper().fillna('') if 'ticker' in df.columns else pd.Series('', index=df.index)
            company_upper = df['company_name'].str.upper().fillna('') if 'company_name' in df.columns else pd.Series('', index=df.index)
            
            # Method 2: Ticker contains query
            mask_ticker_contains = ticker_upper.str.contains(query_upper, regex=False)
            
            # Method 3: Company name contains query (case insensitive)
            mask_company_contains = company_upper.str.contains(query_upper, regex=False)
            
            # Method 4: Partial match at start of words in company name
            # Only apply this if the company_name column actually exists and has non-empty values
            if 'company_name' in df.columns and not df['company_name'].empty:
                # Optimized word start check using regex
                mask_company_word_match = df['company_name'].str.contains(r'\b' + re.escape(query_upper), case=False, na=False, regex=True)
            
            # Combine all results and remove duplicates
            # Use combined_mask for performance
            combined_mask = mask_ticker_exact | mask_ticker_contains | mask_company_contains | mask_company_word_match
            all_matches = df[combined_mask].copy()
            
            if not all_matches.empty:
                # Add relevance score for sorting
                all_matches['relevance'] = 0
                all_matches.loc[mask_ticker_exact[combined_mask], 'relevance'] = 100 # Highest for exact ticker
                all_matches.loc[mask_ticker_contains[combined_mask], 'relevance'] += 50
                all_matches.loc[mask_company_contains[combined_mask], 'relevance'] += 30
                all_matches.loc[mask_company_word_match[combined_mask], 'relevance'] += 20 # Lower weight for word match
                
                # Sort by relevance then master score (descending)
                return all_matches.sort_values(['relevance', 'master_score'], ascending=[False, False]).drop('relevance', axis=1)
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            return pd.DataFrame()

# ============================================
# EXPORT ENGINE - ENHANCED
# ============================================

class ExportEngine:
    """Handle all export operations with streaming for large datasets"""
    
    @staticmethod
    @PerformanceMonitor.timer(target_time=1.0)
    def create_excel_report(df: pd.DataFrame, template: str = 'full') -> BytesIO:
        """Create comprehensive Excel report with smart templates"""
        
        output = BytesIO()
        
        # Define export templates
        templates = {
            'day_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'rvol', 
                           'momentum_score', 'acceleration_score', 'ret_1d', 'ret_7d', 
                           'volume_score', 'vmi', 'wave_state', 'patterns', 'category', 'sector', 'industry'], 
                'focus': 'Intraday momentum and volume'
            },
            'swing_trader': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 
                           'breakout_score', 'position_score', 'position_tension',
                           'from_high_pct', 'from_low_pct', 'trend_quality', 
                           'momentum_harmony', 'patterns', 'sector', 'industry'], 
                'focus': 'Position and breakout setups'
            },
            'investor': {
                'columns': ['rank', 'ticker', 'company_name', 'master_score', 'pe', 
                           'eps_current', 'eps_change_pct', 'ret_1y', 'ret_3y', 
                           'long_term_strength', 'money_flow_mm', 'category', 'sector', 'industry'], 
                'focus': 'Fundamentals and long-term performance'
            },
            'full': {
                'columns': None,  # Use all columns
                'focus': 'Complete analysis'
            }
        }
        
        try:
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                workbook = writer.book
                
                # Define formats
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#3498db',
                    'font_color': 'white',
                    'border': 1
                })
                
                # Standard number formats for various data types
                float_format = workbook.add_format({'num_format': '#,##0.00'})
                percent_format = workbook.add_format({'num_format': '0.0%'})
                currency_format = workbook.add_format({'num_format': 'â‚¹#,##0'})
                currency_m_format = workbook.add_format({'num_format': 'â‚¹#,##0.0,"M"'}) # For Money Flow in Millions
                rvol_format = workbook.add_format({'num_format': '0.0"x"'})
                score_format = workbook.add_format({'num_format': '0.0'})
                integer_format = workbook.add_format({'num_format': '#,##0'})

                # Map column names to formats
                column_formats = {
                    'price': currency_format,
                    'master_score': score_format,
                    'position_score': score_format,
                    'volume_score': score_format,
                    'momentum_score': score_format,
                    'acceleration_score': score_format,
                    'breakout_score': score_format,
                    'rvol_score': score_format,
                    'trend_quality': score_format,
                    'pe': float_format,
                    'eps_current': float_format,
                    'from_low_pct': percent_format,
                    'from_high_pct': percent_format,
                    'ret_1d': percent_format, 'ret_3d': percent_format, 'ret_7d': percent_format, 
                    'ret_30d': percent_format, 'ret_3m': percent_format, 'ret_6m': percent_format, 
                    'ret_1y': percent_format, 'ret_3y': percent_format, 'ret_5y': percent_format,
                    'eps_change_pct': percent_format,
                    'rvol': rvol_format,
                    'vmi': float_format,
                    'money_flow_mm': currency_m_format,
                    'position_tension': float_format,
                    'momentum_harmony': integer_format,
                    'overall_wave_strength': score_format
                }
                
                # 1. Top 100 Stocks
                top_100 = df.nlargest(min(100, len(df)), 'master_score')
                
                if template in templates and templates[template]['columns']:
                    export_cols = [col for col in templates[template]['columns'] if col in top_100.columns]
                else:
                    # For 'full' template, get all columns except internal ones
                    internal_cols = ['percentile', 'category_rank', 'category_percentile', 'eps_tier', 'pe_tier', 'price_tier', 'signal_count', 'shift_strength', 'surge_score', 'total_stocks', 'analyzed_stocks', 'flow_score', 'avg_score', 'median_score', 'std_score', 'avg_momentum', 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'total_money_flow', 'rank_flow_score', 'dummy_money_flow']
                    export_cols = [col for col in top_100.columns if col not in internal_cols]
                
                top_100_export = top_100[export_cols]
                top_100_export.to_excel(writer, sheet_name='Top 100 Stocks', index=False)
                
                # Apply formats to 'Top 100 Stocks' sheet
                worksheet = writer.sheets['Top 100 Stocks']
                for i, col in enumerate(top_100_export.columns):
                    worksheet.write(0, i, col, header_format) # Write header with format
                    # Apply column-specific formatting
                    if col in column_formats:
                        col_letter = chr(ord('A') + i) # Get Excel column letter
                        # Apply format to the whole column (excluding header row)
                        worksheet.set_column(f'{col_letter}:{col_letter}', None, column_formats[col])
                    # Auto-fit column width
                    worksheet.autofit()
                
                # 2. Market Intelligence
                intel_data = []
                regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
                intel_data.append({
                    'Metric': 'Market Regime',
                    'Value': regime,
                    'Details': f"Breadth: {regime_metrics.get('breadth', 0):.1%} | Avg RVOL: {regime_metrics.get('avg_rvol', 1):.1f}x"
                })
                ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
                intel_data.append({
                    'Metric': 'Advance/Decline Ratio (1D)',
                    'Value': f"{ad_metrics.get('ad_ratio', 1):.2f}",
                    'Details': f"Advances: {ad_metrics.get('advancing', 0)}, Declines: {ad_metrics.get('declining', 0)}, Unchanged: {ad_metrics.get('unchanged', 0)}"
                })
                
                intel_df = pd.DataFrame(intel_data)
                intel_df.to_excel(writer, sheet_name='Market Intelligence', index=False)
                worksheet = writer.sheets['Market Intelligence']
                for i, col in enumerate(intel_df.columns): worksheet.write(0, i, col, header_format)
                worksheet.autofit()

                # 3. Sector Rotation
                sector_rotation = MarketIntelligence.detect_sector_rotation(df)
                if not sector_rotation.empty:
                    sector_rotation.to_excel(writer, sheet_name='Sector Rotation')
                    worksheet = writer.sheets['Sector Rotation']
                    for i, col in enumerate(sector_rotation.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()
                
                # 4. Industry Rotation (NEW)
                industry_rotation = MarketIntelligence.detect_industry_rotation(df)
                if not industry_rotation.empty:
                    industry_rotation.to_excel(writer, sheet_name='Industry Rotation')
                    worksheet = writer.sheets['Industry Rotation']
                    for i, col in enumerate(industry_rotation.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()

                # 5. Pattern Analysis
                pattern_counts = {}
                for patterns_str in df['patterns'].dropna():
                    if patterns_str:
                        for p in patterns_str.split(' | '):
                            pattern_counts[p] = pattern_counts.get(p, 0) + 1
                
                if pattern_counts:
                    pattern_df = pd.DataFrame(
                        list(pattern_counts.items()),
                        columns=['Pattern', 'Count']
                    ).sort_values('Count', ascending=False)
                    pattern_df.to_excel(writer, sheet_name='Pattern Analysis', index=False)
                    worksheet = writer.sheets['Pattern Analysis']
                    for i, col in enumerate(pattern_df.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()
                
                # 6. Wave Radar Signals (Top 50 from filtered_df)
                wave_signals = df[
                    (df['momentum_score'].fillna(0) >= 60) & 
                    (df['acceleration_score'].fillna(0) >= 70) &
                    (df['rvol'].fillna(0) >= 2)
                ].nlargest(50, 'master_score', keep='first') # Use nlargest for robust selection
                
                if not wave_signals.empty:
                    wave_cols = ['ticker', 'company_name', 'master_score', 
                                'momentum_score', 'acceleration_score', 'rvol',
                                'wave_state', 'patterns', 'category', 'sector', 'industry']
                    available_wave_cols = [col for col in wave_cols if col in wave_signals.columns]
                    
                    wave_signals[available_wave_cols].to_excel(
                        writer, sheet_name='Wave Radar Signals', index=False
                    )
                    worksheet = writer.sheets['Wave Radar Signals']
                    for i, col in enumerate(wave_signals.columns): worksheet.write(0, i, col, header_format)
                    worksheet.autofit()

                # 7. Summary Statistics
                summary_stats = {
                    'Total Stocks Processed': len(df),
                    'Average Master Score (All)': df['master_score'].mean() if not df.empty else 0,
                    'Stocks with Patterns (All)': (df['patterns'] != '').sum() if 'patterns' in df.columns else 0,
                    'High RVOL (>2x) (All)': (df['rvol'].fillna(0) > 2).sum() if 'rvol' in df.columns else 0,
                    'Positive 30D Returns (All)': (df['ret_30d'].fillna(0) > 0).sum() if 'ret_30d' in df.columns else 0,
                    'Data Completeness %': st.session_state.data_quality.get('completeness', 0),
                    'Clipping Events Count': sum(DataValidator.get_clipping_counts().values()), # Get current counts
                    'Template Used': template,
                    'Export Date (UTC)': datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
                }
                
                summary_df = pd.DataFrame(list(summary_stats.items()), columns=['Metric', 'Value'])
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                worksheet = writer.sheets['Summary']
                for i, col in enumerate(summary_df.columns): worksheet.write(0, i, col, header_format)
                worksheet.autofit()

                logger.info(f"Excel report created successfully with {len(writer.sheets)} sheets")
                
        except Exception as e:
            logger.error(f"Error creating Excel report: {str(e)}", exc_info=True)
            raise
        
        output.seek(0)
        return output
    
    @staticmethod
    def create_csv_export(df: pd.DataFrame) -> str:
        """Create CSV export efficiently"""
        
        export_cols = [
            'rank', 'ticker', 'company_name', 'master_score',
            'position_score', 'volume_score', 'momentum_score',
            'acceleration_score', 'breakout_score', 'rvol_score',
            'trend_quality', 'price', 'pe', 'eps_current', 'eps_change_pct',
            'from_low_pct', 'from_high_pct',
            'ret_1d', 'ret_7d', 'ret_30d', 'ret_3m', 'ret_6m', 'ret_1y',
            'rvol', 'vmi', 'money_flow_mm', 'position_tension',
            'momentum_harmony', 'wave_state', 'patterns', 
            'category', 'sector', 'industry', 'eps_tier', 'pe_tier', 'price_tier', 'overall_wave_strength' 
        ]
        
        available_cols = [col for col in export_cols if col in df.columns]
        
        export_df = df[available_cols].copy()
        
        # Convert volume ratios back to percentage for display if they were ratios initially
        # Only process columns that are explicitly volume ratios in CONFIG
        for col_name in CONFIG.VOLUME_RATIO_COLUMNS:
            if col_name in export_df.columns:
                export_df[col_name] = (export_df[col_name] - 1) * 100
                
        # Fill NaN values in numeric columns with empty strings for cleaner CSV display
        for col in export_df.select_dtypes(include=np.number).columns:
            export_df[col] = export_df[col].fillna('')

        # Fill NaN in object columns (like patterns, strings) with empty string
        for col in export_df.select_dtypes(include='object').columns:
            export_df[col] = export_df[col].fillna('')

        return export_df.to_csv(index=False)

# ============================================
# UI COMPONENTS
# ============================================

class UIComponents:
    """Reusable UI components"""
    
    @staticmethod
    def render_metric_card(label: str, value: Any, delta: Optional[str] = None, 
                          help_text: Optional[str] = None) -> None:
        """Render a styled metric card"""
        if help_text:
            st.metric(label, value, delta, help=help_text)
        else:
            st.metric(label, value, delta)
    
    @staticmethod
    def render_summary_section(df: pd.DataFrame) -> None:
        """Render enhanced summary dashboard"""
        
        if df.empty:
            st.warning("No data available for summary")
            return
        
        # 1. MARKET PULSE
        st.markdown("### ðŸ“Š Market Pulse")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            # A/D Ratio
            ad_metrics = MarketIntelligence.calculate_advance_decline_ratio(df)
            ad_ratio = ad_metrics.get('ad_ratio', 1.0)
            
            # Guard against inf ratio for delta calculation
            display_ad_ratio = f"{ad_ratio:.2f}" if ad_ratio != float('inf') else "âˆž"

            if ad_ratio > 2:
                ad_emoji = "ðŸ”¥"
            elif ad_ratio > 1:
                ad_emoji = "ðŸ“ˆ"
            else:
                ad_emoji = "ðŸ“‰"
            
            UIComponents.render_metric_card(
                "A/D Ratio",
                f"{ad_emoji} {display_ad_ratio}",
                f"{ad_metrics.get('advancing', 0)}/{ad_metrics.get('declining', 0)}",
                "Advance/Decline Ratio (Advancing stocks / Declining stocks over 1 Day)"
            )
        
        with col2:
            # Momentum Health
            # Ensure 'momentum_score' exists and handle NaNs
            if 'momentum_score' in df.columns:
                high_momentum = len(df[df['momentum_score'].fillna(0) >= 70])
                momentum_pct = (high_momentum / len(df) * 100) if len(df) > 0 else 0
            else:
                high_momentum = 0
                momentum_pct = 0
            
            UIComponents.render_metric_card(
                "Momentum Health",
                f"{momentum_pct:.0f}%",
                f"{high_momentum} strong stocks",
                "Percentage of stocks with Momentum Score â‰¥ 70."
            )
        
        with col3:
            # Volume State
            if 'rvol' in df.columns:
                avg_rvol = df['rvol'].fillna(1.0).median()
                high_vol_count = len(df[df['rvol'].fillna(0) > 2])
            else:
                avg_rvol = 1.0
                high_vol_count = 0
            
            if avg_rvol > 1.5:
                vol_emoji = "ðŸŒŠ"
            elif avg_rvol > 1.2:
                vol_emoji = "ðŸ’§"
            else:
                vol_emoji = "ðŸœï¸"
            
            UIComponents.render_metric_card(
                "Volume State",
                f"{vol_emoji} {avg_rvol:.1f}x",
                f"{high_vol_count} surges",
                "Median Relative Volume (RVOL). Surges indicate stocks with RVOL > 2x."
            )
        
        with col4:
            # Risk Level
            risk_factors = 0
            
            if 'from_high_pct' in df.columns and 'momentum_score' in df.columns:
                overextended = len(df[(df['from_high_pct'].fillna(-100) >= 0) & (df['momentum_score'].fillna(0) < 50)])
                if overextended > 20: # Arbitrary threshold, tune as needed
                    risk_factors += 1
            
            if 'rvol' in df.columns:
                pump_risk = len(df[(df['rvol'].fillna(0) > 10) & (df['master_score'].fillna(0) < 50)])
                if pump_risk > 10: # Arbitrary threshold, tune as needed
                    risk_factors += 1
            
            if 'trend_quality' in df.columns:
                downtrends = len(df[df['trend_quality'].fillna(50) < 40])
                if downtrends > len(df) * 0.3 and len(df) > 0: # More than 30% in downtrend
                    risk_factors += 1
            
            risk_levels = ["ðŸŸ¢ LOW", "ðŸŸ¡ MODERATE", "ðŸŸ  HIGH", "ðŸ”´ EXTREME"]
            risk_level = risk_levels[min(risk_factors, 3)]
            
            UIComponents.render_metric_card(
                "Risk Level",
                risk_level,
                f"{risk_factors} factors",
                "Composite risk assessment based on overextension, extreme volume, and downtrends."
            )
        
        # 2. TODAY'S OPPORTUNITIES
        st.markdown("### ðŸŽ¯ Today's Best Opportunities")
        
        opp_col1, opp_col2, opp_col3 = st.columns(3)
        
        with opp_col1:
            # Ready to Run
            ready_to_run = df[
                (df['momentum_score'].fillna(0) >= 70) & 
                (df['acceleration_score'].fillna(0) >= 70) &
                (df['rvol'].fillna(0) >= 2)
            ].nlargest(5, 'master_score', keep='first')
            
            st.markdown("**ðŸš€ Ready to Run**")
            if not ready_to_run.empty:
                for _, stock in ready_to_run.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"Score: {stock['master_score']:.1f} | RVOL: {stock.get('rvol', 0):.1f}x")
            else:
                st.info("No momentum leaders found")
        
        with opp_col2:
            # Hidden Gems
            hidden_gems = df[df['patterns'].str.contains('ðŸ’Ž HIDDEN GEM', na=False)].nlargest(5, 'master_score', keep='first')
            
            st.markdown("**ðŸ’Ž Hidden Gems**")
            if not hidden_gems.empty:
                for _, stock in hidden_gems.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"Cat %ile: {stock.get('category_percentile', 0):.0f} | Score: {stock['master_score']:.1f}")
            else:
                st.info("No hidden gems today")
        
        with opp_col3:
            # Volume Alerts
            volume_alerts = df[df['rvol'].fillna(0) > 3].nlargest(5, 'master_score', keep='first')
            
            st.markdown("**âš¡ Volume Alerts**")
            if not volume_alerts.empty:
                for _, stock in volume_alerts.iterrows():
                    st.write(f"â€¢ **{stock['ticker']}** - {stock['company_name'][:25]}")
                    st.caption(f"RVOL: {stock.get('rvol', 0):.1f}x | {stock.get('wave_state', 'N/A')}")
            else:
                st.info("No extreme volume detected")
        
        # 3. MARKET INTELLIGENCE
        st.markdown("### ðŸ§  Market Intelligence")
        
        intel_col1, intel_col2 = st.columns([2, 1])
        
        with intel_col1:
            # Sector Rotation Map
            sector_rotation = MarketIntelligence.detect_sector_rotation(df)
            
            if not sector_rotation.empty:
                fig = go.Figure()
                
                fig.add_trace(go.Bar(
                    x=sector_rotation.index[:10],  # Top 10 sectors
                    y=sector_rotation['flow_score'][:10],
                    text=[f"{val:.1f}" for val in sector_rotation['flow_score'][:10]],
                    textposition='outside',
                    marker_color=['#2ecc71' if score > 60 else '#e74c3c' if score < 40 else '#f39c12' 
                                 for score in sector_rotation['flow_score'][:10]],
                    hovertemplate=(
                        'Sector: %{x}<br>'
                        'Flow Score: %{y:.1f}<br>'
                        'Analyzed: %{customdata[0]} of %{customdata[1]} stocks<br>'
                        'Avg Score: %{customdata[2]:.1f}<br>'
                        'Median Score: %{customdata[3]:.1f}<extra></extra>'
                    ),
                    customdata=np.column_stack((
                        sector_rotation['analyzed_stocks'][:10],
                        sector_rotation['total_stocks'][:10],
                        sector_rotation['avg_score'][:10],
                        sector_rotation['median_score'][:10]
                    ))
                ))
                
                fig.update_layout(
                    title="Sector Rotation Map - Smart Money Flow (Dynamically Sampled)",
                    xaxis_title="Sector",
                    yaxis_title="Flow Score",
                    height=400,
                    template='plotly_white',
                    showlegend=False
                )
                
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No sector rotation data available for visualization.")
        
        with intel_col2:
            # Market Regime
            regime, regime_metrics = MarketIntelligence.detect_market_regime(df)
            
            st.markdown(f"**ðŸŽ¯ Market Regime**")
            st.markdown(f"### {regime}")
            
            st.markdown("**ðŸ“¡ Key Signals**")
            
            signals = []
            
            # Breadth signal
            breadth = regime_metrics.get('breadth', 0.5)
            if breadth > 0.6:
                signals.append("âœ… Strong breadth")
            elif breadth < 0.4:
                signals.append("âš ï¸ Weak breadth")
            
            # Category rotation
            category_spread = regime_metrics.get('category_spread', 0)
            if category_spread > 10:
                signals.append("ðŸ”„ Small caps leading")
            elif category_spread < -10:
                signals.append("ðŸ›¡ï¸ Large caps defensive")
            
            # Volume signal
            avg_rvol = regime_metrics.get('avg_rvol', 1.0)
            if avg_rvol > 1.5:
                signals.append("ðŸŒŠ High volume activity")
            
            # Pattern emergence
            pattern_count = (df['patterns'].fillna('') != '').sum() # Count non-empty pattern strings
            if pattern_count > len(df) * 0.2 and len(df) > 0:
                signals.append("ðŸŽ¯ Many patterns emerging")
            
            if signals:
                for signal in signals:
                    st.write(signal)
            else:
                st.info("No significant market signals detected.")
            
            # Market strength meter
            st.markdown("**ðŸ’ª Market Strength**")
            
            # Ensure components are numeric for calculation, fill NaN
            strength_score = (
                (breadth * 50) +
                (min(avg_rvol, 2) * 25) +
                ((pattern_count / len(df) if len(df) > 0 else 0) * 25)
            )
            
            if strength_score > 70:
                strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢"
            elif strength_score > 50:
                strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢âšª"
            elif strength_score > 30:
                strength_meter = "ðŸŸ¢ðŸŸ¢ðŸŸ¢âšªâšª"
            else:
                strength_meter = "ðŸŸ¢ðŸŸ¢âšªâšªâšª"
            
            st.write(strength_meter)
        
        else:
            st.warning("No data available for summary. Please adjust filters.")
    
    @staticmethod
    def render_pagination_controls(df: pd.DataFrame, display_count: int, page_key: str) -> pd.DataFrame:
        """Renders pagination controls and returns the DataFrame slice for the current page."""
        
        total_rows = len(df)
        if total_rows == 0:
            st.caption("No data to display.")
            return df
        
        # Initialize current page if not in session state
        if f'wd_current_page_{page_key}' not in st.session_state:
            st.session_state[f'wd_current_page_{page_key}'] = 0
            
        current_page = st.session_state[f'wd_current_page_{page_key}']
        
        total_pages = int(np.ceil(total_rows / display_count))
        
        start_idx = current_page * display_count
        end_idx = min(start_idx + display_count, total_rows)
        
        # Display pagination info
        st.caption(f"Showing {start_idx + 1}-{end_idx} of {total_rows} stocks (Page {current_page + 1} of {total_pages})")
        
        # Pagination buttons
        col_prev, col_page_num, col_next = st.columns([1, 0.5, 1])
        
        with col_prev:
            if st.button("â¬…ï¸ Previous Page", disabled=(current_page == 0), key=f'wd_prev_page_{page_key}'):
                st.session_state[f'wd_current_page_{page_key}'] -= 1
                st.rerun()
        with col_page_num:
            # Optionally allow direct page input
            pass # Keep blank for now, or add st.number_input for direct page jump
        with col_next:
            if st.button("Next Page âž¡ï¸", disabled=(current_page >= total_pages - 1), key=f'wd_next_page_{page_key}'):
                st.session_state[f'wd_current_page_{page_key}'] += 1
                st.rerun()
        
        return df.iloc[start_idx:end_idx]

# ============================================
# SESSION STATE MANAGER
# ============================================

class SessionStateManager:
    """Manage session state properly"""
    
    @staticmethod
    def initialize():
        """Initialize all session state variables with explicit defaults."""
        
        defaults = {
            'wd_search_query': "",
            'last_refresh': datetime.now(timezone.utc),
            'data_source': "sheet",
            'user_preferences': {
                'default_top_n': CONFIG.DEFAULT_TOP_N,
                'display_mode': 'Technical',
                'last_filters': {}
            },
            'filters': {},
            'active_filter_count': 0,
            'quick_filter': None,
            'wd_quick_filter_applied': False, # Renamed for consistency
            'wd_show_debug': False, # Renamed for consistency
            'performance_metrics': {},
            'data_quality': {},
            'wd_trigger_clear': False, # Renamed for consistency

            # Explicit Initialization for all filter-related keys (NEW/IMPROVED)
            # These ensure consistency and prevent 'None' or unexpected types in st.session_state
            # if they were not explicitly initialized elsewhere or via a widget interaction.
            'wd_category_filter': [],
            'wd_sector_filter': [],
            'wd_industry_filter': [], # New industry filter state
            'wd_min_score': 0,
            'wd_patterns': [],
            'wd_trend_filter': "All Trends", # Default string value for selectbox
            'wd_eps_tier_filter': [],
            'wd_pe_tier_filter': [],
            'wd_price_tier_filter': [],
            'wd_min_eps_change': "", # Text input default
            'wd_min_pe': "", # Text input default
            'wd_max_pe': "", # Text input default
            'wd_require_fundamental_data': False, # Checkbox default
            'wd_wave_states_filter': [], # New multiselect filter
            'wd_wave_strength_range_slider': (0, 100), # New slider filter default
            'wd_show_sensitivity_details': False, # Wave Radar checkbox
            'wd_show_market_regime': True, # Wave Radar checkbox
            'wd_wave_timeframe_select': "All Waves", # Wave Radar selectbox
            'wd_wave_sensitivity': "Balanced", # Wave Radar select slider
            'user_spreadsheet_id': None, # Stores the validated GID from user input
            'wd_current_page_rankings': 0 # For pagination on rankings tab
        }
        
        for key, default_value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
    
    @staticmethod
    def clear_filters():
        """Clear all filter states properly, resetting to their initial defaults."""
        
        # Reset all filter-related session state keys.
        filter_keys = [
            'wd_category_filter', 'wd_sector_filter', 'wd_industry_filter', 
            'wd_pe_tier_filter', 'wd_price_tier_filter', 'wd_patterns',
            'wd_min_score', 'wd_trend_filter', 'wd_min_eps_change',
            'wd_min_pe', 'wd_max_pe', 'wd_require_fundamental_data',
            'quick_filter', 'wd_quick_filter_applied', # Quick filters are tied to state
            'wd_wave_states_filter', 
            'wd_wave_strength_range_slider', 
            'wd_show_sensitivity_details', 
            'wd_show_market_regime', 
            'wd_wave_timeframe_select', 
            'wd_wave_sensitivity', 
        ]
        
        for key in filter_keys:
            if key in st.session_state:
                if isinstance(st.session_state[key], list):
                    st.session_state[key] = []
                elif isinstance(st.session_state[key], bool):
                    st.session_state[key] = False
                elif isinstance(st.session_state[key], str):
                    if key == 'wd_trend_filter': 
                        st.session_state[key] = "All Trends"
                    elif key == 'wd_wave_timeframe_select':
                        st.session_state[key] = "All Waves"
                    elif key == 'wd_wave_sensitivity':
                        st.session_state[key] = "Balanced"
                    else: 
                        st.session_state[key] = ""
                elif isinstance(st.session_state[key], tuple): 
                    if key == 'wd_wave_strength_range_slider':
                        st.session_state[key] = (0, 100)
                    else: 
                        st.session_state[key] = None # Should not happen for pre-defined tuples
                elif isinstance(st.session_state[key], (int, float)):
                    if key == 'wd_min_score':
                        st.session_state[key] = 0
                    else: 
                        st.session_state[key] = 0 
                else: 
                    st.session_state[key] = None
        
        # Reset pagination for rankings tab
        st.session_state['wd_current_page_rankings'] = 0

        # Reset filter dictionaries
        st.session_state.filters = {}
        st.session_state.active_filter_count = 0
        st.session_state.wd_trigger_clear = False # Ensure this is reset after being triggered

# ============================================
# MAIN APPLICATION
# ============================================

def main():
    """Main Streamlit application - Final Production Version"""
    
    # Page configuration
    st.set_page_config(
        page_title="Wave Detection Ultimate 3.0",
        page_icon="ðŸŒŠ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Initialize session state
    SessionStateManager.initialize()
    
    # Custom CSS for production UI
    st.markdown("""
    <style>
    /* Production-ready CSS */
    .main {padding: 0rem 1rem;}
    .stTabs [data-baseweb="tab-list"] {gap: 8px;}
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
    div[data-testid="metric-container"] {
        background-color: rgba(28, 131, 225, 0.1);
        border: 1px solid rgba(28, 131, 225, 0.2);
        padding: 5% 5% 5% 10%;
        border-radius: 5px;
        overflow-wrap: break-word;
    }
    .stAlert {
        padding: 1rem;
        border-radius: 5px;
    }
    /* Button styling */
    div.stButton > button {
        width: 100%;
        transition: all 0.3s ease;
    }
    div.stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 10px rgba(0,0,0,0.2);
    }
    /* Mobile responsive */
    @media (max-width: 768px) {
        .stDataFrame {font-size: 12px;}
        div[data-testid="metric-container"] {padding: 3%;}
        .main {padding: 0rem 0.5rem;}
    }
    /* Table optimization */
    .stDataFrame > div {overflow-x: auto;}
    /* Loading animation */
    .stSpinner > div {
        border-color: #3498db;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    ">
        <h1 style="margin: 0; font-size: 2.5rem;">ðŸŒŠ Wave Detection Ultimate 3.0</h1>
        <p style="margin: 0.5rem 0 0 0; opacity: 0.9;">
            Professional Stock Ranking System â€¢ Final Production Version
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar configuration
    with st.sidebar:
        st.markdown("### ðŸŽ¯ Quick Actions")
        
        # Control buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ðŸ”„ Refresh Data", type="primary", use_container_width=True, key="wd_refresh_data_button"):
                st.cache_data.clear()
                st.session_state.last_refresh = datetime.now(timezone.utc)
                st.rerun()
        
        with col2:
            if st.button("ðŸ§¹ Clear Cache", use_container_width=True, key="wd_clear_cache_button"):
                st.cache_data.clear()
                gc.collect()  # Force garbage collection
                st.success("Cache cleared!")
                time.sleep(0.5)
                st.rerun()
        
        # Data source selection - Two prominent buttons
        st.markdown("---")
        st.markdown("### ðŸ“‚ Data Source")
        
        data_source_col1, data_source_col2 = st.columns(2)
        with data_source_col1:
            if st.button("ðŸ“Š Google Sheets", type="primary" if st.session_state.data_source == "sheet" else "secondary", use_container_width=True, key="wd_sheets_button"):
                st.session_state.data_source = "sheet"
                st.rerun()
        with data_source_col2:
            if st.button("ðŸ“ Upload CSV", type="primary" if st.session_state.data_source == "upload" else "secondary", use_container_width=True, key="wd_upload_button"):
                st.session_state.data_source = "upload"
                st.rerun()

        uploaded_file = None
        if st.session_state.data_source == "upload":
            uploaded_file = st.file_uploader(
                "Choose CSV file", 
                type="csv",
                help="Upload a CSV file with stock data. Must contain 'ticker' and 'price' columns.",
                key="wd_csv_uploader"
            )
            if uploaded_file is None:
                st.info("Please upload a CSV file to continue")
        
        # Google Sheet ID input for dynamic data source
        if st.session_state.data_source == "sheet":
            st.markdown("#### ðŸ”— Google Sheet Configuration")
            # Use current GID from session state for default value in text_input
            # If st.session_state.user_spreadsheet_id is None, default to an empty string
            current_gid_input_value = st.session_state.get('user_spreadsheet_id', '') or "" # Starts as empty string if None
            
            user_gid_input_widget = st.text_input(
                "Enter Google Spreadsheet ID:",
                value=current_gid_input_value,
                placeholder=f"e.g., 1OEQ_qxL4lXbO9LlKWDGlDju2yQC1iYvOYeXF3mTQuJM",
                help="The unique ID from your Google Sheet URL (the part after '/d/' and before '/edit/'). This is typically 44 characters, alphanumeric.",
                key="wd_user_gid_input" # Unique key for the widget
            )

            # Process and validate user input from the widget
            # Check if the value from the widget has changed compared to what's stored in session state
            new_id_from_widget = st.session_state.wd_user_gid_input.strip()
            
            # This flag controls if a rerun is needed due to GID change
            trigger_gid_rerun = False

            if new_id_from_widget != st.session_state.get('user_spreadsheet_id', ''): # Compare with stored GID
                if not new_id_from_widget: # User cleared input field
                    if st.session_state.get('user_spreadsheet_id') is not None: # If there was previously a custom ID
                        st.session_state.user_spreadsheet_id = None # Explicitly clear custom ID
                        st.info("Spreadsheet ID cleared. Using default.")
                        trigger_gid_rerun = True
                elif len(new_id_from_widget) == 44 and new_id_from_widget.isalnum(): # Valid 44-char alphanumeric format
                    if st.session_state.get('user_spreadsheet_id') != new_id_from_widget: # Only update if actually changed
                        st.session_state.user_spreadsheet_id = new_id_from_widget
                        st.success("Spreadsheet ID updated. Reloading data...")
                        trigger_gid_rerun = True
                else: # Invalid format, non-empty input
                    st.error("Invalid Spreadsheet ID format. Please enter a 44-character alphanumeric ID.")
                    # IMPORTANT: Do NOT set st.session_state.user_spreadsheet_id here.
                    # It should retain its previous (valid) state, or remain None.
                    # The data loading logic (load_and_process_data) will handle the absence/invalidity.
            
            if trigger_gid_rerun:
                st.rerun()

        # Data quality indicator
        if st.session_state.data_quality:
            with st.expander("ðŸ“Š Data Quality", expanded=True):
                quality = st.session_state.data_quality
                
                col1, col2 = st.columns(2)
                with col1:
                    completeness = quality.get('completeness', 0)
                    if completeness > 80:
                        emoji = "ðŸŸ¢"
                    elif completeness > 60:
                        emoji = "ðŸŸ¡"
                    else:
                        emoji = "ðŸ”´"
                    
                    st.metric("Completeness", f"{emoji} {completeness:.1f}%")
                    st.metric("Total Stocks", f"{quality.get('total_rows', 0):,}")
                
                with col2:
                    if 'timestamp' in quality:
                        age = datetime.now(timezone.utc) - quality['timestamp']
                        minutes = int(age.total_seconds() / 60)
                        
                        if minutes < 60:
                            freshness = "ðŸŸ¢ Fresh"
                        elif minutes < 24 * 60:
                            freshness = "ðŸŸ¡ Recent"
                        else:
                            freshness = "ðŸ”´ Stale"
                        
                        st.metric("Data Age", freshness)
                    
                    duplicates = quality.get('duplicate_tickers', 0)
                    if duplicates > 0:
                        st.metric("Duplicates", f"âš ï¸ {duplicates}")
        
        # Performance metrics
        if st.session_state.performance_metrics:
            with st.expander("âš¡ Performance"):
                perf = st.session_state.performance_metrics
                
                total_time = sum(perf.values())
                if total_time < 3:
                    perf_emoji = "ðŸŸ¢"
                elif total_time < 5:
                    perf_emoji = "ðŸŸ¡"
                else:
                    perf_emoji = "ðŸ”´"
                
                st.metric("Load Time", f"{perf_emoji} {total_time:.1f}s")
                
                # Show slowest operations
                if len(perf) > 0:
                    slowest = sorted(perf.items(), key=lambda x: x[1], reverse=True)[:3]
                    for func_name, elapsed in slowest:
                        if elapsed > 0.001: 
                            st.caption(f"{func_name}: {elapsed:.4f}s")
        
        st.markdown("---")
        st.markdown("### ðŸ” Smart Filters")
        
        # Count active filters
        active_filter_count = 0
        
        if st.session_state.get('wd_quick_filter_applied', False):
            active_filter_count += 1
        
        # Check all filter states
        filter_checks = [
            ('wd_category_filter', lambda x: x and len(x) > 0),
            ('wd_sector_filter', lambda x: x and len(x) > 0),
            ('wd_industry_filter', lambda x: x and len(x) > 0), 
            ('wd_min_score', lambda x: x > 0),
            ('wd_patterns', lambda x: x and len(x) > 0),
            ('wd_trend_filter', lambda x: x != 'All Trends'),
            ('wd_eps_tier_filter', lambda x: x and len(x) > 0),
            ('wd_pe_tier_filter', lambda x: x and len(x) > 0),
            ('wd_price_tier_filter', lambda x: x and len(x) > 0),
            ('wd_min_eps_change', lambda x: x is not None and str(x).strip() != ''),
            ('wd_min_pe', lambda x: x is not None and str(x).strip() != ''),
            ('wd_max_pe', lambda x: x is not None and str(x).strip() != ''),
            ('wd_require_fundamental_data', lambda x: x),
            ('wd_wave_states_filter', lambda x: x and len(x) > 0),
            ('wd_wave_strength_range_slider', lambda x: x != (0, 100))
        ]
        
        for key, check_func in filter_checks:
            if key in st.session_state and check_func(st.session_state[key]):
                active_filter_count += 1
        
        st.session_state.active_filter_count = active_filter_count
        
        # Show active filter count
        if active_filter_count > 0:
            st.info(f"ðŸ” **{active_filter_count} filter{'s' if active_filter_count > 1 else ''} active**")
        
        # Clear filters button
        if st.button("ðŸ—‘ï¸ Clear All Filters", 
                    use_container_width=True, 
                    type="primary" if active_filter_count > 0 else "secondary",
                    key="wd_clear_all_filters_button"): # Unique key
            SessionStateManager.clear_filters()
            st.success("âœ… All filters cleared!")
            st.rerun()
        
        # Debug mode
        st.markdown("---")
        show_debug = st.checkbox("ðŸ› Show Debug Info", 
                               value=st.session_state.get('wd_show_debug', False),
                               key="wd_show_debug") # Unique key for debug checkbox
    
    # --- Initial Data Load Stop (Crucial for Sheet Mode) ---
    if st.session_state.data_source == "sheet":
        if st.session_state.get('user_spreadsheet_id') is None:
            # If user_spreadsheet_id is None AND it's sheet mode, stop and ask for input
            st.warning("Please enter your Google Spreadsheet ID in the sidebar to load data. Using default ID if input is empty.")
            # Do not st.stop() here as the default GID will be picked up by load_and_process_data if the user input is empty.
            # Only st.stop if load_and_process_data throws a critical validation error.

    # Data loading and processing
    try:
        # Determine which GID to use: user input (from session_state.user_spreadsheet_id) or default
        # If user cleared their input, user_spreadsheet_id will be None. In that case, use CONFIG.DEFAULT_GID.
        active_gid_for_load = st.session_state.get('user_spreadsheet_id') or CONFIG.DEFAULT_GID
        
        # Generate a unique cache key based on GID and date for daily invalidation
        # and a hash of the GID itself for more robust invalidation if the GID changes
        gid_hash = hashlib.md5(active_gid_for_load.encode()).hexdigest()
        cache_data_version = f"{datetime.now(timezone.utc).strftime('%Y%m%d')}_{gid_hash}"

        # Check if we need to load data from uploaded file
        if st.session_state.data_source == "upload" and uploaded_file is None:
            st.warning("Please upload a CSV file to continue")
            st.stop() # Stop if upload mode and no file
        
        # Load and process data
        with st.spinner("ðŸ“¥ Loading and processing data..."):
            try:
                if st.session_state.data_source == "upload" and uploaded_file is not None:
                    ranked_df, data_timestamp, metadata = load_and_process_data(
                        "upload", file_data=uploaded_file, data_version=cache_data_version
                    )
                else:
                    ranked_df, data_timestamp, metadata = load_and_process_data(
                        "sheet", data_version=cache_data_version # GID is now derived internally from session_state
                    )
                
                st.session_state.ranked_df = ranked_df
                st.session_state.data_timestamp = data_timestamp
                st.session_state.last_refresh = datetime.now(timezone.utc)
                
                # Show any warnings or errors
                if metadata.get('warnings'):
                    for warning in metadata['warnings']:
                        st.warning(warning)
                
                if metadata.get('errors'):
                    for error in metadata['errors']:
                        st.error(error)
                
            except ValueError as ve:
                logger.error(f"Data validation or loading setup error: {str(ve)}")
                st.error(f"âŒ Data Configuration Error: {str(ve)}")
                st.info("Please ensure your Google Spreadsheet ID is correct and accessible.")
                st.stop() # Stop on critical validation errors from load_and_process_data
            except Exception as e:
                logger.error(f"Failed to load data: {str(e)}")
                
                # Try to use last good data
                if 'last_good_data' in st.session_state:
                    ranked_df, data_timestamp, metadata = st.session_state.last_good_data
                    st.warning("Failed to load fresh data, using cached version.")
                    st.warning(f"Error during load: {str(e)}")
                else:
                    st.error(f"âŒ Error: {str(e)}")
                    st.info("Common issues:\n- Network connectivity\n- Google Sheets permissions\n- Invalid CSV format or GID not found.")
                    st.stop() # If no cached data and load fails, stop.
        
    except Exception as e:
        st.error(f"âŒ Critical Application Error: {str(e)}")
        with st.expander("ðŸ” Error Details"):
            st.code(str(e))
        st.stop()
    
    # Quick Action Buttons
    st.markdown("### âš¡ Quick Actions")
    qa_col1, qa_col2, qa_col3, qa_col4, qa_col5 = st.columns(5)
    
    # Check for quick filter state
    quick_filter_applied = st.session_state.get('wd_quick_filter_applied', False)
    quick_filter = st.session_state.get('quick_filter', None) # No 'wd_' prefix for this one as it's not directly a widget key
    
    with qa_col1:
        if st.button("ðŸ“ˆ Top Gainers", use_container_width=True, key="wd_qa_top_gainers"):
            st.session_state['quick_filter'] = 'top_gainers'
            st.session_state['wd_quick_filter_applied'] = True
            st.rerun()
    
    with qa_col2:
        if st.button("ðŸ”¥ Volume Surges", use_container_width=True, key="wd_qa_volume_surges"):
            st.session_state['quick_filter'] = 'volume_surges'
            st.session_state['wd_quick_filter_applied'] = True
            st.rerun()
    
    with qa_col3:
        if st.button("ðŸŽ¯ Breakout Ready", use_container_width=True, key="wd_qa_breakout_ready"):
            st.session_state['quick_filter'] = 'breakout_ready'
            st.session_state['wd_quick_filter_applied'] = True
            st.rerun()
    
    with qa_col4:
        if st.button("ðŸ’Ž Hidden Gems", use_container_width=True, key="wd_qa_hidden_gems"):
            st.session_state['quick_filter'] = 'hidden_gems'
            st.session_state['wd_quick_filter_applied'] = True
            st.rerun()
    
    with qa_col5:
        if st.button("ðŸŒŠ Show All", use_container_width=True, key="wd_qa_show_all"):
            st.session_state['quick_filter'] = None
            st.session_state['wd_quick_filter_applied'] = False
            st.rerun()
    
    # Apply quick filters
    if quick_filter and ranked_df is not None and not ranked_df.empty:
        if quick_filter == 'top_gainers':
            # Ensure column exists and handle NaNs
            if 'momentum_score' in ranked_df.columns:
                ranked_df_display = ranked_df[ranked_df['momentum_score'].fillna(0) >= 80]
                st.info(f"Showing {len(ranked_df_display)} stocks with momentum score â‰¥ 80")
            else:
                ranked_df_display = ranked_df.copy() # No filter applied if column missing
                st.warning("Momentum score data not available for 'Top Gainers' quick filter.")
        elif quick_filter == 'volume_surges':
            if 'rvol' in ranked_df.columns:
                ranked_df_display = ranked_df[ranked_df['rvol'].fillna(0) >= 3]
                st.info(f"Showing {len(ranked_df_display)} stocks with RVOL â‰¥ 3x")
            else:
                ranked_df_display = ranked_df.copy()
                st.warning("RVOL data not available for 'Volume Surges' quick filter.")
        elif quick_filter == 'breakout_ready':
            if 'breakout_score' in ranked_df.columns:
                ranked_df_display = ranked_df[ranked_df['breakout_score'].fillna(0) >= 80]
                st.info(f"Showing {len(ranked_df_display)} stocks with breakout score â‰¥ 80")
            else:
                ranked_df_display = ranked_df.copy()
                st.warning("Breakout score data not available for 'Breakout Ready' quick filter.")
        elif quick_filter == 'hidden_gems':
            if 'patterns' in ranked_df.columns:
                ranked_df_display = ranked_df[ranked_df['patterns'].str.contains('ðŸ’Ž HIDDEN GEM', na=False)]
                st.info(f"Showing {len(ranked_df_display)} hidden gem stocks")
            else:
                ranked_df_display = ranked_df.copy()
                st.warning("Patterns data not available for 'Hidden Gems' quick filter.")
        else:
            ranked_df_display = ranked_df.copy()
    else:
        ranked_df_display = ranked_df.copy()
    
    # Sidebar filters
    with st.sidebar:
        # Initialize filters dict (populated from session state for persistence)
        filters = {}
        
        # Display Mode
        st.markdown("### ðŸ“Š Display Mode")
        display_mode = st.radio(
            "Choose your view:",
            options=["Technical", "Hybrid (Technical + Fundamentals)"],
            index=0 if st.session_state.user_preferences['display_mode'] == 'Technical' else 1,
            help="Technical: Pure momentum analysis | Hybrid: Adds PE & EPS data",
            key="wd_display_mode_toggle" # Unique key
        )
        
        st.session_state.user_preferences['display_mode'] = display_mode
        show_fundamentals = (display_mode == "Hybrid (Technical + Fundamentals)")
        
        st.markdown("---")
        
        # Category filter
        categories_options = FilterEngine.get_filter_options(ranked_df_display, 'category', filters)
        
        selected_categories = st.multiselect(
            "Market Cap Category",
            options=categories_options,
            default=st.session_state.get('wd_category_filter', []), # Persist filter state
            placeholder="Select categories (empty = All)",
            key="wd_category_filter" # Unique key
        )
        
        filters['categories'] = selected_categories
        
        # Sector filter
        sectors_options = FilterEngine.get_filter_options(ranked_df_display, 'sector', filters)
        
        selected_sectors = st.multiselect(
            "Sector",
            options=sectors_options,
            default=st.session_state.get('wd_sector_filter', []), # Persist filter state
            placeholder="Select sectors (empty = All)",
            key="wd_sector_filter" # Unique key
        )
        
        filters['sectors'] = selected_sectors

        # Industry filter
        industries_options = FilterEngine.get_filter_options(ranked_df_display, 'industry', filters)
        
        selected_industries = st.multiselect(
            "Industry",
            options=industries_options,
            default=st.session_state.get('wd_industry_filter', []), # Persist filter state
            placeholder="Select industries (empty = All)",
            key="wd_industry_filter" # Unique key
        )
        filters['industries'] = selected_industries
        
        # Score filter
        filters['min_score'] = st.slider(
            "Minimum Master Score",
Â  Â  Â  Â  Â  Â  min_value=0,
Â  Â  Â  Â  Â  Â  max_value=100,
Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_min_score', 0),
Â  Â  Â  Â  Â  Â  step=5,
Â  Â  Â  Â  Â  Â  help="Filter stocks by minimum score",
Â  Â  Â  Â  Â  Â  key="wd_min_score"
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Pattern filter
Â  Â  Â  Â  all_patterns = set()
Â  Â  Â  Â  for patterns_str in ranked_df_display['patterns'].dropna():
Â  Â  Â  Â  Â  Â  if patterns_str:
Â  Â  Â  Â  Â  Â  Â  Â  all_patterns.update(patterns_str.split(' | '))
Â  Â  Â  Â Â 
Â  Â  Â  Â  if all_patterns:
Â  Â  Â  Â  Â  Â  filters['patterns'] = st.multiselect(
Â  Â  Â  Â  Â  Â  Â  Â  "Patterns",
Â  Â  Â  Â  Â  Â  Â  Â  options=sorted(all_patterns),
Â  Â  Â  Â  Â  Â  Â  Â  default=st.session_state.get('wd_patterns', []),
Â  Â  Â  Â  Â  Â  Â  Â  placeholder="Select patterns (empty = All)",
Â  Â  Â  Â  Â  Â  Â  Â  help="Filter by specific patterns",
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_patterns"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Trend filter
Â  Â  Â  Â  st.markdown("#### ðŸ“ˆ Trend Strength")
Â  Â  Â  Â  trend_options = {
Â  Â  Â  Â  Â  Â  "All Trends": (0, 100),
Â  Â  Â  Â  Â  Â  "ðŸ”¥ Strong Uptrend (80+)": (80, 100),
Â  Â  Â  Â  Â  Â  "âœ… Good Uptrend (60-79)": (60, 79),
Â  Â  Â  Â  Â  Â  "âž¡ï¸ Neutral Trend (40-59)": (40, 59),
Â  Â  Â  Â  Â  Â  "âš ï¸ Weak/Downtrend (<40)": (0, 39)
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  # SAFELY get index for trend_filter
Â  Â  Â  Â  default_trend_key = st.session_state.get('wd_trend_filter', "All Trends")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  current_trend_index = list(trend_options.keys()).index(default_trend_key)
Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  logger.warning(f"Invalid trend_filter state '{default_trend_key}' found in session_state, defaulting to 'All Trends'.")
Â  Â  Â  Â  Â  Â  current_trend_index = 0 # Default to 'All Trends' (first option)

filters['trend_filter'] = st.selectbox(
Â  Â  Â  Â  Â  Â  "Trend Quality",
Â  Â  Â  Â  Â  Â  options=list(trend_options.keys()),
Â  Â  Â  Â  Â  Â  index=current_trend_index,
Â  Â  Â  Â  Â  Â  key="wd_trend_filter",
Â  Â  Â  Â  Â  Â  help="Filter stocks by trend strength based on SMA alignment"
Â  Â  Â  Â  )
Â  Â  Â  Â  filters['trend_range'] = trend_options[filters['trend_filter']]

Â  Â  Â  Â  # Wave Filters
Â  Â  Â  Â  st.markdown("#### ðŸŒŠ Wave Filters")
Â  Â  Â  Â  wave_states_options = FilterEngine.get_filter_options(ranked_df_display, 'wave_state', filters)
Â  Â  Â  Â  filters['wave_states'] = st.multiselect(
Â  Â  Â  Â  Â  Â  "Wave State",
Â  Â  Â  Â  Â  Â  options=wave_states_options,
Â  Â  Â  Â  Â  Â  default=st.session_state.get('wd_wave_states_filter', []),
Â  Â  Â  Â  Â  Â  placeholder="Select wave states (empty = All)",
Â  Â  Â  Â  Â  Â  help="Filter by the detected 'Wave State'",
Â  Â  Â  Â  Â  Â  key="wd_wave_states_filter"
Â  Â  Â  Â  )

Â  Â  Â  Â  if 'overall_wave_strength' in ranked_df_display.columns:
Â  Â  Â  Â  Â  Â  # Slider bounds are fixed (0-100) for consistent UX
Â  Â  Â  Â  Â  Â  slider_min_val = 0
Â  Â  Â  Â  Â  Â  slider_max_val = 100
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Retrieve stored value for slider, default to full range
Â  Â  Â  Â  Â  Â  current_slider_value = st.session_state.get('wd_wave_strength_range_slider', (slider_min_val, slider_max_val))
Â  Â  Â  Â  Â  Â  # Ensure the stored value is within the fixed min/max boundaries
Â  Â  Â  Â  Â  Â  current_slider_value = (
Â  Â  Â  Â  Â  Â  Â  Â  max(slider_min_val, min(slider_max_val, current_slider_value[0])),
Â  Â  Â  Â  Â  Â  Â  Â  max(slider_min_val, min(slider_max_val, current_slider_value[1]))
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  filters['wave_strength_range'] = st.slider(
Â  Â  Â  Â  Â  Â  Â  Â  "Overall Wave Strength",
Â  Â  Â  Â  Â  Â  Â  Â  min_value=slider_min_val,
Â  Â  Â  Â  Â  Â  Â  Â  max_value=slider_max_val,
Â  Â  Â  Â  Â  Â  Â  Â  value=current_slider_value,
Â  Â  Â  Â  Â  Â  Â  Â  step=1,
Â  Â  Â  Â  Â  Â  Â  Â  help="Filter by the calculated 'Overall Wave Strength' score",
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_wave_strength_range_slider"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  filters['wave_strength_range'] = (0, 100) # Default to full range if column is missing
Â  Â  Â  Â  Â  Â  st.info("Overall Wave Strength data not available.")

Â  Â  Â  Â Â 
Â  Â  Â  Â  # Advanced filters
Â  Â  Â  Â  with st.expander("ðŸ”§ Advanced Filters"):
Â  Â  Â  Â  Â  Â  # Tier filters
Â  Â  Â  Â  Â  Â  for tier_type, col_name in [
Â  Â  Â  Â  Â  Â  Â  Â  ('eps_tiers', 'eps_tier'),
Â  Â  Â  Â  Â  Â  Â  Â  ('pe_tiers', 'pe_tier'),
Â  Â  Â  Â  Â  Â  Â  Â  ('price_tiers', 'price_tier')
Â  Â  Â  Â  Â  Â  ]:
Â  Â  Â  Â  Â  Â  Â  Â  if col_name in ranked_df_display.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tier_options = FilterEngine.get_filter_options(ranked_df_display, col_name, filters)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  selected_tiers = st.multiselect(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{col_name.replace('_', ' ').title()}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  options=tier_options,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  default=st.session_state.get(f'wd_{col_name}_filter', []), # Persist filter state
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  placeholder=f"Select {col_name.replace('_', ' ')}s (empty = All)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key=f"wd_{col_name}_filter" # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters[tier_type] = selected_tiers
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # EPS change filter
Â  Â  Â  Â  Â  Â  if 'eps_change_pct' in ranked_df_display.columns:
Â  Â  Â  Â  Â  Â  Â  Â  eps_change_input = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Min EPS Change %",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_min_eps_change', ""),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  placeholder="e.g. -50 or leave empty",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="Enter minimum EPS growth percentage",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_min_eps_change" # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if eps_change_input.strip():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['min_eps_change'] = float(eps_change_input)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("Please enter a valid number for EPS change")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['min_eps_change'] = None
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['min_eps_change'] = None
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # PE filters (only in hybrid mode)
Â  Â  Â  Â  Â  Â  if show_fundamentals and 'pe' in ranked_df_display.columns:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ” Fundamental Filters**")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  col1, col2 = st.columns(2)
Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_pe_input = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Min PE Ratio",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_min_pe', ""),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  placeholder="e.g. 10",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_min_pe" # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if min_pe_input.strip():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['min_pe'] = float(min_pe_input)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("Invalid Min PE")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['min_pe'] = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['min_pe'] = None
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_pe_input = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Max PE Ratio",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_max_pe', ""),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  placeholder="e.g. 30",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_max_pe" # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if max_pe_input.strip():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['max_pe'] = float(max_pe_input)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("Invalid Max PE")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['max_pe'] = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filters['max_pe'] = None
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Data completeness filter
Â  Â  Â  Â  Â  Â  Â  Â  filters['require_fundamental_data'] = st.checkbox(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Only show stocks with PE and EPS data",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_require_fundamental_data', False), # Retain state
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_require_fundamental_data" # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â Â 
Â  Â  # Apply filters
Â  Â  if quick_filter_applied:
Â  Â  Â  Â  # Apply filters on the quick-filtered DataFrame
Â  Â  Â  Â  filtered_df = FilterEngine.apply_filters(ranked_df_display, filters)
Â  Â  else:
Â  Â  Â  Â  # Apply filters on the full ranked DataFrame
Â  Â  Â  Â  filtered_df = FilterEngine.apply_filters(ranked_df, filters)
Â  Â Â 
Â  Â  filtered_df = filtered_df.sort_values('rank')
Â  Â Â 
Â  Â  # Save current filters
Â  Â  st.session_state.user_preferences['last_filters'] = filters
Â  Â Â 
Â  Â  # Debug info
Â  Â  if show_debug:
Â  Â  Â  Â  with st.sidebar.expander("ðŸ› Debug Info", expanded=True):
Â  Â  Â  Â  Â  Â  st.write("**Active Filters:**")
Â  Â  Â  Â  Â  Â  for key, value in filters.items():
Â  Â  Â  Â  Â  Â  Â  Â  if value is not None and value != [] and \
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (not (isinstance(value, (int, float)) and value == 0)) and \
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (not (isinstance(value, str) and value == "")) and \
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (not (isinstance(value, tuple) and value == (0,100) and key == 'wave_strength_range')):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"â€¢ {key}: {value}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.write(f"\n**Filter Result:**")
Â  Â  Â  Â  Â  Â  st.write(f"Before: {len(ranked_df)} stocks")
Â  Â  Â  Â  Â  Â  st.write(f"After: {len(filtered_df)} stocks")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Display clipping counts
Â  Â  Â  Â  Â  Â  clipping_counts = DataValidator.get_clipping_counts()
Â  Â  Â  Â  Â  Â  if clipping_counts:
Â  Â  Â  Â  Â  Â  Â  Â  st.write("\n**Data Clipping Events (current session):**")
Â  Â  Â  Â  Â  Â  Â  Â  for col, count in clipping_counts.items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"â€¢ {col}: {count} values clipped")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.write("\n**Data Clipping Events:** None detected this session.")

Â  Â  Â  Â  Â  Â  if st.session_state.performance_metrics:
Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"\n**Performance:**")
Â  Â  Â  Â  Â  Â  Â  Â  for func, time_taken in st.session_state.performance_metrics.items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if time_taken > 0.001:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"â€¢ {func}: {time_taken:.4f}s")
Â  Â Â 
Â  Â  # Main content area
Â  Â  # Show filter status
Â  Â  if st.session_state.active_filter_count > 0 or quick_filter_applied:
Â  Â  Â  Â  filter_status_col1, filter_status_col2 = st.columns([5, 1])
Â  Â  Â  Â  with filter_status_col1:
Â  Â  Â  Â  Â  Â  if quick_filter:
Â  Â  Â  Â  Â  Â  Â  Â  quick_filter_names = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'top_gainers': 'ðŸ“ˆ Top Gainers',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'volume_surges': 'ðŸ”¥ Volume Surges',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'breakout_ready': 'ðŸŽ¯ Breakout Ready',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'hidden_gems': 'ðŸ’Ž Hidden Gems'
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  filter_display = quick_filter_names.get(quick_filter, 'Filtered')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.active_filter_count > 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"**Viewing:** {filter_display} + {st.session_state.active_filter_count - 1} other filter{'s' if st.session_state.active_filter_count > 2 else ''} | **{len(filtered_df):,} stocks** shown")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"**Viewing:** {filter_display} | **{len(filtered_df):,} stocks** shown")
Â  Â  Â  Â Â 
Â  Â  Â  Â  with filter_status_col2:
Â  Â  Â  Â  Â  Â  # Trigger the clear filters logic from the sidebar button
Â  Â  Â  Â  Â  Â  if st.button("Clear Filters", type="secondary", key="wd_clear_filters_main_button"): # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.wd_trigger_clear = TrueÂ 
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â Â 
Â  Â  # Summary metrics
Â  Â  col1, col2, col3, col4, col5, col6 = st.columns(6)
Â  Â Â 
Â  Â  with col1:
Â  Â  Â  Â  total_stocks = len(filtered_df)
Â  Â  Â  Â  total_original = len(ranked_df)
Â  Â  Â  Â  pct_of_all = (total_stocks/total_original*100) if total_original > 0 else 0
Â  Â  Â  Â Â 
Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  "Total Stocks",
Â  Â  Â  Â  Â  Â  f"{total_stocks:,}",
Â  Â  Â  Â  Â  Â  f"{pct_of_all:.0f}% of {total_original:,}",
Â  Â  Â  Â  Â  Â  "Total number of stocks matching current filters."
Â  Â  Â  Â  )
Â  Â Â 
Â  Â  with col2:
Â  Â  Â  Â  if not filtered_df.empty and 'master_score' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  avg_score = filtered_df['master_score'].mean()
Â  Â  Â  Â  Â  Â  std_score = filtered_df['master_score'].std()
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Avg Score",
Â  Â  Â  Â  Â  Â  Â  Â  f"{avg_score:.1f}",
Â  Â  Â  Â  Â  Â  Â  Â  f"Ïƒ={std_score:.1f}",
Â  Â  Â  Â  Â  Â  Â  Â  "Average Master Score of displayed stocks. Sigma (Ïƒ) is standard deviation."
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Avg Score", "N/A", help_text="Average Master Score not available.")
Â  Â Â 
Â  Â  with col3:
Â  Â  Â  Â  if show_fundamentals and 'pe' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  valid_pe = filtered_df['pe'].notna() & (filtered_df['pe'] > 0) & (filtered_df['pe'] < 10000)
Â  Â  Â  Â  Â  Â  pe_coverage = valid_pe.sum()
Â  Â  Â  Â  Â  Â  pe_pct = (pe_coverage / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if pe_coverage > 0:
Â  Â  Â  Â  Â  Â  Â  Â  median_pe = filtered_df.loc[valid_pe, 'pe'].median()
Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Median PE",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{median_pe:.1f}x",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{pe_pct:.0f}% have data",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Median Price-to-Earnings ratio for stocks with valid PE data."
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("PE Data", "Limited", "No PE data available for filtered stocks.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  if not filtered_df.empty and 'master_score' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  min_score = filtered_df['master_score'].min()
Â  Â  Â  Â  Â  Â  Â  Â  max_score = filtered_df['master_score'].max()
Â  Â  Â  Â  Â  Â  Â  Â  score_range = f"{min_score:.1f}-{max_score:.1f}"
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  score_range = "N/A"
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Score Range", score_range, help_text="Range of Master Scores in the current view.")
Â  Â Â 
Â  Â  with col4:
Â  Â  Â  Â  if show_fundamentals and 'eps_change_pct' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  valid_eps_change = filtered_df['eps_change_pct'].notna()
Â  Â  Â  Â  Â  Â  positive_eps_growth = valid_eps_change & (filtered_df['eps_change_pct'] > 0)
Â  Â  Â  Â  Â  Â  strong_growth = valid_eps_change & (filtered_df['eps_change_pct'] > 50)
Â  Â  Â  Â  Â  Â  mega_growth = valid_eps_change & (filtered_df['eps_change_pct'] > 100)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  growth_count = positive_eps_growth.sum()
Â  Â  Â  Â  Â  Â  strong_count = strong_growth.sum()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if mega_growth.sum() > 0:
Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "EPS Growth +ve",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{growth_count}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{strong_count} >50% | {mega_growth.sum()} >100%",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Number of stocks with positive EPS growth. Shows counts for strong (>50%) and mega (>100%) growth."
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "EPS Growth +ve",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{growth_count}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{valid_eps_change.sum()} have data",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Number of stocks with positive EPS growth. Indicates total stocks with EPS data."
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  if 'acceleration_score' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  accelerating = (filtered_df['acceleration_score'] >= 80).sum()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  accelerating = 0
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Accelerating",
Â  Â  Â  Â  Â  Â  Â  Â  f"{accelerating}",
Â  Â  Â  Â  Â  Â  Â  Â  help_text="Number of stocks with an Acceleration Score of 80 or higher."
Â  Â  Â  Â  Â  Â  )
Â  Â Â 
Â  Â  with col5:
Â  Â  Â  Â  if 'rvol' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  high_rvol = (filtered_df['rvol'] > 2).sum()
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  high_rvol = 0
Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  "High RVOL",
Â  Â  Â  Â  Â  Â  f"{high_rvol}",
Â  Â  Â  Â  Â  Â  help_text="Number of stocks with Relative Volume (RVOL) greater than 2x."
Â  Â  Â  Â  )
Â  Â Â 
Â  Â  with col6:
Â  Â  Â  Â  if 'trend_quality' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  strong_trends = (filtered_df['trend_quality'] >= 80).sum()
Â  Â  Â  Â  Â  Â  total = len(filtered_df)
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Strong Trends",Â 
Â  Â  Â  Â  Â  Â  Â  Â  f"{strong_trends}",
Â  Â  Â  Â  Â  Â  Â  Â  f"{strong_trends/total*100:.0f}%" if total > 0 else "0%",
Â  Â  Â  Â  Â  Â  Â  Â  "Number and percentage of stocks with a Trend Quality score of 80 or higher."
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  with_patterns = (filtered_df['patterns'] != '').sum()
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "With Patterns",
Â  Â  Â  Â  Â  Â  Â  Â  f"{with_patterns}",
Â  Â  Â  Â  Â  Â  Â  Â  help_text="Number of stocks currently displaying one or more detected patterns."
Â  Â  Â  Â  Â  Â  )
Â  Â Â 
Â  Â  # Main tabs
Â  Â  tabs = st.tabs([
Â  Â  Â  Â  "ðŸ“Š Summary", "ðŸ† Rankings", "ðŸŒŠ Wave Radar", "ðŸ“Š Analysis", "ðŸ” Search", "ðŸ“¥ Export", "â„¹ï¸ About"
Â  Â  ])
Â  Â Â 
Â  Â  # Tab 0: Summary - Enhanced
Â  Â  with tabs[0]:
Â  Â  Â  Â  st.markdown("### ðŸ“Š Executive Summary Dashboard")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not filtered_df.empty:
Â  Â  Â  Â  Â  Â  # Render the enhanced summary section
Â  Â  Â  Â  Â  Â  UIComponents.render_summary_section(filtered_df)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Download section
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸ’¾ Download Clean Processed Data")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  download_cols = st.columns(3)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with download_cols[0]:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ“Š Current View Data**")
Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"Includes {len(filtered_df)} stocks matching current filters")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  csv_filtered = ExportEngine.create_csv_export(filtered_df)
Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download Filtered Data (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_filtered,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"wave_detection_filtered_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="Download currently filtered stocks with all scores and indicators",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_download_filtered_csv"
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with download_cols[1]:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ† Top 100 Stocks**")
Â  Â  Â  Â  Â  Â  Â  Â  st.write("Elite stocks ranked by Master Score")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  top_100_for_download = filtered_df.nlargest(100, 'master_score', keep='first')
Â  Â  Â  Â  Â  Â  Â  Â  csv_top100 = ExportEngine.create_csv_export(top_100_for_download)
Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download Top 100 (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_top100,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"wave_detection_top100_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="Download top 100 stocks by Master Score",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_download_top100_csv"
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with download_cols[2]:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸŽ¯ Pattern Stocks Only**")
Â  Â  Â  Â  Â  Â  Â  Â  pattern_stocks = filtered_df[filtered_df['patterns'] != '']
Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"Includes {len(pattern_stocks)} stocks with patterns")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if len(pattern_stocks) > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_patterns = ExportEngine.create_csv_export(pattern_stocks)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download Pattern Stocks (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_patterns,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"wave_detection_patterns_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="Download only stocks showing patterns",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_download_patterns_csv"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("No stocks with patterns in current filter")
Â  Â  Â  Â Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning("No data available for summary. Please adjust filters.")
Â  Â Â 
Â  Â  # Tab 1: Rankings
Â  Â  with tabs[1]:
Â  Â  Â  Â  st.markdown("### ðŸ† Top Ranked Stocks")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Display options
Â  Â  Â  Â  col1, col2, col3 = st.columns([2, 2, 6])
Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  display_count = st.selectbox(
Â  Â  Â  Â  Â  Â  Â  Â  "Show top",
Â  Â  Â  Â  Â  Â  Â  Â  options=CONFIG.AVAILABLE_TOP_N,
Â  Â  Â  Â  Â  Â  Â  Â  index=CONFIG.AVAILABLE_TOP_N.index(st.session_state.user_preferences['default_top_n']),
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_rankings_display_count" # Unique key
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  st.session_state.user_preferences['default_top_n'] = display_count
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  sort_options = ['Rank', 'Master Score', 'RVOL', 'Momentum', 'Money Flow']
Â  Â  Â  Â  Â  Â  if 'trend_quality' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  sort_options.append('Trend')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  sort_by = st.selectbox(
Â  Â  Â  Â  Â  Â  Â  Â  "Sort by",
Â  Â  Â  Â  Â  Â  Â  Â  options=sort_options,
Â  Â  Â  Â  Â  Â  Â  Â  index=0,
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_rankings_sort_by" # Unique key
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Get display data - No longer head(display_count) here, but later in pagination
Â  Â  Â  Â  display_df = filtered_df.copy()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Apply sorting
Â  Â  Â  Â  if sort_by == 'Master Score':
Â  Â  Â  Â  Â  Â  display_df = display_df.sort_values('master_score', ascending=False)
Â  Â  Â  Â  elif sort_by == 'RVOL':
Â  Â  Â  Â  Â  Â  display_df = display_df.sort_values('rvol', ascending=False)
Â  Â  Â  Â  elif sort_by == 'Momentum':
Â  Â  Â  Â  Â  Â  display_df = display_df.sort_values('momentum_score', ascending=False)
Â  Â  Â  Â  elif sort_by == 'Money Flow' and 'money_flow_mm' in display_df.columns:
Â  Â  Â  Â  Â  Â  display_df = display_df.sort_values('money_flow_mm', ascending=False)
Â  Â  Â  Â  elif sort_by == 'Trend' and 'trend_quality' in display_df.columns:
Â  Â  Â  Â  Â  Â  display_df = display_df.sort_values('trend_quality', ascending=False)
Â  Â  Â  Â  # Default sort is 'Rank' (already sorted by rank before this block)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not display_df.empty:
Â  Â  Â  Â  Â  Â  # Add trend indicator
Â  Â  Â  Â  Â  Â  if 'trend_quality' in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  def get_trend_indicator(score):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.isna(score):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "âž–"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif score >= 80:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "ðŸ”¥"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif score >= 60:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "âœ…"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif score >= 40:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "âž¡ï¸"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "âš ï¸"
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  display_df['trend_indicator'] = display_df['trend_quality'].apply(get_trend_indicator)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Prepare display columns
Â  Â  Â  Â  Â  Â  display_cols = {
Â  Â  Â  Â  Â  Â  Â  Â  'rank': 'Rank',
Â  Â  Â  Â  Â  Â  Â  Â  'ticker': 'Ticker',
Â  Â  Â  Â  Â  Â  Â  Â  'company_name': 'Company',
Â  Â  Â  Â  Â  Â  Â  Â  'master_score': 'Score',
Â  Â  Â  Â  Â  Â  Â  Â  'wave_state': 'Wave'
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if 'trend_indicator' in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  display_cols['trend_indicator'] = 'Trend'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  display_cols['price'] = 'Price'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Add fundamental columns if enabled
Â  Â  Â  Â  Â  Â  if show_fundamentals:
Â  Â  Â  Â  Â  Â  Â  Â  if 'pe' in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_cols['pe'] = 'PE'
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if 'eps_change_pct' in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_cols['eps_change_pct'] = 'EPS Î”%'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Add remaining columns
Â  Â  Â  Â  Â  Â  display_cols.update({
Â  Â  Â  Â  Â  Â  Â  Â  'from_low_pct': 'From Low',
Â  Â  Â  Â  Â  Â  Â  Â  'ret_30d': '30D Ret',
Â  Â  Â  Â  Â  Â  Â  Â  'rvol': 'RVOL',
Â  Â  Â  Â  Â  Â  Â  Â  'vmi': 'VMI',
Â  Â  Â  Â  Â  Â  Â  Â  'patterns': 'Patterns',
Â  Â  Â  Â  Â  Â  Â  Â  'category': 'Category',
Â  Â  Â  Â  Â  Â  Â  Â  'sector': 'Sector',
Â  Â  Â  Â  Â  Â  Â  Â  'industry': 'Industry'
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Format numeric columns
Â  Â  Â  Â  Â  Â  format_rules = {
Â  Â  Â  Â  Â  Â  Â  Â  'master_score': '{:.1f}',
Â  Â  Â  Â  Â  Â  Â  Â  'price': 'â‚¹{:,.0f}',
Â  Â  Â  Â  Â  Â  Â  Â  'from_low_pct': '{:.0f}%',
Â  Â  Â  Â  Â  Â  Â  Â  'ret_30d': '{:+.1f}%',
Â  Â  Â  Â  Â  Â  Â  Â  'rvol': '{:.1f}x',
Â  Â  Â  Â  Â  Â  Â  Â  'vmi': '{:.2f}'
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Smart PE formatting
Â  Â  Â  Â  Â  Â  def format_pe(value):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.isna(value):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return '-'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  val = float(value)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if val <= 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return 'Loss'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif val > 10000:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return '>10K'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif val > 1000:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f"{val:.0f}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f"{val:.1f}"
Â  Â  Â  Â  Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return '-'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Smart EPS change formatting
Â  Â  Â  Â  Â  Â  def format_eps_change(value):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.isna(value):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return '-'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  val = float(value)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if abs(val) >= 1000:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f"{val/1000:+.1f}K%"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif abs(val) >= 100:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f"{val:+.0f}%"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f"{val:+.1f}%"
Â  Â  Â  Â  Â  Â  Â  Â  except:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return '-'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Apply formatting
Â  Â  Â  Â  Â  Â  for col, fmt in format_rules.items():
Â  Â  Â  Â  Â  Â  Â  Â  if col in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_df[col] = display_df[col].apply(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lambda x: fmt.format(x) if pd.notna(x) and isinstance(x, (int, float)) else '-'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Error formatting column {col}: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pass # Continue without formatting if error occurs
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Apply special formatting
Â  Â  Â  Â  Â  Â  if show_fundamentals:
Â  Â  Â  Â  Â  Â  Â  Â  if 'pe' in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_df['pe'] = display_df['pe'].apply(format_pe)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if 'eps_change_pct' in display_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_df['eps_change_pct'] = display_df['eps_change_pct'].apply(format_eps_change)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Select and rename columns for display (after all formatting)
Â  Â  Â  Â  Â  Â  available_display_cols = [c for c in display_cols.keys() if c in display_df.columns]
Â  Â  Â  Â  Â  Â  display_df = display_df[available_display_cols]
Â  Â  Â  Â  Â  Â  display_df.columns = [display_cols[c] for c in available_display_cols]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- Pagination for Rankings Table ---
Â  Â  Â  Â  Â  Â  paginated_df = UIComponents.render_pagination_controls(display_df, display_count, 'rankings')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Display with enhanced styling
Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  paginated_df,
Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True,
Â  Â  Â  Â  Â  Â  Â  Â  height=min(600, len(paginated_df) * 35 + 50), # Adjust height dynamically
Â  Â  Â  Â  Â  Â  Â  Â  hide_index=True
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Quick stats
Â  Â  Â  Â  Â  Â  with st.expander("ðŸ“Š Quick Statistics"):
Â  Â  Â  Â  Â  Â  Â  Â  stat_cols = st.columns(4)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with stat_cols[0]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Score Distribution**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'master_score' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use .dropna() before calculating stats to avoid NaN issues
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scores_data = filtered_df['master_score'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not scores_data.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Max: {scores_data.max():.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Min: {scores_data.min():.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Mean: {scores_data.mean():.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Median: {scores_data.median():.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Q1: {scores_data.quantile(0.25):.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Q3: {scores_data.quantile(0.75):.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Std: {scores_data.std():.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No valid scores.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Master Score data not available.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with stat_cols[1]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Returns (30D)**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'ret_30d' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  returns_data = filtered_df['ret_30d'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not returns_data.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Max: {returns_data.max():.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Min: {returns_data.min():.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Avg: {returns_data.mean():.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Positive: {(returns_data > 0).sum()}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No valid 30D returns.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No 30D return data available")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with stat_cols[2]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if show_fundamentals:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Fundamentals**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'pe' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  valid_pe = filtered_df['pe'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  valid_pe = valid_pe[(valid_pe > 0) & (valid_pe < 10000)]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not valid_pe.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  median_pe = valid_pe.median()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Median PE: {median_pe:.1f}x")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No valid PE.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'eps_change_pct' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  valid_eps = filtered_df['eps_change_pct'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not valid_eps.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  positive = (valid_eps > 0).sum()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Positive EPS: {positive}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No valid EPS change.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Fundamental data not available.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Volume**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'rvol' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rvol_data = filtered_df['rvol'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not rvol_data.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Max: {rvol_data.max():.1f}x")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Avg: {rvol_data.mean():.1f}x")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f">2x: {(rvol_data > 2).sum()}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No valid RVOL.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("RVOL data not available.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with stat_cols[3]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Trend Distribution**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'trend_quality' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  trend_data = filtered_df['trend_quality'].dropna()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not trend_data.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_stocks_in_filter = len(trend_data)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  avg_trend_score = trend_data.mean() if total_stocks_in_filter > 0 else 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stocks_above_all_smas = (trend_data >= 85).sum() # Roughly 'strong trend'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stocks_in_uptrend = (trend_data >= 60).sum() # Good or Strong uptrend
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stocks_in_downtrend = (trend_data < 40).sum() # Weak/Downtrend
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Avg Trend Score: {avg_trend_score:.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Above All SMAs: {stocks_above_all_smas}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"In Uptrend (60+): {stocks_in_uptrend}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"In Downtrend (<40): {stocks_in_downtrend}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No valid trend data.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("No trend data available")
Â  Â  Â  Â Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning("No stocks match the selected filters.")
Â  Â Â 
Â  Â  # Tab 2: Wave Radar - Enhanced
Â  Â  with tabs[2]:
Â  Â  Â  Â  st.markdown("### ðŸŒŠ Wave Radar - Early Momentum Detection System")
Â  Â  Â  Â  st.markdown("*Catch waves as they form, not after they've peaked!*")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Wave Radar Controls
Â  Â  Â  Â  radar_col1, radar_col2, radar_col3, radar_col4 = st.columns([2, 2, 2, 1])
Â  Â  Â  Â Â 
Â  Â  Â  Â  with radar_col1:
Â  Â  Â  Â  Â  Â  wave_timeframe = st.selectbox(
Â  Â  Â  Â  Â  Â  Â  Â  "Wave Detection Timeframe",
Â  Â  Â  Â  Â  Â  Â  Â  options=[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "All Waves",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Intraday Surge",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "3-Day Buildup",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Weekly Breakout",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Monthly Trend"
Â  Â  Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  Â  Â  index=["All Waves", "Intraday Surge", "3-Day Buildup", "Weekly Breakout", "Monthly Trend"].index(st.session_state.get('wd_wave_timeframe_select', "All Waves")), # Persist filter state
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_wave_timeframe_select", # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  help="""
Â  Â  Â  Â  Â  Â  Â  Â  ðŸŒŠ All Waves: Complete unfiltered view
Â  Â  Â  Â  Â  Â  Â  Â  âš¡ Intraday Surge: High RVOL & today's movers
Â  Â  Â  Â  Â  Â  Â  Â  ðŸ“ˆ 3-Day Buildup: Building momentum patterns
Â  Â  Â  Â  Â  Â  Â  Â  ðŸš€ Weekly Breakout: Near 52w highs with volume
Â  Â  Â  Â  Â  Â  Â  Â  ðŸ’ª Monthly Trend: Established trends with SMAs
Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  with radar_col2:
Â  Â  Â  Â  Â  Â  sensitivity = st.select_slider(
Â  Â  Â  Â  Â  Â  Â  Â  "Detection Sensitivity",
Â  Â  Â  Â  Â  Â  Â  Â  options=["Conservative", "Balanced", "Aggressive"],
Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_wave_sensitivity', "Balanced"), # Persist sensitivity
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_wave_sensitivity", # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  help="Conservative = Stronger signals, Aggressive = More signals"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Sensitivity details toggle
Â  Â  Â  Â  Â  Â  show_sensitivity_details = st.checkbox(
Â  Â  Â  Â  Â  Â  Â  Â  "Show thresholds",
Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_show_sensitivity_details', False), # Persist state
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_show_sensitivity_details", # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  help="Display exact threshold values for current sensitivity"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  with radar_col3:
Â  Â  Â  Â  Â  Â  show_market_regime = st.checkbox(
Â  Â  Â  Â  Â  Â  Â  Â  "ðŸ“Š Market Regime Analysis",
Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_show_market_regime', True), # Persist state
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_show_market_regime", # Unique key
Â  Â  Â  Â  Â  Â  Â  Â  help="Show category rotation flow and market regime detection"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Initialize wave_filtered_df
Â  Â  Â  Â  wave_filtered_df = filtered_df.copy()
Â  Â  Â  Â Â 
Â  Â  Â  Â  with radar_col4:
Â  Â  Â  Â  Â  Â  # Calculate Wave Strength
Â  Â  Â  Â  Â  Â  if not wave_filtered_df.empty and 'overall_wave_strength' in wave_filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use fillna(0) for mean calculation to avoid NaN propagating to score
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_strength_score = wave_filtered_df['overall_wave_strength'].fillna(0).mean()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if wave_strength_score > 70:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_emoji = "ðŸŒŠðŸ”¥"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_color_delta = "ðŸŸ¢"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif wave_strength_score > 50:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_emoji = "ðŸŒŠ"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_color_delta = "ðŸŸ¡"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_emoji = "ðŸ’¤"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_color_delta = "ðŸ”´"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Wave Strength",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{wave_emoji} {wave_strength_score:.0f}%",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{wave_color_delta} Market",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Overall strength of wave signals in the current filtered dataset. Reflects market trend bias."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Error calculating wave strength: {str(e)}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Wave Strength", "N/A", "Error calculating strength.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Wave Strength", "N/A", "Data not available for strength calculation.")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Display sensitivity thresholds if enabled
Â  Â  Â  Â  if show_sensitivity_details:
Â  Â  Â  Â  Â  Â  with st.expander("ðŸ“Š Current Sensitivity Thresholds", expanded=True):
Â  Â  Â  Â  Â  Â  Â  Â  if sensitivity == "Conservative":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **Conservative Settings** ðŸ›¡ï¸
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Momentum Shifts:** Score â‰¥ 60, Acceleration â‰¥ 70
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Emerging Patterns:** Within 5% of qualifying threshold
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Volume Surges:** RVOL â‰¥ 3.0x (extreme volumes only)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Acceleration Alerts:** Score â‰¥ 85 (strongest signals)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Pattern Distance:** 5% from qualification
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """)
Â  Â  Â  Â  Â  Â  Â  Â  elif sensitivity == "Balanced":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **Balanced Settings** âš–ï¸
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Momentum Shifts:** Score â‰¥ 50, Acceleration â‰¥ 60
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Emerging Patterns:** Within 10% of qualifying threshold
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Volume Surges:** RVOL â‰¥ 2.0x (standard threshold)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Acceleration Alerts:** Score â‰¥ 70 (good acceleration)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Pattern Distance:** 10% from qualification
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """)
Â  Â  Â  Â  Â  Â  Â  Â  else:Â  # Aggressive
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **Aggressive Settings** ðŸš€
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Momentum Shifts:** Score â‰¥ 40, Acceleration â‰¥ 50
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Emerging Patterns:** Within 15% of qualifying threshold
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Volume Surges:** RVOL â‰¥ 1.5x (building volume)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Acceleration Alerts:** Score â‰¥ 60 (early signals)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - **Pattern Distance:** 15% from qualification
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.info("ðŸ’¡ **Tip**: Start with Balanced, then adjust based on market conditions and your risk tolerance.")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Apply timeframe filtering
Â  Â  Â  Â  if wave_timeframe != "All Waves":
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  if wave_timeframe == "Intraday Surge":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Ensure columns exist before filtering, and handle NaNs with sensible defaults for conditions
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  required_cols = ['rvol', 'ret_1d', 'price', 'prev_close']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all(col in wave_filtered_df.columns for col in required_cols):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['rvol'].fillna(0) >= 2.5) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['ret_1d'].fillna(0) > 2) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['price'].fillna(0) > wave_filtered_df['prev_close'].fillna(0) * 1.02)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Required data for '{wave_timeframe}' not available. Showing all waves.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = filtered_df.copy() # Revert to full filtered_df
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  elif wave_timeframe == "3-Day Buildup":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  required_cols = ['ret_3d', 'vol_ratio_7d_90d', 'price', 'sma_20d']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all(col in wave_filtered_df.columns for col in required_cols):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['ret_3d'].fillna(0) > 5) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['vol_ratio_7d_90d'].fillna(0) > 1.5) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['price'].fillna(0) > wave_filtered_df['sma_20d'].fillna(0))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Required data for '{wave_timeframe}' not available. Showing all waves.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = filtered_df.copy()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  elif wave_timeframe == "Weekly Breakout":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  required_cols = ['ret_7d', 'vol_ratio_7d_90d', 'from_high_pct']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all(col in wave_filtered_df.columns for col in required_cols):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['ret_7d'].fillna(0) > 8) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['vol_ratio_7d_90d'].fillna(0) > 2.0) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['from_high_pct'].fillna(-100) > -10)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Required data for '{wave_timeframe}' not available. Showing all waves.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = filtered_df.copy()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  elif wave_timeframe == "Monthly Trend":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  required_cols = ['ret_30d', 'price', 'sma_20d', 'sma_50d', 'vol_ratio_30d_180d', 'from_low_pct']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all(col in wave_filtered_df.columns for col in required_cols):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['ret_30d'].fillna(0) > 15) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['price'].fillna(0) > wave_filtered_df['sma_20d'].fillna(0)) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['sma_20d'].fillna(0) > wave_filtered_df['sma_50d'].fillna(0)) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['vol_ratio_30d_180d'].fillna(0) > 1.2) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['from_low_pct'].fillna(0) > 30)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Required data for '{wave_timeframe}' not available. Showing all waves.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = filtered_df.copy()
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Error applying {wave_timeframe} filter: {str(e)}")
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"Some data not available for {wave_timeframe} filter, showing all relevant stocks.")
Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df = filtered_df.copy() # Ensure df is not empty on error
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not wave_filtered_df.empty:
Â  Â  Â  Â  Â  Â  # 1. MOMENTUM SHIFT DETECTION
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸš€ Momentum Shifts - Stocks Entering Strength")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Set thresholds based on sensitivity
Â  Â  Â  Â  Â  Â  if sensitivity == "Conservative":
Â  Â  Â  Â  Â  Â  Â  Â  momentum_threshold = 60
Â  Â  Â  Â  Â  Â  Â  Â  acceleration_threshold = 70
Â  Â  Â  Â  Â  Â  Â  Â  min_rvol_signal = 3.0
Â  Â  Â  Â  Â  Â  elif sensitivity == "Balanced":
Â  Â  Â  Â  Â  Â  Â  Â  momentum_threshold = 50
Â  Â  Â  Â  Â  Â  Â  Â  acceleration_threshold = 60
Â  Â  Â  Â  Â  Â  Â  Â  min_rvol_signal = 2.0
Â  Â  Â  Â  Â  Â  else:Â  # Aggressive
Â  Â  Â  Â  Â  Â  Â  Â  momentum_threshold = 40
Â  Â  Â  Â  Â  Â  Â  Â  acceleration_threshold = 50
Â  Â  Â  Â  Â  Â  Â  Â  min_rvol_signal = 1.5
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Find momentum shifts (ensure columns exist and fillna for comparison)
Â  Â  Â  Â  Â  Â  momentum_shifts = wave_filtered_df.copy()
Â  Â  Â  Â  Â  Â  if 'momentum_score' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts = momentum_shifts[momentum_shifts['momentum_score'].fillna(0) >= momentum_threshold]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts = pd.DataFrame() # No momentum data
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if 'acceleration_score' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts = momentum_shifts[momentum_shifts['acceleration_score'].fillna(0) >= acceleration_threshold]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts = pd.DataFrame() # No acceleration data
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not momentum_shifts.empty:
Â  Â  Â  Â  Â  Â  Â  Â  # Calculate signal count
Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts['signal_count'] = 0
Â  Â  Â  Â  Â  Â  Â  Â  if 'momentum_score' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts.loc[momentum_shifts['momentum_score'].fillna(0) >= momentum_threshold, 'signal_count'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  if 'acceleration_score' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts.loc[momentum_shifts['acceleration_score'].fillna(0) >= acceleration_threshold, 'signal_count'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  if 'rvol' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts.loc[momentum_shifts['rvol'].fillna(0) >= min_rvol_signal, 'signal_count'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  if 'breakout_score' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts.loc[momentum_shifts['breakout_score'].fillna(0) >= 75, 'signal_count'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  if 'vol_ratio_7d_90d' in momentum_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts.loc[momentum_shifts['vol_ratio_7d_90d'].fillna(0) >= 1.5, 'signal_count'] += 1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Calculate shift strength, filling NaNs for calculation
Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts['shift_strength'] = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts['momentum_score'].fillna(50) * 0.4 +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts['acceleration_score'].fillna(50) * 0.4 +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  momentum_shifts['rvol_score'].fillna(50) * 0.2
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Get top shifts
Â  Â  Â  Â  Â  Â  Â  Â  top_shifts = momentum_shifts.sort_values(['signal_count', 'shift_strength'], ascending=[False, False]).head(20)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Display
Â  Â  Â  Â  Â  Â  Â  Â  display_columns = ['ticker', 'company_name', 'master_score', 'momentum_score',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'acceleration_score', 'rvol', 'signal_count', 'wave_state']
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if 'ret_7d' in top_shifts.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_columns.insert(-2, 'ret_7d')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  display_columns.append('category')
Â  Â  Â  Â  Â  Â  Â  Â  display_columns.append('sector')
Â  Â  Â  Â  Â  Â  Â  Â  display_columns.append('industry')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  shift_display = top_shifts[[col for col in display_columns if col in top_shifts.columns]].copy()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Add signal indicator
Â  Â  Â  Â  Â  Â  Â  Â  shift_display['Signals'] = shift_display['signal_count'].apply(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lambda x: f"{'ðŸ”¥' * min(x, 3)} {x}/5"
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Format for display, handling NaNs
Â  Â  Â  Â  Â  Â  Â  Â  if 'ret_7d' in shift_display.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shift_display['7D Return'] = shift_display['ret_7d'].apply(lambda x: f"{x:.1f}%" if pd.notna(x) else '-')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  shift_display['RVOL'] = shift_display['rvol'].apply(lambda x: f"{x:.1f}x" if pd.notna(x) else '-')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Rename columns
Â  Â  Â  Â  Â  Â  Â  Â  shift_display = shift_display.rename(columns={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'ticker': 'Ticker',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'company_name': 'Company',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'master_score': 'Score',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'momentum_score': 'Momentum',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'acceleration_score': 'Acceleration',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'wave_state': 'Wave',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'category': 'Category',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'sector': 'Sector',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'industry': 'Industry'
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  shift_display = shift_display.drop('signal_count', axis=1)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(shift_display, use_container_width=True, hide_index=True)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Summary
Â  Â  Â  Â  Â  Â  Â  Â  multi_signal = len(top_shifts[top_shifts['signal_count'] >= 3])
Â  Â  Â  Â  Â  Â  Â  Â  if multi_signal > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"ðŸ† Found {multi_signal} stocks with 3+ signals (strongest momentum)")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Show stocks with 4+ signals separately
Â  Â  Â  Â  Â  Â  Â  Â  super_signals = top_shifts[top_shifts['signal_count'] >= 4]
Â  Â  Â  Â  Â  Â  Â  Â  if len(super_signals) > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"ðŸ”¥ðŸ”¥ {len(super_signals)} stocks showing EXTREME momentum (4+ signals)!")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"No momentum shifts detected in {wave_timeframe} timeframe for '{sensitivity}' sensitivity.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 2. ACCELERATION PROFILES
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸš€ Acceleration Profiles - Momentum Building Over Time")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Set thresholds based on sensitivity
Â  Â  Â  Â  Â  Â  if sensitivity == "Conservative":
Â  Â  Â  Â  Â  Â  Â  Â  accel_threshold = 85
Â  Â  Â  Â  Â  Â  elif sensitivity == "Balanced":
Â  Â  Â  Â  Â  Â  Â  Â  accel_threshold = 70
Â  Â  Â  Â  Â  Â  else:Â  # Aggressive
Â  Â  Â  Â  Â  Â  Â  Â  accel_threshold = 60
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Filter for stocks with sufficient acceleration data
Â  Â  Â  Â  Â  Â  if 'acceleration_score' in wave_filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  accelerating_stocks = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df['acceleration_score'].fillna(0) >= accel_threshold
Â  Â  Â  Â  Â  Â  Â  Â  ].nlargest(10, 'acceleration_score', keep='first')
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  accelerating_stocks = pd.DataFrame()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not accelerating_stocks.empty:
Â  Â  Â  Â  Â  Â  Â  Â  # Create acceleration profiles chart
Â  Â  Â  Â  Â  Â  Â  Â  fig_accel = Visualizer.create_acceleration_profiles(accelerating_stocks, n=10)
Â  Â  Â  Â  Â  Â  Â  Â  st.plotly_chart(fig_accel, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Summary stats
Â  Â  Â  Â  Â  Â  Â  Â  col1, col2, col3 = st.columns(3)
Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  perfect_accel = len(accelerating_stocks[accelerating_stocks['acceleration_score'].fillna(0) >= 90])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Perfect Acceleration (90+)", perfect_accel, help_text="Number of stocks with Acceleration Score >= 90.")
Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  strong_accel = len(accelerating_stocks[accelerating_stocks['acceleration_score'].fillna(0) >= 80])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Strong Acceleration (80+)", strong_accel, help_text="Number of stocks with Acceleration Score >= 80.")
Â  Â  Â  Â  Â  Â  Â  Â  with col3:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  avg_accel = accelerating_stocks['acceleration_score'].fillna(0).mean()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Avg Acceleration Score", f"{avg_accel:.1f}", help_text="Average Acceleration Score for displayed stocks.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"No stocks meet the acceleration threshold ({accel_threshold}+) for '{sensitivity}' sensitivity.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 3. CATEGORY ROTATION FLOW
Â  Â  Â  Â  Â  Â  if show_market_regime:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸ’° Category Rotation - Smart Money Flow")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  col1, col2 = st.columns([3, 2])
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Calculate category performance with dynamic sampling
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'category' in wave_filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  category_dfs = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for cat in wave_filtered_df['category'].dropna().unique():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Apply dynamic sampling using the helper method
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sampled_cat_df = MarketIntelligence._apply_dynamic_sampling(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wave_filtered_df[wave_filtered_df['category'] == cat].copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not sampled_cat_df.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  category_dfs.append(sampled_cat_df)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if category_dfs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  normalized_cat_df = pd.concat(category_dfs, ignore_index=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  category_flow = MarketIntelligence._calculate_flow_metrics(normalized_cat_df, 'category')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add original category size for reference
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_cat_counts = df.groupby('category').size().rename('total_stocks')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  category_flow = category_flow.join(original_cat_counts, how='left')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  category_flow['analyzed_stocks'] = category_flow['count']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not category_flow.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Determine flow direction based on top category
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top_category_name = category_flow.index[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'Small' in top_category_name or 'Micro' in top_category_name:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  flow_direction = "ðŸ”¥ RISK-ON"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif 'Large' in top_category_name or 'Mega' in top_category_name:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  flow_direction = "â„ï¸ RISK-OFF"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  flow_direction = "âž¡ï¸ Neutral"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create visualization
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig_flow = go.Figure()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig_flow.add_trace(go.Bar(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  x=category_flow.index,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  y=category_flow['flow_score'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text=[f"{val:.1f}" for val in category_flow['flow_score']],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  textposition='outside',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  marker_color=['#2ecc71' if score > 60 else '#e74c3c' if score < 40 else '#f39c12'Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â for score in category_flow['flow_score']],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hovertemplate='Category: %{x}<br>Flow Score: %{y:.1f}<br>Stocks: %{customdata[0]} of %{customdata[1]}<extra></extra>',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  customdata=np.column_stack((category_flow['analyzed_stocks'], category_flow['total_stocks']))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig_flow.update_layout(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title=f"Smart Money Flow Direction: {flow_direction} (Dynamically Sampled)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xaxis_title="Market Cap Category",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  yaxis_title="Flow Score",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  height=300,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  template='plotly_white',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  showlegend=False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.plotly_chart(fig_flow, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("Insufficient data for category flow analysis after sampling.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("No valid stocks found in categories for flow analysis after sampling.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("Category data not available for flow analysis.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Error in category flow analysis: {str(e)}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("Unable to analyze category flow")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use .get() for safe access in case category_flow is not defined
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'category_flow' in locals() and not category_flow.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ðŸŽ¯ Market Regime: {flow_direction}**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ’Ž Strongest Categories:**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, (cat, row) in enumerate(category_flow.head(3).iterrows()):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  emoji = "ðŸ¥‡" if i == 0 else "ðŸ¥ˆ" if i == 1 else "ðŸ¥‰"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"{emoji} **{cat}**: Score {row['flow_score']:.1f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Category shifts - ensure scores are available
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'Small Cap' in category_flow.index and 'Large Cap' in category_flow.index:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  small_caps_score = category_flow.loc[['Small Cap', 'Micro Cap'], 'flow_score'].mean()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  large_caps_score = category_flow.loc[['Large Cap', 'Mega Cap'], 'flow_score'].mean()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if small_caps_score > large_caps_score + 10:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("ðŸ“ˆ Small Caps Leading - Early Bull Signal!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif large_caps_score > small_caps_score + 10:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("ðŸ“‰ Large Caps Leading - Defensive Mode")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("âž¡ï¸ Balanced Market - No Clear Leader")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("Insufficient category data for shift analysis.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("Category data not available")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 4. EMERGING PATTERNS
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸŽ¯ Emerging Patterns - About to Qualify")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Set pattern distance based on sensitivity
Â  Â  Â  Â  Â  Â  pattern_distance = {"Conservative": 5, "Balanced": 10, "Aggressive": 15}[sensitivity]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  emergence_data = []
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Check patterns about to emerge
Â  Â  Â  Â  Â  Â  if 'category_percentile' in wave_filtered_df.columns and 'master_score' in wave_filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  close_to_leader = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['category_percentile'].fillna(0) >= (90 - pattern_distance)) &Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['category_percentile'].fillna(0) < 90)
Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  for _, stock in close_to_leader.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  emergence_data.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Ticker': stock['ticker'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Company': stock['company_name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Pattern': 'ðŸ”¥ CAT LEADER',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Distance': f"{90 - stock['category_percentile'].fillna(0):.1f}% away",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Current': f"{stock['category_percentile'].fillna(0):.1f}%ile",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Score': stock['master_score']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if 'breakout_score' in wave_filtered_df.columns and 'master_score' in wave_filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  close_to_breakout = wave_filtered_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['breakout_score'].fillna(0) >= (80 - pattern_distance)) &Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (wave_filtered_df['breakout_score'].fillna(0) < 80)
Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  for _, stock in close_to_breakout.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  emergence_data.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Ticker': stock['ticker'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Company': stock['company_name'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Pattern': 'ðŸŽ¯ BREAKOUT',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Distance': f"{80 - stock['breakout_score'].fillna(0):.1f} pts away",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Current': f"{stock['breakout_score'].fillna(0):.1f} score",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Score': stock['master_score']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if emergence_data:
Â  Â  Â  Â  Â  Â  Â  Â  emergence_df = pd.DataFrame(emergence_data).sort_values('Score', ascending=False).head(15)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  col1, col2 = st.columns([3, 1])
Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(emergence_df, use_container_width=True, hide_index=True)
Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Emerging Patterns", len(emergence_df), help_text="Number of stocks close to qualifying for key patterns.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"No patterns emerging within {pattern_distance}% threshold.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 5. VOLUME SURGE DETECTION
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸŒŠ Volume Surges - Unusual Activity NOW")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Set RVOL threshold based on sensitivity
Â  Â  Â  Â  Â  Â  rvol_threshold_display = {"Conservative": 3.0, "Balanced": 2.0, "Aggressive": 1.5}[sensitivity]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Filter volume surges, ensuring 'rvol' exists and filling NaNs for comparison
Â  Â  Â  Â  Â  Â  if 'rvol' in wave_filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  volume_surges = wave_filtered_df[wave_filtered_df['rvol'].fillna(0) >= rvol_threshold_display].copy()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  volume_surges = pd.DataFrame()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not volume_surges.empty:
Â  Â  Â  Â  Â  Â  Â  Â  # Calculate surge score, filling NaNs for calculation
Â  Â  Â  Â  Â  Â  Â  Â  volume_surges['surge_score'] = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  volume_surges['rvol_score'].fillna(50) * 0.5 +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  volume_surges['volume_score'].fillna(50) * 0.3 +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  volume_surges['momentum_score'].fillna(50) * 0.2
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  top_surges = volume_surges.nlargest(15, 'surge_score', keep='first')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  col1, col2 = st.columns([2, 1])
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_cols_surge = ['ticker', 'company_name', 'rvol', 'price', 'money_flow_mm', 'wave_state', 'category', 'sector', 'industry']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'ret_1d' in top_surges.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_cols_surge.insert(3, 'ret_1d')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display = top_surges[[col for col in display_cols_surge if col in top_surges.columns]].copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add surge type
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display['Type'] = surge_display['rvol'].apply(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lambda x: "ðŸ”¥ðŸ”¥ðŸ”¥" if x > 5 else "ðŸ”¥ðŸ”¥" if x > 3 else "ðŸ”¥" if x > 1.5 else "-"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Format columns
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'ret_1d' in surge_display.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display['ret_1d'] = surge_display['ret_1d'].apply(lambda x: f"{x:+.1f}%" if pd.notna(x) else '-')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'money_flow_mm' in surge_display.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display['money_flow_mm'] = surge_display['money_flow_mm'].apply(lambda x: f"â‚¹{x:.1f}M" if pd.notna(x) else '-')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display['price'] = surge_display['price'].apply(lambda x: f"â‚¹{x:,.0f}" if pd.notna(x) else '-')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display['rvol'] = surge_display['rvol'].apply(lambda x: f"{x:.1f}x" if pd.notna(x) else '-')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Rename columns
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rename_dict_surge = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'ticker': 'Ticker',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'company_name': 'Company',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'rvol': 'RVOL',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'price': 'Price',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'money_flow_mm': 'Money Flow',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'wave_state': 'Wave',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'category': 'Category',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'sector': 'Sector',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'industry': 'Industry',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'ret_1d': '1D Ret'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_display = surge_display.rename(columns=rename_dict_surge)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(surge_display, use_container_width=True, hide_index=True)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Active Surges", len(volume_surges), help_text=f"Number of stocks with RVOL >= {rvol_threshold_display}x.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Extreme (>5x)", len(volume_surges[volume_surges['rvol'].fillna(0) > 5]), help_text="Stocks with RVOL > 5x.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("High (>3x)", len(volume_surges[volume_surges['rvol'].fillna(0) > 3]), help_text="Stocks with RVOL > 3x.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Surge distribution by category
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'category' in volume_surges.columns:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ“Š Surge by Category:**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use dropna and value_counts for robustness
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  surge_categories = volume_surges['category'].dropna().value_counts()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not surge_categories.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for cat, count in surge_categories.head(3).items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"â€¢ {cat}: {count} stocks")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption("No categories with surges.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"No volume surges detected with '{sensitivity}' sensitivity (requires RVOL â‰¥ {rvol_threshold_display}x).")
Â  Â  Â  Â Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning(f"No data available for Wave Radar analysis with '{wave_timeframe}' timeframe. Please adjust filters or timeframe.")
Â  Â Â 
Â  Â  # Tab 3: Analysis
Â  Â  with tabs[3]:
Â  Â  Â  Â  st.markdown("### ðŸ“Š Market Analysis")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not filtered_df.empty:
Â  Â  Â  Â  Â  Â  # Score distribution
Â  Â  Â  Â  Â  Â  col1, col2 = st.columns(2)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  # Score distribution chart
Â  Â  Â  Â  Â  Â  Â  Â  fig_dist = Visualizer.create_score_distribution(filtered_df)
Â  Â  Â  Â  Â  Â  Â  Â  st.plotly_chart(fig_dist, use_container_width=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  # Pattern analysis
Â  Â  Â  Â  Â  Â  Â  Â  pattern_counts = {}
Â  Â  Â  Â  Â  Â  Â  Â  for patterns_str in filtered_df['patterns'].dropna():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if patterns_str:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for p in patterns_str.split(' | '):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pattern_counts[p] = pattern_counts.get(p, 0) + 1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if pattern_counts:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pattern_df = pd.DataFrame(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  list(pattern_counts.items()),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  columns=['Pattern', 'Count']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ).sort_values('Count', ascending=True).tail(15)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig_patterns = go.Figure([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  go.Bar(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  x=pattern_df['Count'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  y=pattern_df['Pattern'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  orientation='h',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  marker_color='#3498db',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text=pattern_df['Count'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  textposition='outside'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig_patterns.update_layout(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title="Pattern Frequency Analysis",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xaxis_title="Number of Stocks",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  yaxis_title="Pattern",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  template='plotly_white',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  height=400,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  margin=dict(l=150)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.plotly_chart(fig_patterns, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("No patterns detected in current selection")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Sector performance
Â  Â  Â  Â  Â  Â  st.markdown("#### Sector Performance (Dynamically Sampled)")
Â  Â  Â  Â  Â  Â  sector_overview_df_local = MarketIntelligence.detect_sector_rotation(filtered_df)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not sector_overview_df_local.empty:
Â  Â  Â  Â  Â  Â  Â  Â  display_cols_overview = ['flow_score', 'avg_score', 'median_score', 'avg_momentum',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'avg_volume', 'avg_rvol', 'avg_ret_30d', 'analyzed_stocks', 'total_stocks']
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  available_overview_cols = [col for col in display_cols_overview if col in sector_overview_df_local.columns]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  sector_overview_display = sector_overview_df_local[available_overview_cols].copy()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  sector_overview_display.columns = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Flow Score', 'Avg Score', 'Median Score', 'Avg Momentum',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Avg Volume', 'Avg RVOL', 'Avg 30D Ret', 'Analyzed Stocks', 'Total Stocks'
Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  sector_overview_display['Coverage %'] = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (sector_overview_display['Analyzed Stocks'] / sector_overview_display['Total Stocks'] * 100)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .replace([np.inf, -np.inf], np.nan)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .fillna(0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .round(1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .apply(lambda x: f"{x}%")
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sector_overview_display.style.background_gradient(subset=['Flow Score', 'Avg Score']),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.info("ðŸ“Š **Normalized Analysis**: Shows metrics for dynamically sampled stocks per sector (by Master Score) to ensure fair comparison across sectors of different sizes.")

Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("No sector data available in the filtered dataset for analysis. Please check your filters.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Category performance
Â  Â  Â  Â  Â  Â  st.markdown("#### Category Performance")
Â  Â  Â  Â  Â  Â  if 'category' in filtered_df.columns:
Â  Â  Â  Â  Â  Â  Â  Â  # Ensure all columns used for aggregation exist and handle NaNs
Â  Â  Â  Â  Â  Â  Â  Â  category_df_agg = filtered_df.groupby('category').agg({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'master_score': ['mean', 'count'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'category_percentile': 'mean',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'money_flow_mm': 'sum' # If column missing, it won't be in result of agg
Â  Â  Â  Â  Â  Â  Â  Â  }).round(2)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Flatten columns names
Â  Â  Â  Â  Â  Â  Â  Â  category_df_agg.columns = ['_'.join(col).strip() for col in category_df_agg.columns.values]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Rename for display
Â  Â  Â  Â  Â  Â  Â  Â  category_df_display = category_df_agg.rename(columns={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'master_score_mean': 'Avg Score',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'master_score_count': 'Count',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'category_percentile_mean': 'Avg Cat %ile',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'money_flow_mm_sum': 'Total Money Flow'
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  category_df_display = category_df_display.sort_values('Avg Score', ascending=False)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  category_df_display.style.background_gradient(subset=['Avg Score']),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("Category column not available in data.")

Â  Â  Â  Â  Â  Â  # Industry performance (now using the dedicated function)
Â  Â  Â  Â  Â  Â  st.markdown("#### Industry Performance (Dynamically Sampled)")
Â  Â  Â  Â  Â  Â  industry_overview_df_local = MarketIntelligence.detect_industry_rotation(filtered_df)
Â  Â  Â  Â  Â  Â  if not industry_overview_df_local.empty:
Â  Â  Â  Â  Â  Â  Â  Â  display_cols_overview_industry = ['flow_score', 'avg_score', 'median_score', 'avg_momentum',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'avg_volume', 'avg_rvol', 'avg_ret_30d', 'analyzed_stocks', 'total_stocks']
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  available_overview_cols_industry = [col for col in display_cols_overview_industry if col in industry_overview_df_local.columns]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  industry_overview_display = industry_overview_df_local[available_overview_cols_industry].copy()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  industry_overview_display.columns = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Flow Score', 'Avg Score', 'Median Score', 'Avg Momentum',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Avg Volume', 'Avg RVOL', 'Avg 30D Ret', 'Analyzed Stocks', 'Total Stocks'
Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  industry_overview_display['Coverage %'] = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (industry_overview_display['Analyzed Stocks'] / industry_overview_display['Total Stocks'] * 100)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .replace([np.inf, -np.inf], np.nan)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .fillna(0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .round(1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .apply(lambda x: f"{x}%")
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  industry_overview_display.style.background_gradient(subset=['Flow Score', 'Avg Score']),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.info("ðŸ“Š **Normalized Analysis**: Shows metrics for dynamically sampled stocks per industry (by Master Score) to ensure fair comparison across industries of different sizes.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("No industry data available in the filtered dataset for analysis. Please check your filters.")
Â  Â  Â  Â Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.info("No data available for analysis. Please adjust filters.")
Â  Â Â 
Â  Â  # Tab 4: Search
Â  Â  with tabs[4]:
Â  Â  Â  Â  st.markdown("### ðŸ” Advanced Stock Search")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Search interface
Â  Â  Â  Â  col1, col2 = st.columns([4, 1])
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  search_query = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  "Search stocks",
Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('wd_search_query', ''), # Persist search query
Â  Â  Â  Â  Â  Â  Â  Â  placeholder="Enter ticker or company name...",
Â  Â  Â  Â  Â  Â  Â  Â  help="Search by ticker symbol or company name",
Â  Â  Â  Â  Â  Â  Â  Â  key="wd_search_input" # Unique key
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  st.markdown("<br>", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  search_clicked = st.button("ðŸ”Ž Search", type="primary", use_container_width=True, key="wd_search_button")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Update session state search query for persistence
Â  Â  Â  Â  if st.session_state.wd_search_input != st.session_state.wd_search_query:
Â  Â  Â  Â  Â  Â  st.session_state.wd_search_query = st.session_state.wd_search_input
Â  Â  Â  Â  Â  Â  st.rerun() # Rerun to apply search immediately if text changes

Â  Â  Â  Â  # Perform search if query is not empty or search button was clicked (and query is not empty)
Â  Â  Â  Â  if st.session_state.wd_search_query or search_clicked:
Â  Â  Â  Â  Â  Â  if not st.session_state.wd_search_query.strip():
Â  Â  Â  Â  Â  Â  Â  Â  st.info("Please enter a search query.")
Â  Â  Â  Â  Â  Â  Â  Â  search_results = pd.DataFrame() # Clear results if query is empty
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Searching..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  search_results = SearchEngine.search_stocks(filtered_df, st.session_state.wd_search_query)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not search_results.empty:
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Found {len(search_results)} matching stock(s)")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Display each result
Â  Â  Â  Â  Â  Â  Â  Â  for idx, stock in search_results.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"ðŸ“Š {stock['ticker']} - {stock['company_name']} "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"(Rank #{int(stock['rank']) if pd.notna(stock['rank']) else 'N/A'})",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  expanded=True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Header metrics
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  metric_cols = st.columns(6)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_cols[0]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Master Score",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{stock['master_score']:.1f}" if pd.notna(stock.get('master_score')) else "N/A",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Rank #{int(stock['rank'])}" if pd.notna(stock.get('rank')) else "N/A",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help_text="Composite score indicating overall strength. Higher is better."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_cols[1]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  price_value = f"â‚¹{stock['price']:,.0f}" if pd.notna(stock.get('price')) else "N/A"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ret_1d_value = f"{stock['ret_1d']:+.1f}%" if pd.notna(stock.get('ret_1d')) else None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card("Price", price_value, ret_1d_value, help_text="Current stock price and 1-day return.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_cols[2]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "From Low",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{stock['from_low_pct']:.0f}%" if pd.notna(stock.get('from_low_pct')) else "N/A",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help_text="Percentage change from 52-week low. Higher means closer to high."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_cols[3]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ret_30d = stock.get('ret_30d')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "30D Return",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{ret_30d:+.1f}%" if pd.notna(ret_30d) else "N/A",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "â†‘" if pd.notna(ret_30d) and ret_30d > 0 else ("â†“" if pd.notna(ret_30d) and ret_30d < 0 else None),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help_text="Percentage return over the last 30 days."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_cols[4]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rvol = stock.get('rvol')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "RVOL",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"{rvol:.1f}x" if pd.notna(rvol) else "N/A",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "High" if pd.notna(rvol) and rvol > 2 else "Normal",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help_text="Relative Volume: current volume compared to average. Higher indicates unusual activity."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_cols[5]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Wave State",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stock.get('wave_state', 'N/A'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stock.get('category', 'N/A'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help_text="Detected current momentum phase (Forming, Building, Cresting, Breaking)."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Score breakdown
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸ“ˆ Score Components")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score_cols_breakdown = st.columns(6)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  components = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("Position", stock.get('position_score'), CONFIG.POSITION_WEIGHT, "52-week range positioning."),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("Volume", stock.get('volume_score'), CONFIG.VOLUME_WEIGHT, "Multi-timeframe volume patterns."),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("Momentum", stock.get('momentum_score'), CONFIG.MOMENTUM_WEIGHT, "30-day price momentum."),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("Acceleration", stock.get('acceleration_score'), CONFIG.ACCELERATION_WEIGHT, "Momentum acceleration signals."),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("Breakout", stock.get('breakout_score'), CONFIG.BREAKOUT_WEIGHT, "Technical breakout readiness."),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("RVOL", stock.get('rvol_score'), CONFIG.RVOL_WEIGHT, "Real-time relative volume score.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, (name, score, weight, help_text_comp) in enumerate(components):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with score_cols_breakdown[i]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.isna(score):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = "âšª"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_score = "N/A"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif score >= 80:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = "ðŸŸ¢"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_score = f"{score:.0f}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif score >= 60:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = "ðŸŸ¡"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_score = f"{score:.0f}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = "ðŸ”´"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_score = f"{score:.0f}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use st.popover for tooltips
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.popover(f"**{name}**", help=help_text_comp):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{name} Score**: {color} {display_score}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"Weighted at **{weight:.0%}** of Master Score.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(help_text_comp)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Patterns
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if stock.get('patterns'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ðŸŽ¯ Patterns:** {stock['patterns']}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸŽ¯ Patterns:** None detected.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Additional details - Reorganized layout
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detail_cols_top = st.columns([1, 1])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with detail_cols_top[0]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ“Š Classification**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Sector: {stock.get('sector', 'Unknown')}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Industry: {stock.get('industry', 'Unknown')}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"Category: {stock.get('category', 'Unknown')}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if show_fundamentals:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ’° Fundamentals**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # PE Ratio
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'pe' in stock and pd.notna(stock['pe']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pe_val = stock['pe']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pe_val <= 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("PE Ratio: ðŸ”´ Loss")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif pe_val < 15:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"PE Ratio: ðŸŸ¢ {pe_val:.1f}x")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif pe_val < 25:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"PE Ratio: ðŸŸ¡ {pe_val:.1f}x")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"PE Ratio: ðŸ”´ {pe_val:.1f}x")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("PE Ratio: N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # EPS Current
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'eps_current' in stock and pd.notna(stock['eps_current']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"EPS Current: â‚¹{stock['eps_current']:.2f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("EPS Current: N/A")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # EPS Change
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'eps_change_pct' in stock and pd.notna(stock['eps_change_pct']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  eps_chg = stock['eps_change_pct']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if eps_chg >= 100:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"EPS Growth: ðŸš€ {eps_chg:+.0f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif eps_chg >= 50:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"EPS Growth: ðŸ”¥ {eps_chg:+.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif eps_chg >= 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"EPS Growth: ðŸ“ˆ {eps_chg:+.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"EPS Growth: ðŸ“‰ {eps_chg:+.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("EPS Growth: N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with detail_cols_top[1]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ“ˆ Performance**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for period, col in [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("1 Day", 'ret_1d'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("7 Days", 'ret_7d'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("30 Days", 'ret_30d'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("3 Months", 'ret_3m'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("6 Months", 'ret_6m'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("1 Year", 'ret_1y')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if col in stock.index and pd.notna(stock[col]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"{period}: {stock[col]:+.1f}%")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"{period}: N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Technicals and Trading Position (next row)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  detail_cols_tech = st.columns([1,1])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with detail_cols_tech[0]: # This will contain 52W info and Trading Position
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ” Technicals**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 52-week range details
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all(col in stock.index and pd.notna(stock[col]) for col in ['low_52w', 'high_52w']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"52W Low: â‚¹{stock.get('low_52w', 0):,.0f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"52W High: â‚¹{stock.get('high_52w', 0):,.0f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("52W Range: N/A")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"From High: {stock.get('from_high_pct', 'N/A'):.0f}%" if pd.notna(stock.get('from_high_pct')) else "From High: N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text(f"From Low: {stock.get('from_low_pct', 'N/A'):.0f}%" if pd.notna(stock.get('from_low_pct')) else "From Low: N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ“Š Trading Position**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tp_col1, tp_col2, tp_col3 = st.columns(3)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_price = stock.get('price', 0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sma_checks = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('sma_20d', '20DMA'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('sma_50d', '50DMA'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('sma_200d', '200DMA')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, (sma_col, sma_label) in enumerate(sma_checks):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_col = [tp_col1, tp_col2, tp_col3][i]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with display_col:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sma_value = stock.get(sma_col)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pd.notna(sma_value) and sma_value > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if current_price > sma_value:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pct_diff = ((current_price - sma_value) / sma_value) * 100
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{sma_label}**: <span style='color:green'>â†‘{pct_diff:.1f}%</span>", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pct_diff = ((sma_value - current_price) / sma_value) * 100
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{sma_label}**: <span style='color:red'>â†“{pct_diff:.1f}%</span>", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{sma_label}**: N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with detail_cols_tech[1]: # This will contain Trend Analysis and Advanced Metrics
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ðŸ“ˆ Trend Analysis**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'trend_quality' in stock.index and pd.notna(stock['trend_quality']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tq = stock['trend_quality']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if tq >= 80:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"ðŸ”¥ Strong Uptrend ({tq:.0f})", help_text="Price is above all key moving averages, and they are aligned.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif tq >= 60:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"âœ… Good Uptrend ({tq:.0f})", help_text="Price is above most key moving averages.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif tq >= 40:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"âž¡ï¸ Neutral Trend ({tq:.0f})", help_text="Price is oscillating around moving averages.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"âš ï¸ Weak/Downtrend ({tq:.0f})", help_text="Price is below most key moving averages.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("Trend: N/A")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Advanced Metrics
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸŽ¯ Advanced Metrics")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  adv_col1, adv_col2 = st.columns(2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with adv_col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'vmi' in stock and pd.notna(stock['vmi']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("VMI", f"{stock['vmi']:.2f}", help_text="Volume Momentum Index: measures the strength of volume trend across timeframes.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("VMI", "N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'momentum_harmony' in stock and pd.notna(stock['momentum_harmony']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  harmony_val = stock['momentum_harmony']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  harmony_emoji = "ðŸŸ¢" if harmony_val >= 3 else "ðŸŸ¡" if harmony_val >= 2 else "ðŸ”´"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Harmony", f"{harmony_emoji} {int(harmony_val)}/4", help_text="Momentum Harmony: measures alignment of returns across multiple timeframes (1D, 7D, 30D, 3M). Max 4/4 is perfect alignment.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Harmony", "N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with adv_col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'position_tension' in stock and pd.notna(stock['position_tension']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Position Tension", f"{stock['position_tension']:.0f}", help_text="Measures the stock's position within its 52-week range relative to its volatility. Higher implies more 'tension' or readiness for a move.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Position Tension", "N/A")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'money_flow_mm' in stock and pd.notna(stock['money_flow_mm']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Money Flow", f"â‚¹{stock['money_flow_mm']:.1f}M", help_text="Estimated institutional money flow in millions (Price * Volume * RVOL). Higher indicates strong buying/selling pressure.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Money Flow", "N/A")

Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("No stocks found matching your search criteria.")
Â  Â Â 
Â  Â  # Tab 5: Export
Â  Â  with tabs[5]:
Â  Â  Â  Â  st.markdown("### ðŸ“¥ Export Data")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Export template selection
Â  Â  Â  Â  st.markdown("#### ðŸ“‹ Export Templates")
Â  Â  Â  Â  export_template = st.radio(
Â  Â  Â  Â  Â  Â  "Choose export template:",
Â  Â  Â  Â  Â  Â  options=[
Â  Â  Â  Â  Â  Â  Â  Â  "Full Analysis (All Data)",
Â  Â  Â  Â  Â  Â  Â  Â  "Day Trader Focus",
Â  Â  Â  Â  Â  Â  Â  Â  "Swing Trader Focus",
Â  Â  Â  Â  Â  Â  Â  Â  "Investor Focus"
Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  key="wd_export_template_radio", # Unique key
Â  Â  Â  Â  Â  Â  help="Select a template based on your trading style"
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Map template names
Â  Â  Â  Â  template_map = {
Â  Â  Â  Â  Â  Â  "Full Analysis (All Data)": "full",
Â  Â  Â  Â  Â  Â  "Day Trader Focus": "day_trader",
Â  Â  Â  Â  Â  Â  "Swing Trader Focus": "swing_trader",
Â  Â  Â  Â  Â  Â  "Investor Focus": "investor"
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  selected_template = template_map[export_template]
Â  Â  Â  Â Â 
Â  Â  Â  Â  col1, col2 = st.columns(2)
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸ“Š Excel Report")
Â  Â  Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  Â  Â  "Comprehensive multi-sheet report including:\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Top 100 stocks with all scores\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Market intelligence dashboard\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Sector rotation analysis\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Industry rotation analysis (NEW)\n" # Added NEW
Â  Â  Â  Â  Â  Â  Â  Â  "- Pattern frequency analysis\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Wave Radar signals\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Summary statistics"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if st.button("Generate Excel Report", type="primary", use_container_width=True, key="wd_generate_excel"):
Â  Â  Â  Â  Â  Â  Â  Â  if len(filtered_df) == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("No data to export. Please adjust your filters.")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Creating Excel report..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  excel_file = ExportEngine.create_excel_report(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filtered_df, template=selected_template
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download Excel Report",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=excel_file,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"wave_detection_report_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.xlsx",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_download_excel_button"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("Excel report generated successfully!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error generating Excel report: {str(e)}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Excel export error: {str(e)}", exc_info=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  st.markdown("#### ðŸ“„ CSV Export")
Â  Â  Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  Â  Â  "Enhanced CSV format with:\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- All ranking scores\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Advanced metrics (VMI, Money Flow)\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Pattern detections\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Wave states\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Category classifications\n"
Â  Â  Â  Â  Â  Â  Â  Â  "- Optimized for further analysis"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if st.button("Generate CSV Export", use_container_width=True, key="wd_generate_csv"):
Â  Â  Â  Â  Â  Â  Â  Â  if len(filtered_df) == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("No data to export. Please adjust your filters.")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_data = ExportEngine.create_csv_export(filtered_df)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Download CSV File",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_data,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=f"wave_detection_data_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="wd_download_csv_button"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("CSV export generated successfully!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error generating CSV: {str(e)}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"CSV export error: {str(e)}", exc_info=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Export statistics
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.markdown("#### ðŸ“Š Export Preview")
Â  Â  Â  Â Â 
Â  Â  Â  Â  export_stats = {
Â  Â  Â  Â  Â  Â  "Total Stocks": len(filtered_df),
Â  Â  Â  Â  Â  Â  "Average Score": f"{filtered_df['master_score'].mean():.1f}" if not filtered_df.empty else "N/A",
Â  Â  Â  Â  Â  Â  "Stocks with Patterns": (filtered_df['patterns'] != '').sum() if 'patterns' in filtered_df.columns else 0,
Â  Â  Â  Â  Â  Â  "High RVOL (>2x)": (filtered_df['rvol'].fillna(0) > 2).sum() if 'rvol' in filtered_df.columns else 0,
Â  Â  Â  Â  Â  Â  "Positive 30D Returns": (filtered_df['ret_30d'].fillna(0) > 0).sum() if 'ret_30d' in filtered_df.columns else 0,
Â  Â  Â  Â  Â  Â  "Data Quality": f"{st.session_state.data_quality.get('completeness', 0):.1f}%"
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  stat_cols = st.columns(3)
Â  Â  Â  Â  for i, (label, value) in enumerate(export_stats.items()):
Â  Â  Â  Â  Â  Â  with stat_cols[i % 3]:
Â  Â  Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(label, value)
Â  Â Â 
Â  Â  # Tab 6: About
Â  Â  with tabs[6]:
Â  Â  Â  Â  st.markdown("### â„¹ï¸ About Wave Detection Ultimate 3.0 - Final Production Version")
Â  Â  Â  Â Â 
Â  Â  Â  Â  col1, col2 = st.columns([2, 1])
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  st.markdown("""
Â  Â  Â  Â  Â  Â  #### ðŸŒŠ Welcome to Wave Detection Ultimate 3.0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  The FINAL production version of the most advanced stock ranking system designed to catch momentum waves early.
Â  Â  Â  Â  Â  Â  This professional-grade tool combines technical analysis, volume dynamics, advanced metrics, andÂ 
Â  Â  Â  Â  Â  Â  smart pattern recognition to identify high-potential stocks before they peak.
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸŽ¯ Core Features - LOCKED IN PRODUCTION
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Master Score 3.0** - Proprietary ranking algorithm (DO NOT MODIFY):
Â  Â  Â  Â  Â  Â  - **Position Analysis (30%)** - 52-week range positioning
Â  Â  Â  Â  Â  Â  - **Volume Dynamics (25%)** - Multi-timeframe volume patterns
Â  Â  Â  Â  Â  Â  - **Momentum Tracking (15%)** - 30-day price momentum
Â  Â  Â  Â  Â  Â  - **Acceleration Detection (10%)** - Momentum acceleration signals
Â  Â  Â  Â  Â  Â  - **Breakout Probability (10%)** - Technical breakout readiness
Â  Â  Â  Â  Â  Â  - **RVOL Integration (10%)** - Real-time relative volume
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Advanced Metrics** - NEW IN FINAL VERSION:
Â  Â  Â  Â  Â  Â  - **Money Flow** - Price Ã— Volume Ã— RVOL in millions (Estimated institutional money movement.)
Â  Â  Â  Â  Â  Â  - **VMI (Volume Momentum Index)** - Weighted volume trend score (Quantifies sustained volume interest.)
Â  Â  Â  Â  Â  Â  - **Position Tension** - Range position stress indicator (Measures readiness for a move based on 52W range.)
Â  Â  Â  Â  Â  Â  - **Momentum Harmony** - Multi-timeframe alignment (0-4) (Scores consistency of momentum across periods.)
Â  Â  Â  Â  Â  Â  - **Wave State** - Real-time momentum classification (Categorizes the stage of a stock's momentum cycle.)
Â  Â  Â  Â  Â  Â  - **Overall Wave Strength** - Composite score for wave filter (Aggregated indicator of underlying wave force.)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Wave Radarâ„¢** - Enhanced detection system:
Â  Â  Â  Â  Â  Â  - Momentum shift detection with signal counting
Â  Â  Â  Â  Â  Â  - Smart money flow tracking by category and **industry (NEW)**
Â  Â  Â  Â  Â  Â  - Pattern emergence alerts with distance metrics
Â  Â  Â  Â  Â  Â  - Market regime detection (Risk-ON/OFF/Neutral)
Â  Â  Â  Â  Â  Â  - Sensitivity controls (Conservative/Balanced/Aggressive)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **25 Pattern Detection** - Complete set:
Â  Â  Â  Â  Â  Â  - 11 Technical patterns
Â  Â  Â  Â  Â  Â  - 5 Fundamental patterns (Hybrid mode)
Â  Â  Â  Â  Â  Â  - 6 Price range patterns
Â  Â  Â  Â  Â  Â  - 3 NEW intelligence patterns (Stealth, Vampire, Perfect Storm)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸ’¡ How to Use
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  1. **Data Source** - Google Sheets (default) or CSV upload
Â  Â  Â  Â  Â  Â  2. **Quick Actions** - Instant filtering for common scenarios
Â  Â  Â  Â  Â  Â  3. **Smart Filters** - Interconnected filtering system, including new Wave filters
Â  Â  Â  Â  Â  Â  4. **Display Modes** - Technical or Hybrid (with fundamentals)
Â  Â  Â  Â  Â  Â  5. **Wave Radar** - Monitor early momentum signals
Â  Â  Â  Â  Â  Â  6. **Export Templates** - Customized for trading styles
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸ”§ Production Features
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  - **Performance Optimized** - Sub-2 second processing
Â  Â  Â  Â  Â  Â  - **Memory Efficient** - Handles 2000+ stocks smoothly
Â  Â  Â  Â  Â  Â  - **Error Resilient** - Graceful degradation with retry logic
Â  Â  Â  Â  Â  Â  - **Data Validation** - Comprehensive quality checks with clipping alerts
Â  Â  Â  Â  Â  Â  - **Smart Caching** - Daily invalidation for data freshness
Â  Â  Â  Â  Â  Â  - **Mobile Responsive** - Works on all devices
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸ“Š Data Processing Pipeline
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  1. Load from Google Sheets or CSV
Â  Â  Â  Â  Â  Â  2. Validate and clean all 41 columns (with clipping notifications)
Â  Â  Â  Â  Â  Â  3. Calculate 6 component scores
Â  Â  Â  Â  Â  Â  4. Generate Master Score 3.0
Â  Â  Â  Â  Â  Â  5. Calculate advanced metrics
Â  Â  Â  Â  Â  Â  6. Detect all 25 patterns
Â  Â  Â  Â  Â  Â  7. Classify into tiers
Â  Â  Â  Â  Â  Â  8. Apply smart ranking
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸŽ¨ Display Modes
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Technical Mode** (Default)
Â  Â  Â  Â  Â  Â  - Pure momentum analysis
Â  Â  Â  Â  Â  Â  - Technical indicators only
Â  Â  Â  Â  Â  Â  - Pattern detection
Â  Â  Â  Â  Â  Â  - Volume dynamics
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Hybrid Mode**
Â  Â  Â  Â  Â  Â  - All technical features
Â  Â  Â  Â  Â  Â  - PE ratio analysis
Â  Â  Â  Â  Â  Â  - EPS growth tracking
Â  Â  Â  Â  Â  Â  - Fundamental patterns
Â  Â  Â  Â  Â  Â  - Value indicators
Â  Â  Â  Â  Â  Â  """)
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  st.markdown("""
Â  Â  Â  Â  Â  Â  #### ðŸ“ˆ Pattern Groups
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Technical Patterns**
Â  Â  Â  Â  Â  Â  - ðŸ”¥ CAT LEADER
Â  Â  Â  Â  Â  Â  - ðŸ’Ž HIDDEN GEM
Â  Â  Â  Â  Â  Â  - ðŸš€ ACCELERATING
Â  Â  Â  Â  Â  Â  - ðŸ¦ INSTITUTIONAL
Â  Â  Â  Â  Â  Â  - âš¡ VOL EXPLOSION
Â  Â  Â  Â  Â  Â  - ðŸŽ¯ BREAKOUT
Â  Â  Â  Â  Â  Â  - ðŸ‘‘ MARKET LEADER
Â  Â  Â  Â  Â  Â  - ðŸŒŠ MOMENTUM WAVE
Â  Â  Â  Â  Â  Â  - ðŸ’° LIQUID LEADER
Â  Â  Â  Â  Â  Â  - ðŸ’ª LONG STRENGTH
Â  Â  Â  Â  Â  Â  - ðŸ“ˆ QUALITY TREND
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Range Patterns**
Â  Â  Â  Â  Â  Â  - ðŸŽ¯ 52W HIGH APPROACH
Â  Â  Â  Â  Â  Â  - ðŸ”„ 52W LOW BOUNCE
Â  Â  Â  Â  Â  Â  - ðŸ‘‘ GOLDEN ZONE
Â  Â  Â  Â  Â  Â  - ðŸ“Š VOL ACCUMULATION
Â  Â  Â  Â  Â  Â  - ðŸ”€ MOMENTUM DIVERGE
Â  Â  Â  Â  Â  Â  - ðŸŽ¯ RANGE COMPRESS
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **NEW Intelligence**
Â  Â  Â  Â  Â  Â  - ðŸ¤« STEALTH
Â  Â  Â  Â  Â  Â  - ðŸ§› VAMPIRE
Â  Â  Â  Â  Â  Â  - â›ˆï¸ PERFECT STORM
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Fundamental** (Hybrid)
Â  Â  Â  Â  Â  Â  - ðŸ’Ž VALUE MOMENTUM
Â  Â  Â  Â  Â  Â  - ðŸ“Š EARNINGS ROCKET
Â  Â  Â  Â  Â  Â  - ðŸ† QUALITY LEADER
Â  Â  Â  Â  Â  Â  - âš¡ TURNAROUND
Â  Â  Â  Â  Â  Â  - âš ï¸ HIGH PE
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### âš¡ Performance
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  - Initial load: <2 seconds
Â  Â  Â  Â  Â  Â  - Filtering: <200ms
Â  Â  Â  Â  Â  Â  - Pattern detection: <500ms
Â  Â  Â  Â  Â  Â  - Search: <50ms
Â  Â  Â  Â  Â  Â  - Export: <1 second
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸ”’ Production Status
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Version**: 3.0.7-FINAL-COMPLETE
Â  Â  Â  Â  Â  Â  **Last Updated**: July 2025
Â  Â  Â  Â  Â  Â  **Status**: PRODUCTION
Â  Â  Â  Â  Â  Â  **Updates**: LOCKED
Â  Â  Â  Â  Â  Â  **Testing**: COMPLETE
Â  Â  Â  Â  Â  Â  **Optimization**: MAXIMUM
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  #### ðŸ’¬ Credits
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Developed for professional traders
Â  Â  Â  Â  Â  Â  requiring reliable, fast, and
Â  Â  Â  Â  Â  Â  comprehensive market analysis.
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  This is the FINAL version.
Â  Â  Â  Â  Â  Â  No further updates will be made.
Â  Â  Â  Â  Â  Â  All features are permanent.
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  ---
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  **Indian Market Optimized**
Â  Â  Â  Â  Â  Â  - â‚¹ Currency formatting
Â  Â  Â  Â  Â  Â  - IST timezone aware
Â  Â  Â  Â  Â  Â  - NSE/BSE categories
Â  Â  Â  Â  Â  Â  - Local number formats
Â  Â  Â  Â  Â  Â  """)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # System stats
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.markdown("#### ðŸ“Š Current Session Statistics")
Â  Â  Â  Â Â 
Â  Â  Â  Â  stats_cols = st.columns(4)
Â  Â  Â  Â Â 
Â  Â  Â  Â  with stats_cols[0]:
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Total Stocks Loaded",
Â  Â  Â  Â  Â  Â  Â  Â  f"{len(ranked_df):,}" if 'ranked_df' in locals() and ranked_df is not None else "0",
Â  Â  Â  Â  Â  Â  Â  Â  help_text="Total number of stocks loaded into the application before any filtering."
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  with stats_cols[1]:
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Currently Filtered",
Â  Â  Â  Â  Â  Â  Â  Â  f"{len(filtered_df):,}" if 'filtered_df' in locals() and filtered_df is not None else "0",
Â  Â  Â  Â  Â  Â  Â  Â  help_text="Number of stocks remaining after applying all selected filters."
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  with stats_cols[2]:
Â  Â  Â  Â  Â  Â  data_quality = st.session_state.data_quality.get('completeness', 0)
Â  Â  Â  Â  Â  Â  quality_emoji = "ðŸŸ¢" if data_quality > 80 else "ðŸŸ¡" if data_quality > 60 else "ðŸ”´"
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Data Quality",
Â  Â  Â  Â  Â  Â  Â  Â  f"{quality_emoji} {data_quality:.1f}%",
Â  Â  Â  Â  Â  Â  Â  Â  help_text="Overall completeness percentage of data fields for loaded stocks."
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  with stats_cols[3]:
Â  Â  Â  Â  Â  Â  cache_time = datetime.now(timezone.utc) - st.session_state.last_refresh
Â  Â  Â  Â  Â  Â  minutes = int(cache_time.total_seconds() / 60)
Â  Â  Â  Â  Â  Â  cache_status = "Fresh" if minutes < 60 else "Stale"
Â  Â  Â  Â  Â  Â  cache_emoji = "ðŸŸ¢" if minutes < 60 else "ðŸ”´"
Â  Â  Â  Â  Â  Â  UIComponents.render_metric_card(
Â  Â  Â  Â  Â  Â  Â  Â  "Cache Age",
Â  Â  Â  Â  Â  Â  Â  Â  f"{cache_emoji} {minutes} min",
Â  Â  Â  Â  Â  Â  Â  Â  cache_status,
Â  Â  Â  Â  Â  Â  Â  Â  help_text="Time since data was last refreshed from source or cache was cleared. Cache invalidates daily."
Â  Â  Â  Â  Â  Â  )
Â  Â Â 
Â  Â  # Footer
Â  Â  st.markdown("---")
Â  Â  st.markdown(
Â  Â  Â  Â  """
Â  Â  Â  Â  <div style="text-align: center; color: #666; padding: 1rem;">
Â  Â  Â  Â  Â  Â  ðŸŒŠ Wave Detection Ultimate 3.0 - Final Production Version<br>
Â  Â  Â  Â  Â  Â  <small>Professional Stock Ranking System â€¢ All Features Complete â€¢ Performance Optimized â€¢ Permanently Locked</small>
Â  Â  Â  Â  </div>
Â  Â  Â  Â  """,
Â  Â  Â  Â  unsafe_allow_html=True
Â  Â  )

# ============================================
# APPLICATION ENTRY POINT
# ============================================

if __name__ == "__main__":
Â  Â  try:
Â  Â  Â  Â  import re # Import re for SearchEngine.search_stocks regex
Â  Â  Â  Â  # Run the application
Â  Â  Â  Â  main()
Â  Â  except Exception as e:
Â  Â  Â  Â  # Global error handler
Â  Â  Â  Â  st.error(f"Critical Application Error: {str(e)}")
Â  Â  Â  Â  logger.error(f"Application crashed: {str(e)}", exc_info=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Show recovery options
Â  Â  Â  Â  if st.button("ðŸ”„ Restart Application"):
Â  Â  Â  Â  Â  Â  st.cache_data.clear()
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button("ðŸ“§ Report Issue"):
Â  Â  Â  Â  Â  Â  st.info("Please take a screenshot and report this error.")

# END OF WAVE DETECTION ULTIMATE 3.0 - FINAL PRODUCTION VERSION
